"""Responses that stream content from LLMs."""

from collections.abc import AsyncIterator, Awaitable, Iterator, Sequence
from typing import Generic, Literal, overload

from ..content import AssistantContent, Text, Thinking, ToolCall
from ..formatting import FormatT, Partial
from ..messages import Message
from ..streams import AsyncStream, AsyncTextStream, Stream, StreamT, TextStream
from .base_response import BaseResponse
from .finish_reason import FinishReason


class StreamResponse(BaseResponse[FormatT], Generic[StreamT, FormatT]):
    """A StreamResponse wraps response content from the LLM with a streaming interface.

    This class is generic over both the stream type (StreamT) and format type (FormatT).
    The StreamT parameter determines whether streaming is synchronous or asynchronous:
    - StreamResponse[Stream, FormatT] for synchronous streaming
    - StreamResponse[AsyncStream, FormatT] for asynchronous streaming

    This class supports iteration to process chunks as they arrive from the model.

    Content can be streamed in one of three ways:
    - Via `response.streams()`, which provides an iterator of streams, where each
      stream contains chunks of streamed data. The chunks contain `delta`s (new content
      in that particular chunk), as well as a accumulated state of that content part
      including all of the chunks that are already available.
    - Via `response.text_stream()` a helper method which provides all of the text content
      in the response, as `str` deltas. Iterating through `text_stream` will only yield
      text content, but it will still consume the full stream.
    - Via `response.structured_stream()`, a helper method which provides partial
      structured outputs from a response (useful when FormatT is set). Iterating through
      `structured_stream` will only yield structured partials, but it will still consume
      the full stream.

    As chunks are consumed, they are collected in-memory on the StreamResponse, and they
    become available in `response.content`, `response.messages`, `response.tool_calls`,
    etc. All of the stream iterators can be replayed after the stream has been consumed,
    in which case they will yield chunks from memory in the original sequence that
    came from the LLM. If the stream is only partially consumed, a fresh iterator will
    first iterate through in-memory content, and then will continue consuming fresh
    chunks from the LLM.

    In the specific case of text chunks, they are included in the response content as soon
    as they become available, via an `llm.Text` part that updates as more deltas come in.
    This enables the behavior where resuming a partially-streamed response will include
    as much text as the model generated.

    For other chunks, like `Thinking` or `ToolCall`, they are only added to response
    content once the corresponding part has fully streamed. This avoids issues like
    adding incomplete tool calls, or thinking blocks missing signatures, to the response.

    For each iterator, fully iterating through the iterator will consume the whole
    LLM stream, such that `finish_reason` will be set (not None). You can pause stream
    execution midway by breaking out of the iterator.

    Here is an example showing semantics for breaking midway through a stream:
    ```python
    # First iteration - stream until we hit a tool call, then break
    stream_count = 0
    for stream in response.streams():
        stream_count += 1
        print(f"First pass: Stream {stream_count} - {stream.type}")
        if stream.content_type == "tool_call":
            break  # Break after seeing tool call stream

    # Second iteration - replays from memory, then continues if more data available
    stream_count = 0
    for stream in response.streams():
        stream_count += 1
        print(f"Second pass: Stream {stream_count} - {stream.type}")

    # Output might be:
    # > First pass: Stream 1 - thinking_stream
    # > First pass: Stream 2 - tool_call_stream
    # > Second pass: Stream 1 - thinking_stream
    # > Second pass: Stream 2 - tool_call_stream
    # > Second pass: Stream 3 - text_stream
    ```

    Example:
        ```python
        from mirascope import llm

        @llm.call("openai:gpt-4o-mini")
        def answer_question(question: str) -> str:
            return f"Answer this question: {question}"

        stream_response = answer_question.stream("What is the capital of France?")
        for chunk in stream_response.stream_text():
            print(chunk, end="", flush=True)
        print()
        ```
    """

    content: Sequence[AssistantContent]
    """The content generated by the LLM.
    
    Content is updated in this array as it is consumed by the stream. Text content will 
    update with each text chunk (this will mutate the Text object that is returned 
    rather than creating a new one). Other content will be added once each part
    is fully streamed."""

    messages: list[Message]
    """The message history, including the most recent assistant message.

    The most recent assistant message will have all of the completed content that has 
    already been consumed from the stream. Text content will be included as each chunk 
    is processed; other content will be included only when its corresponding part is 
    completed (to avoid partial tool calls and the like). If no content has been 
    streamed, then the final assistant message will be present (to maintain turn order 
    expectations), but will be empty."""

    texts: Sequence[Text]
    """The text content in the generated response, if any.
    
    Text content updates with each text chunk as it streams. The Text objects are 
    mutated in place rather than creating new ones for each chunk."""

    tool_calls: Sequence[ToolCall]
    """The tools the LLM wants called on its behalf, if any.
    
    Tool calls are only added to this sequence once they have been fully streamed 
    to avoid partial tool calls in the response."""

    thinkings: Sequence[Thinking]
    """The thinking content in the generated response, if any.
    
    Thinking content is only added to this sequence once it has been fully streamed 
    to avoid partial thinking blocks in the response."""

    finish_reason: FinishReason | None
    """The reason why the LLM finished generating a response.
    
    Will be None until the stream has finished streaming completely."""

    @overload
    def text_stream(self: "StreamResponse[Stream, FormatT]") -> TextStream: ...

    @overload
    def text_stream(
        self: "StreamResponse[AsyncStream, FormatT]",
    ) -> Awaitable[AsyncTextStream]: ...

    def text_stream(self) -> TextStream | Awaitable[AsyncTextStream]:
        """Get a text stream containing all the text content from the response.

        The return type depends on the StreamT generic parameter:
        - For StreamResponse[Stream, FormatT]: returns TextStream (synchronous iterator)
        - For StreamResponse[AsyncStream, FormatT]: returns Awaitable[AsyncTextStream]

        If the response contains multiple distinct Text components, then they will be
        combined with newlines separating them.

        Fully iterating through this iterator will fully consume the underlying stream,
        updating the Response with all collected content (Text and other).

        When called, text_stream always returns a fresh iterator that replays text
        chunks starting at the beginning of the stream.
        """
        raise NotImplementedError()

    @overload
    def streams(self: "StreamResponse[Stream, FormatT]") -> Iterator[Stream]: ...

    """Overload for synchronous streaming."""

    @overload
    def streams(
        self: "StreamResponse[AsyncStream, FormatT]",
    ) -> Awaitable[AsyncIterator[AsyncStream]]: ...

    """Overload for asynchronous streaming."""

    def streams(self) -> Iterator[Stream] | Awaitable[AsyncIterator[AsyncStream]]:
        """Get an iterator that yields streams for each content part in the response.

        The return type depends on the StreamT generic parameter:
        - For StreamResponse[Stream, FormatT]: returns Iterator[Stream] (synchronous)
        - For StreamResponse[AsyncStream, FormatT]: returns Awaitable[AsyncIterator[AsyncStream]]

        Each content part in the response will correspond to one stream, which will yield
        chunks of content as they come in from the underlying LLM.

        Fully iterating through this iterator will fully consume the underlying stream,
        updating the Response with all collected content.

        As content is consumed, it is cached on the StreamResponse. If a new iterator
        is constructed via calling `streams()`, it will start by replaying the cached
        content from the response, and (if there is still more content to consume from
        the LLM), it will proceed to consume it once it has iterated through all the
        cached chunks.
        """
        raise NotImplementedError()

    @overload
    def structured_stream(
        self: "StreamResponse[Stream, FormatT]",
    ) -> Iterator[Partial[FormatT]]: ...

    @overload
    def structured_stream(
        self: "StreamResponse[AsyncStream, FormatT]",
    ) -> Awaitable[AsyncIterator[Partial[FormatT]]]: ...

    def structured_stream(
        self,
    ) -> Iterator[Partial[FormatT]] | Awaitable[AsyncIterator[Partial[FormatT]]]:
        """Get an iterator that yields partial structured objects as content streams.

        The return type depends on the StreamT generic parameter:
        - For StreamResponse[Stream, FormatT]: returns Iterator[Partial[FormatT]] (synchronous)
        - For StreamResponse[AsyncStream, FormatT]: returns Awaitable[AsyncIterator[Partial[FormatT]]]

        This method yields Partial[FormatT] objects as the response content is streamed,
        allowing you to access partial structured data before the response is fully complete.
        Each yielded object represents the current state of the parsed structure with all
        fields optional.

        Fully iterating through this iterator will fully consume the underlying stream,
        updating the Response with all collected content.
        """
        raise NotImplementedError()

    @overload
    def format(self, partial: Literal[True]) -> Partial[FormatT]:
        """Format the response into a Partial[BaseModel] (with optional fields).

        This is useful for when the stream is only partially consumed, in which case the
        structured output may only be partially available."""
        ...

    @overload
    def format(self, partial: Literal[False] = False) -> FormatT:
        """Format the response into a Pydantic BaseModel."""
        ...

    def format(self, partial: bool = False) -> FormatT | Partial[FormatT]:
        """Format the response according to the response format parser.

        It will parse the response content according to the specified format (if present)
        and return a structured object. Returns None if there was no format.

        When called with `partial=True`, it will return a partial of the model, with all
        fields optional.

        Returns:
            The formatted response object of type T.

        Raises:
            ValueError: If the response cannot be formatted according to the
                specified format.
        """
        raise NotImplementedError()
