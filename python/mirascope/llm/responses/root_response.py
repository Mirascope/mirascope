"""Base interface for all LLM responses."""

import json
from abc import ABC
from collections.abc import Sequence
from typing import TYPE_CHECKING, Any, Generic, Literal, overload

from ..content import AssistantContentPart, Text, Thinking, ToolCall
from ..formatting import FormatT, Partial
from ..messages import Message
from ..tools import ToolkitT
from ..types import NoneType
from . import _utils
from .finish_reason import FinishReason

if TYPE_CHECKING:
    from ..clients import BaseParams, ModelId, Provider
    from ..models import Model


class RootResponse(Generic[ToolkitT, FormatT], ABC):
    """Base class for LLM responses."""

    raw: Any
    """The raw response from the LLM."""

    provider: "Provider"
    """The provider that generated this response."""

    model_id: "ModelId"
    """The model id that generated this response."""

    params: "BaseParams | None"
    """The params that were used to generate this response (or None)."""

    toolkit: ToolkitT
    """The toolkit containing the tools used when generating this response."""

    messages: list[Message]
    """The message history, including the most recent assistant message."""

    content: Sequence[AssistantContentPart]
    """The content generated by the LLM."""

    texts: Sequence[Text]
    """The text content in the generated response, if any."""

    tool_calls: Sequence[ToolCall]
    """The tools the LLM wants called on its behalf, if any."""

    thoughts: Sequence[Thinking]
    """The thinking content in the generated response, if any."""

    finish_reason: FinishReason | None
    """The reason why the LLM finished generating a response, if available."""

    format_type: type[FormatT] | None
    """The type of output expected from formatting."""

    @overload
    def format(self: "RootResponse[ToolkitT, None]", partial: Literal[True]) -> None:
        """Format the response into a `Partial[BaseModel]` (with optional fields).

        This is useful for when the stream is only partially consumed, in which case the
        structured output may only be partially available.
        """
        ...

    @overload
    def format(
        self: "RootResponse[ToolkitT, FormatT]", partial: Literal[True]
    ) -> Partial[FormatT]:
        """Format the response into a `Partial[BaseModel]` (with optional fields).

        This is useful for when the stream is only partially consumed, in which case the
        structured output may only be partially available.
        """
        ...

    @overload
    def format(
        self: "RootResponse[ToolkitT, None]", partial: Literal[False] = False
    ) -> None:
        """Overload when the format type is `None`."""
        ...

    @overload
    def format(
        self: "RootResponse[ToolkitT, FormatT]", partial: Literal[False] = False
    ) -> FormatT:
        """Overload when the format type is not `None`."""
        ...

    def format(self, partial: bool = False) -> FormatT | Partial[FormatT] | None:
        """Format the response according to the response format parser.

        Returns:
            The formatted response object of type FormatT.

        Raises:
            json.JSONDecodeError: If the response's textual content can't be parsed as
                JSON.
            pydantic.ValidationError: If the response's content fails validation for the
                format type.
        """
        if self.format_type is None or self.format_type is NoneType:
            return None

        if partial:
            raise NotImplementedError

        text = "".join(text.text for text in self.texts)

        json_text = _utils.extract_json_object(text)

        parsed_json = json.loads(json_text)
        return self.format_type.model_validate(parsed_json)

    def pretty(self) -> str:
        """Return a string representation of all response content.

        The response content will be represented in a way that emphasies clarity and
        readability, but may not include all metadata (like thinking signatures or tool
        call ids), and thus cannot be used to reconstruct the response. For example:

        **Thinking:**
          The user is asking a math problem. I should use the calculator tool.

        **Tool Call (calculator)** {'operation': 'mult', 'a': 1337, 'b': 4242}

        I am going to use the calculator and answer your question for you!
        """
        if not self.content:
            return "**[No Content]**"

        pretty_parts: list[str] = []
        for part in self.content:
            if isinstance(part, Text):
                pretty_parts.append(part.text)
            elif isinstance(part, ToolCall):
                pretty_parts.append(f"**ToolCall ({part.name}):** {part.args}")
            elif isinstance(part, Thinking):
                indented_thinking = "\n".join(
                    f"  {line}" for line in part.thinking.split("\n")
                )
                pretty_parts.append(f"**Thinking:**\n{indented_thinking}")
            else:
                pretty_parts.append(
                    f"[{type(part).__name__}: {str(part)}]"
                )  # pragma: no cover

        return "\n\n".join(pretty_parts)

    @property
    def model(self) -> "Model":
        """A `Model` with parameters matching this response."""
        from ..clients import get_client
        from ..models import _utils as _model_utils, get_model_from_context

        if context_model := get_model_from_context():
            return context_model

        return _model_utils.assumed_safe_llm_create(
            provider=self.provider,
            model_id=self.model_id,
            client=get_client(self.provider),
            params=self.params,
        )
