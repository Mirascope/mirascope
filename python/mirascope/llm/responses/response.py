"""Interfaces for LLM responses.

This module defines interfaces for the responses returned by language models,
including methods for formatting the response according to a specified format.
"""

from collections.abc import Sequence
from dataclasses import dataclass
from decimal import Decimal
from typing import TYPE_CHECKING, Any, Generic

from ..content import AssistantContent, Audio, Image, Thinking, ToolCall
from ..context import Context
from ..messages import Message
from .finish_reason import FinishReason
from .usage import Usage

if TYPE_CHECKING:
    from ..clients import (
        REGISTERED_LLMS,
    )

from ..context import DepsT
from ..formats import FormatT


@dataclass
class Response(Generic[DepsT, FormatT]):
    """The response generated by an LLM."""

    raw: Any
    """The raw response from the LLM."""

    model: REGISTERED_LLMS
    """The model identifier that generated the response (e.g. "openai:gpt-4", "anthropic:claude-3-5-sonnet-latet")."""

    args: dict[str, Any]
    """The arguments used to generate the response."""

    template: str | None
    """The string template used to define the messages array, if any."""

    messages: list[Message]
    """The complete message history, including the final LLM message."""

    context: Context[DepsT]
    """The context used to generate this response."""

    content: Sequence[AssistantContent]
    """The content generated by the LLM."""

    texts: Sequence[str]
    """The text content in the generated response, if any."""

    images: Sequence[Image]
    """The image content in the generated response, if any."""

    audios: Sequence[Audio]
    """The audio content in the generated response, if any."""

    thinkings: Sequence[Thinking]
    """The thinking content in the generated response, if any."""

    finish_reason: FinishReason
    """The reason why the LLM finished generating a response."""

    usage: Usage | None
    """The token usage statistics for the request to the LLM."""

    cost: Decimal | None
    """The cost of the request to the LLM, if available."""

    tool_calls: Sequence[ToolCall]
    """The tools the LLM wants called on its behalf, if any."""

    @property
    def tool_call(self) -> ToolCall | None:
        """Returns the first tool used in the response, if any."""
        raise NotImplementedError()

    @property
    def text(self) -> str | None:
        """Returns the first text in the response content, if any."""
        raise NotImplementedError()

    @property
    def image(self) -> Image | None:
        """Returns the first image in the response content, if any."""
        raise NotImplementedError()

    @property
    def audio(self) -> Audio | None:
        """Returns the first audio in the response content, if any."""
        raise NotImplementedError()

    @property
    def thinking(self) -> Thinking | None:
        """Returns the first thinking in the response content, if any."""
        raise NotImplementedError()

    def format(self) -> FormatT:
        """Format the response according to the response format parser.

        It will parse the response content according to the specified format (if present)
        and return a structured object. Returns None if there was no format.

        Returns:
            The formatted response object of type T.

        Raises:
            ValueError: If the response cannot be formatted according to the
                specified format.
        """
        raise NotImplementedError()

    def __repr__(self) -> str:
        """Return a string representation of all response content including embedded media.

        The resulting string includes all raw text directly, and includes placeholder
        representations for embedded media, eg {image: url=...} or {thinking: thoughts=...}

        Each content piece will be separated by newlines.
        """
        raise NotImplementedError()
