"""Concrete LLM response implementation."""

from collections.abc import Sequence
from typing import TYPE_CHECKING, Any

from ..content import Text, Thinking, ToolCall
from ..formatting import FormatT
from ..messages import AssistantMessage, Message
from ..types import NoneType
from .base_response import BaseResponse
from .finish_reason import FinishReason

if TYPE_CHECKING:
    from ..clients import Model, Provider


class Response(BaseResponse[FormatT]):
    """The response generated by an LLM."""

    def __init__(
        self,
        *,
        provider: "Provider",
        model: "Model",
        format: type[FormatT] = NoneType,
        input_messages: Sequence[Message],
        assistant_message: AssistantMessage,
        finish_reason: FinishReason | None,
        raw: Any,  # noqa: ANN401
    ) -> None:
        """Initialize a Response.

        Args:
            model: The model identifier that generated the response.
            input_messages: The message history before the final assistant message.
            assistant_message: The final assistant message containing the response content.
            raw: The raw response from the LLM.
            finish_reason: The reason why the LLM finished generating a response.
        """
        self.provider = provider
        self.model = model
        self.raw = raw
        self.finish_reason = finish_reason
        self.format_type = format

        self.messages = list(input_messages) + [assistant_message]
        self.content = assistant_message.content

        self.texts, self.tool_calls, self.thinkings = [], [], []
        for part in self.content:
            if isinstance(part, Text):
                self.texts.append(part)
            elif isinstance(part, ToolCall):
                self.tool_calls.append(part)
            elif isinstance(part, Thinking):
                self.thinkings.append(part)
            else:
                raise NotImplementedError
