"""Interfaces for LLM responses.

This module defines interfaces for the responses returned by language models,
including methods for formatting the response according to a specified format.
"""

from collections.abc import Sequence
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, Generic, Literal, overload

from ..content import AssistantContent, Audio, Image, Thinking, ToolCall
from ..formatting import FormatT, Partial
from ..messages import Message
from .finish_reason import FinishReason
from .usage import Usage

if TYPE_CHECKING:
    from ..clients import (
        REGISTERED_LLMS,
    )


@dataclass
class Response(Generic[FormatT]):
    """The response generated by an LLM."""

    raw: Any
    """The raw response from the LLM."""

    model: REGISTERED_LLMS
    """The model identifier that generated the response (e.g. "openai:gpt-4", "anthropic:claude-3-5-sonnet-latet")."""

    args: dict[str, Any]
    """The arguments used to generate the response."""

    template: str | None
    """The string template used to define the messages array, if any."""

    messages: list[Message]
    """The complete message history, including the final LLM message."""

    content: Sequence[AssistantContent]
    """The content generated by the LLM."""

    texts: Sequence[str]
    """The text content in the generated response, if any."""

    images: Sequence[Image]
    """The image content in the generated response, if any."""

    audios: Sequence[Audio]
    """The audio content in the generated response, if any."""

    thinkings: Sequence[Thinking]
    """The thinking content in the generated response, if any."""

    finish_reason: FinishReason
    """The reason why the LLM finished generating a response."""

    usage: Usage | None
    """The token usage statistics for the request to the LLM."""

    tool_calls: Sequence[ToolCall]
    """The tools the LLM wants called on its behalf, if any."""

    @property
    def text(self, delimiter: str = "\n") -> str:
        """Returns the response's textual content.

        If the response has multiple Text parts, they will be concatenated together
        using delimiter (default newline).

        If the response has no text content, an empty string is returned.
        """
        raise NotImplementedError()

    @overload
    def format(self, partial: Literal[True]) -> Partial[FormatT]:
        """Format the response into a Partial[BaseModel] (with optional fields).

        This is useful in the case where a partially-concluded stream has been
        converted to a response via `stream.to_response()`, in which case the
        structured output may only be partially available."""
        ...

    @overload
    def format(self, partial: Literal[False] = False) -> FormatT:
        """Format the response into a Pydantic BaseModel."""
        ...

    def format(self, partial: bool = False) -> FormatT | Partial[FormatT]:
        """Format the response according to the response format parser.

        It will parse the response content according to the specified format (if present)
        and return a structured object. Returns None if there was no format.

        When called with `partial=True`, it will return a partial of the model, with all
        fields optional.

        Returns:
            The formatted response object of type T.

        Raises:
            ValueError: If the response cannot be formatted according to the
                specified format.
        """
        raise NotImplementedError()

    def __repr__(self) -> str:
        """Return a string representation of all response content including embedded media.

        The resulting string includes all raw text directly, and includes placeholder
        representations for embedded media, eg {image: url=...} or {thinking: thoughts=...}

        Each content piece will be separated by newlines.
        """
        raise NotImplementedError()
