"""The `llm` module for writing provider-agnostic LLM Generations.

This module provides a unified interface for interacting with different LLM providers,
including messages, tools, response formatting, and streaming. It allows you to write
code that works with multiple LLM providers without changing your application logic.
"""

# TODO: Across the API, audit docstrings to ensure they are compliant Google-style docstrings
# (Write some tooling to ensure this happens consistently + in CI)

from . import (
    calls,
    clients,
    content,
    exceptions,
    formatting,
    mcp,
    messages,
    models,
    prompts,
    responses,
    tools,
    types,
)
from .calls import call
from .clients import ModelId, Params, Provider, client, get_client
from .content import (
    AssistantContentChunk,
    AssistantContentPart,
    Audio,
    Document,
    Image,
    Text,
    TextChunk,
    TextEndChunk,
    TextStartChunk,
    Thought,
    ThoughtChunk,
    ThoughtEndChunk,
    ThoughtStartChunk,
    ToolCall,
    ToolCallChunk,
    ToolCallEndChunk,
    ToolCallStartChunk,
    ToolOutput,
    UserContentPart,
)
from .context import Context
from .exceptions import (
    APIError,
    AuthenticationError,
    BadRequestError,
    ConnectionError,
    FeatureNotSupportedError,
    FormattingModeNotSupportedError,
    MirascopeError,
    NotFoundError,
    PermissionError,
    RateLimitError,
    ServerError,
    TimeoutError,
    ToolNotFoundError,
)
from .formatting import Format, FormattingMode, Partial, format
from .messages import (
    AssistantContent,
    AssistantMessage,
    Message,
    SystemContent,
    SystemMessage,
    UserContent,
    UserMessage,
)
from .models import Model, model
from .prompts import prompt
from .responses import (
    AsyncChunkIterator,
    AsyncContextResponse,
    AsyncContextStreamResponse,
    AsyncResponse,
    AsyncStream,
    AsyncStreamResponse,
    AsyncTextStream,
    AsyncThoughtStream,
    AsyncToolCallStream,
    ChunkIterator,
    ContextResponse,
    ContextStreamResponse,
    FinishReason,
    RawContentChunk,
    Response,
    Stream,
    StreamResponse,
    StreamResponseChunk,
    TextStream,
    ThoughtStream,
    ToolCallStream,
)
from .tools import (
    AsyncContextTool,
    AsyncContextToolkit,
    AsyncTool,
    AsyncToolkit,
    ContextTool,
    ContextToolkit,
    Tool,
    Toolkit,
    tool,
)

__all__ = [
    "APIError",
    "AssistantContent",
    "AssistantContentChunk",
    "AssistantContentPart",
    "AssistantMessage",
    "AsyncChunkIterator",
    "AsyncContextResponse",
    "AsyncContextStreamResponse",
    "AsyncContextTool",
    "AsyncContextToolkit",
    "AsyncResponse",
    "AsyncStream",
    "AsyncStreamResponse",
    "AsyncTextStream",
    "AsyncThoughtStream",
    "AsyncTool",
    "AsyncToolCallStream",
    "AsyncToolkit",
    "Audio",
    "AuthenticationError",
    "BadRequestError",
    "ChunkIterator",
    "ConnectionError",
    "Context",
    "ContextResponse",
    "ContextStreamResponse",
    "ContextTool",
    "ContextToolkit",
    "Document",
    "FeatureNotSupportedError",
    "FinishReason",
    "Format",
    "FormattingMode",
    "FormattingModeNotSupportedError",
    "Image",
    "Message",
    "MirascopeError",
    "Model",
    "NotFoundError",
    "Params",
    "Partial",
    "PermissionError",
    "RateLimitError",
    "RawContentChunk",
    "Response",
    "ServerError",
    "Stream",
    "StreamResponse",
    "StreamResponseChunk",
    "SystemContent",
    "SystemMessage",
    "Text",
    "TextChunk",
    "TextEndChunk",
    "TextStartChunk",
    "TextStream",
    "Thought",
    "ThoughtChunk",
    "ThoughtEndChunk",
    "ThoughtStartChunk",
    "ThoughtStream",
    "TimeoutError",
    "Tool",
    "ToolCall",
    "ToolCallChunk",
    "ToolCallEndChunk",
    "ToolCallStartChunk",
    "ToolCallStream",
    "ToolNotFoundError",
    "ToolOutput",
    "Toolkit",
    "UserContent",
    "UserContentPart",
    "UserMessage",
    "call",
    "calls",
    "client",
    "clients",
    "content",
    "exceptions",
    "format",
    "formatting",
    "get_client",
    "mcp",
    "messages",
    "model",
    "models",
    "prompt",
    "prompts",
    "responses",
    "tool",
    "tools",
    "types",
]
