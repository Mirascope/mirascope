"""The `llm.agents` module for implementing simple "Agent" functionality.

TODO: think of a good interface / way to enable query transformations as part of the
agent. The user query provided is part of the prompt, and sometimes you'll want to steer
pieces of that (e.g. let user's fill out pieces that you template like with calls). We
just need to enable overloading on a function that takes `P` and returns `Content` or
`Sequence[Content]`, essentially a middleware so the consumer of the agent can still
provide only the necessary template pieces rather than the full user content. Note this
is different from a chat interface where it's likely the user will be writing the entire
user message.
"""

from collections.abc import Sequence
from dataclasses import dataclass
from typing import TYPE_CHECKING, Generic, ParamSpec, Protocol, TypeAlias, overload

from typing_extensions import TypeVar, Unpack

from .context import Context
from .messages import Content
from .models import LLM, Client, Params
from .response_formatting import ResponseFormat
from .responses import Response
from .streams import AsyncStream, AsyncStructuredStream, Stream, StructuredStream
from .tools import ToolDef

if TYPE_CHECKING:
    from .models.anthropic import (
        ANTHROPIC_REGISTERED_LLMS,
        AnthropicClient,
        AnthropicParams,
    )
    from .models.context import REGISTERED_LLMS
    from .models.google import (
        GOOGLE_REGISTERED_LLMS,
        GoogleClient,
        GoogleParams,
    )
    from .models.openai import (
        OPENAI_REGISTERED_LLMS,
        OpenAIClient,
        OpenAIParams,
    )


NoneType = type(None)
P = ParamSpec("P")
DepsT = TypeVar("DepsT", default=None)
T = TypeVar("T", default=None)


class AgentStringReturn(Protocol[P, DepsT]):
    """Protocol for a prompt template function that returns a single string."""

    def __call__(
        self, ctx: Context[DepsT], *args: P.args, **kwargs: P.kwargs
    ) -> str: ...


class AsyncAgentStringReturn(Protocol[P, DepsT]):
    """Protocol for a prompt template function that returns a single string."""

    async def __call__(
        self, ctx: Context[DepsT], *args: P.args, **kwargs: P.kwargs
    ) -> str: ...


class AgentContentReturn(Protocol[P, DepsT]):
    """Protocol for a prompt template function that returns a single content part."""

    def __call__(
        self, ctx: Context[DepsT], *args: P.args, **kwargs: P.kwargs
    ) -> Content: ...


class AsyncAgentContentReturn(Protocol[P, DepsT]):
    """Protocol for a prompt template function that returns a single content part."""

    async def __call__(
        self, ctx: Context[DepsT], *args: P.args, **kwargs: P.kwargs
    ) -> Content: ...


class AgentContentSequenceReturn(Protocol[P, DepsT]):
    """Protocol for a prompt template function that returns a content parts sequence."""

    def __call__(
        self, ctx: Context[DepsT], *args: P.args, **kwargs: P.kwargs
    ) -> Sequence[Content]: ...


class AsyncAgentContentSequenceReturn(Protocol[P, DepsT]):
    """Protocol for a prompt template function that returns a content parts sequence."""

    async def __call__(
        self, ctx: Context[DepsT], *args: P.args, **kwargs: P.kwargs
    ) -> Sequence[Content]: ...


SystemPromptTemplate: TypeAlias = (
    AgentStringReturn[P, DepsT]
    | AgentContentReturn[P, DepsT]
    | AgentContentSequenceReturn[P, DepsT]
)
AsyncSystemPromptTemplate: TypeAlias = (
    AsyncAgentStringReturn[P, DepsT]
    | AsyncAgentContentReturn[P, DepsT]
    | AsyncAgentContentSequenceReturn[P, DepsT]
)


@dataclass
class Agent(Generic[DepsT]):
    """Agent class for generating responses using LLMs with tools."""

    ctx: Context[DepsT]
    """The context for the agent, such as the history of messages."""

    tools: Sequence[ToolDef] | None
    """The tools available to the agent, if any."""

    model: LLM
    """The default model the agent will use if not specified through context."""

    def __call__(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> Response:
        """Generates a response by running the agent loop."""
        raise NotImplementedError()

    async def run_async(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> Response:
        """Generates a response by running the agent loop asynchronously."""
        raise NotImplementedError()

    def stream(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> Stream:
        """Streams the response generated by running the agent loop."""
        raise NotImplementedError()

    async def stream_async(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> AsyncStream:
        """Streams the response generated by running the agent loop asynchronously."""
        raise NotImplementedError()


@dataclass
class AsyncAgent(Generic[DepsT]):
    """Asynchronous agent class for generating responses using LLMs with tools."""

    ctx: Context[DepsT]
    """The context for the agent, such as the history of messages."""

    tools: Sequence[ToolDef] | None
    """The tools available to the agent, if any."""

    model: LLM
    """The default model the agent will use if not specified through context."""

    async def __call__(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> Response:
        """Generates a response by running the agent loop asynchronously."""
        raise NotImplementedError()

    async def run_async(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> Response:
        """Generates a response by running the agent loop asynchronously."""
        return await self(query, deps=deps)

    async def stream(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> AsyncStream:
        """Streams the response generated by running the agent loop asynchronously."""
        raise NotImplementedError()

    async def stream_async(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> AsyncStream:
        """Streams the response generated by running the agent loop asynchronously."""
        return await self.stream(query, deps=deps)


@dataclass
class StructuredAgent(Generic[DepsT, T]):
    """Structured agent class for generating structured responses using LLMs with tools."""

    ctx: Context[DepsT]
    """The context for the agent, such as the history of messages."""

    response_format: ResponseFormat[T] | None
    """The response format for the agent, if any."""

    tools: Sequence[ToolDef] | None
    """The tools available to the agent, if any."""

    model: LLM
    """The default model the agent will use if not specified through context."""

    def __call__(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> Response[T]:
        """Generates a structured response by running the agent loop."""
        raise NotImplementedError()

    async def run_async(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> Response[T]:
        """Generates a structured response by running the agent loop asynchronously."""
        raise NotImplementedError()

    def stream(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> StructuredStream[T]:
        """Streams the structured response generated by running the agent loop."""
        raise NotImplementedError()

    async def stream_async(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> AsyncStructuredStream[T]:
        """Streams the structured response generated by running the agent loop asynchronously."""
        raise NotImplementedError()


@dataclass
class AsyncStructuredAgent(Generic[DepsT, T]):
    """Asynchronous structured agent class for generating structured responses using LLMs with tools."""

    ctx: Context[DepsT]
    """The context for the agent, such as the history of messages."""

    response_format: ResponseFormat[T] | None
    """The response format for the agent, if any."""

    tools: Sequence[ToolDef] | None
    """The tools available to the agent, if any."""

    model: LLM
    """The default model the agent will use if not specified through context."""

    async def __call__(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> Response[T]:
        """Generates a structured response by running the agent loop asynchronously."""
        raise NotImplementedError()

    async def call_async(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> Response[T]:
        """Generates a structured response by running the agent loop asynchronously."""
        return await self(query, deps=deps)

    async def stream(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> AsyncStructuredStream[T]:
        """Streams the structured response generated by running the agent loop asynchronously."""
        raise NotImplementedError()

    async def stream_async(
        self, query: Content | Sequence[Content], *, deps: DepsT = None
    ) -> AsyncStructuredStream[T]:
        """Streams the structured response generated by running the agent loop asynchronously."""
        return await self.stream(query, deps=deps)


class AgentDecorator(Protocol[DepsT]):
    """Protocol for the `agent` decorator."""

    @overload
    def __call__(self, fn: AsyncSystemPromptTemplate[P, DepsT]) -> Agent[DepsT]:
        """Decorator for creating an async only agent."""
        ...

    @overload
    def __call__(self, fn: SystemPromptTemplate[P, DepsT]) -> Agent[DepsT]:
        """Decorator for creating an agent."""
        ...

    def __call__(
        self, fn: SystemPromptTemplate[P, DepsT] | AsyncSystemPromptTemplate[P, DepsT]
    ) -> Agent[DepsT] | AsyncAgent[DepsT]:
        """Decorator for creating an agent."""
        ...


class StructuredAgentDecorator(Protocol[DepsT, T]):
    """Protocol for the `agent` decorator with a response format."""

    @overload
    def __call__(
        self, fn: AsyncSystemPromptTemplate[P, DepsT]
    ) -> StructuredAgent[DepsT, T]:
        """Decorator for creating an async only structured agent."""
        ...

    @overload
    def __call__(self, fn: SystemPromptTemplate[P, DepsT]) -> StructuredAgent[DepsT, T]:
        """Decorator for creating a structured agent."""
        ...

    def __call__(
        self,
        fn: SystemPromptTemplate[P, DepsT] | AsyncSystemPromptTemplate[P, DepsT],
    ) -> StructuredAgent[DepsT, T] | AsyncStructuredAgent[DepsT, T]:
        """Decorator for creating a structured agent."""
        ...


@overload
def agent(
    model: ANTHROPIC_REGISTERED_LLMS,
    *,
    deps_type: type[DepsT] = NoneType,
    tools: Sequence[ToolDef] | None = None,
    response_format: None = None,
    client: AnthropicClient | None = None,
    **params: Unpack[AnthropicParams],
) -> AgentDecorator[DepsT]:
    """Overload for Anthropic agents."""
    ...


@overload
def agent(
    model: ANTHROPIC_REGISTERED_LLMS,
    *,
    deps_type: type[DepsT] = NoneType,
    tools: Sequence[ToolDef] | None = None,
    response_format: ResponseFormat[T],
    client: AnthropicClient | None = None,
    **params: Unpack[AnthropicParams],
) -> StructuredAgentDecorator[DepsT, T]:
    """Overload for Anthropic agents with response format."""
    ...


@overload
def agent(
    model: GOOGLE_REGISTERED_LLMS,
    *,
    deps_type: type[DepsT] = NoneType,
    tools: Sequence[ToolDef] | None = None,
    response_format: None = None,
    client: GoogleClient | None = None,
    **params: Unpack[GoogleParams],
) -> AgentDecorator[DepsT]:
    """Overload for Google agents."""
    ...


@overload
def agent(
    model: GOOGLE_REGISTERED_LLMS,
    *,
    deps_type: type[DepsT] = NoneType,
    tools: Sequence[ToolDef] | None = None,
    response_format: ResponseFormat[T],
    client: GoogleClient | None = None,
    **params: Unpack[GoogleParams],
) -> StructuredAgentDecorator[DepsT, T]:
    """Overload for Google agents with response format."""
    ...


@overload
def agent(
    model: OPENAI_REGISTERED_LLMS,
    *,
    deps_type: type[DepsT] = NoneType,
    tools: Sequence[ToolDef] | None = None,
    response_format: None = None,
    client: OpenAIClient | None = None,
    **params: Unpack[OpenAIParams],
) -> AgentDecorator[DepsT]:
    """Overload for OpenAI agents."""
    ...


@overload
def agent(
    model: OPENAI_REGISTERED_LLMS,
    *,
    deps_type: type[DepsT] = NoneType,
    tools: Sequence[ToolDef] | None = None,
    response_format: ResponseFormat[T],
    client: OpenAIClient | None = None,
    **params: Unpack[OpenAIParams],
) -> StructuredAgentDecorator[DepsT, T]:
    """Overload for OpenAI agents with response format."""
    ...


@overload
def agent(
    model: REGISTERED_LLMS,
    *,
    deps_type: type[DepsT] = NoneType,
    tools: Sequence[ToolDef] | None = None,
    response_format: ResponseFormat[T] | None = None,
    client: Client | None = None,
    **params: Unpack[Params],
) -> AgentDecorator[DepsT] | StructuredAgentDecorator[DepsT, T]:
    """Overload for all registered models so that autocomplete works."""
    ...


def agent(
    model: REGISTERED_LLMS,
    *,
    deps_type: type[DepsT] = NoneType,
    tools: Sequence[ToolDef] | None = None,
    response_format: ResponseFormat[T] | None = None,
    client: Client | None = None,
    **params: Unpack[Params],
) -> AgentDecorator[DepsT] | StructuredAgentDecorator[DepsT, T]:
    """Decorator for creating an agent or structured agent.

    Args:
        model: The model to use for the agent.
        deps_type: The type of dependencies for the agent, injected into the context.
        tools: The tools available to the agent.
        response_format: The response format for the agent.
        client: The client to use for the agent.
        **params: Additional parameters for the model.

    Returns:
        An instance of `AgentDecorator` or `StructuredAgentDecorator`.
    """
    raise NotImplementedError()
