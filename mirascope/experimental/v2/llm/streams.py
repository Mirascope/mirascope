"""Interfaces for streaming responses from LLMs.

This module provides interfaces for both synchronous and asynchronous streaming
of responses from language models. Streaming allows for incremental processing of
responses as they are generated, rather than waiting for the complete response.

TODO: this interface is missing stuff from v1 like usage etc. that we collect during
the stream for convenience (e.g. calling stream.cost after the stream is done).
"""

from collections.abc import AsyncIterator, Iterator
from dataclasses import dataclass
from typing import Generic

from typing_extensions import TypeVar

from .messages import ResponseContent

T = TypeVar("T", default=None)


@dataclass
class StreamChunk:
    """A chunk of a streaming response from an LLM.

    Stream chunks represent incremental pieces of a response that are delivered
    as they are generated by the model, allowing for real-time display and processing.
    """

    content: ResponseContent
    """The content in this chunk of the response."""


class Stream:
    """A synchronous stream of response chunks from an LLM.

    This class supports iteration to process chunks as they arrive from the model.

    Example:
        ```python
        from mirascope import llm

        @llm.call("openai:gpt-4o-mini")
        def answer_question(question: str) -> str:
            return f"Answer this question: {question}"

        stream = answer_question.stream("What is the capital of France?")
        for chunk in stream:
            print(chunk.content, end="", flush=True)
        ```
    """

    def __iter__(self) -> Iterator[StreamChunk]:
        """Iterate through the chunks of the stream.

        Returns:
            An iterator yielding StreamChunk objects.
        """
        raise NotImplementedError()


class AsyncStream:
    """An asynchronous stream of response chunks from an LLM.

    This class supports async iteration to process chunks as they arrive from the model.

    Example:
        ```python
        from mirascope import llm

        @llm.call("openai:gpt-4o-mini")
        def answer_question(question: str) -> str:
            return f"Answer this question: {question}"

        stream = await answer_question.stream_async("What is the capital of France?")
        async for chunk in stream:
            print(chunk.content, end="", flush=True)
        ```
    """

    def __aiter__(self) -> AsyncIterator[StreamChunk]:
        """Iterate through the chunks of the stream asynchronously.

        Returns:
            An async iterator yielding StreamChunk objects.
        """
        raise NotImplementedError()


class Partial(Generic[T]):
    """A partial structured output from an LLM."""


class StructuredStream(Generic[T]):
    """A synchronous stream of partial structured outputs from an LLM.

    This class supports iteration to process structured outputs as they arrive from the
    model.

    Example:
        ```python
        from mirascope import llm

        @llm.response_format(parser="json")
        class Book:
            title: str
            author: str

        @llm.call("openai:gpt-4o-mini", response_format=Book)
        def answer_question(question: str) -> str:
            return f"Answer this question: {question}"

        stream = answer_question.stream("What is the capital of France?")
        for partial_book in stream:
            print(partial_book)
        ```
    """

    def __iter__(self) -> Iterator[Partial[T]]:
        """Iterate through the structured outputs of the stream.

        Returns:
            An iterator yielding structured output objects.
        """
        raise NotImplementedError()


class AsyncStructuredStream(Generic[T]):
    """An asynchronous stream of partial structured outputs from an LLM.

    This class supports async iteration to process structured outputs as they arrive
    from the model.

    Example:
        ```python
        from mirascope import llm

        @llm.response_format(parser="json")
        class Book:
            title: str
            author: str

        @llm.call("openai:gpt-4o-mini", response_format=Book)
        def answer_question(question: str) -> str:
            return f"Answer this question: {question}"

        stream = await answer_question.stream_async("What is the capital of France?")
        async for partial_book in stream:
            print(partial_book)
        ```
    """

    def __aiter__(self) -> AsyncIterator[Partial[T]]:
        """Iterate through the structured outputs of the stream asynchronously.

        Returns:
            An async iterator yielding structured output objects.
        """
        raise NotImplementedError()
