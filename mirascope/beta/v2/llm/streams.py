"""Interfaces for streaming responses from LLMs.

This module provides interfaces for both synchronous and asynchronous streaming
of responses from language models. Streaming allows for incremental processing of
responses as they are generated, rather than waiting for the complete response.
"""

from collections.abc import AsyncIterator, Iterator
from typing import Generic

from typing_extensions import TypeVar

T = TypeVar("T", default=None)


class StreamChunk(Generic[T]):
    """A chunk of a streaming response from an LLM.

    Stream chunks represent incremental pieces of a response that are delivered
    as they are generated by the model, allowing for real-time display and processing.
    """

    content: str
    """The text content in this chunk of the response."""

    index: int
    """The position of this chunk in the overall stream."""

    is_last: bool
    """Whether this is the final chunk in the stream."""

    def format(self) -> T:
        """Format the cumulative response up to this chunk according to the response format.

        Only available if the generation was created with a ResponseFormat[T].
        This method will only work reliably on the final chunk in most cases.

        Returns:
            The formatted response according to the specified ResponseFormat.
        """
        raise NotImplementedError()


class Stream(Generic[T]):
    """A synchronous stream of response chunks from an LLM.

    This class supports iteration to process chunks as they arrive from the model.

    Example:
        ```python
        from mirascope import llm

        with llm.model("openai:gpt-4"):
            stream = generation.stream(user_input)
            for chunk in stream:
                print(chunk.content, end="", flush=True)
        ```
    """

    def __iter__(self) -> Iterator[StreamChunk[T]]:
        """Iterate through the chunks of the stream.

        Returns:
            An iterator yielding StreamChunk objects.
        """
        raise NotImplementedError()

    def collect(self) -> T:
        """Collect the entire stream and return the formatted result.

        This method waits for all chunks to arrive and then formats
        the complete response.

        Returns:
            The formatted complete response.
        """
        raise NotImplementedError()


class AsyncStream(Generic[T]):
    """An asynchronous stream of response chunks from an LLM.

    This class supports async iteration to process chunks as they arrive from the model.

    Example:
        ```python
        from mirascope import llm

        async with llm.model("openai:gpt-4"):
            stream = await generation.astream(user_input)
            async for chunk in stream:
                print(chunk.content, end="", flush=True)
        ```
    """

    def __aiter__(self) -> AsyncIterator[StreamChunk[T]]:
        """Iterate through the chunks of the stream asynchronously.

        Returns:
            An async iterator yielding StreamChunk objects.
        """
        raise NotImplementedError()

    async def collect(self) -> T:
        """Collect the entire stream and return the formatted result asynchronously.

        This method waits for all chunks to arrive and then formats
        the complete response.

        Returns:
            The formatted complete response.
        """
        raise NotImplementedError()
