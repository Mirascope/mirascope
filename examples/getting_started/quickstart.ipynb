{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirascope Quickstart Guide\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "2. [Prompt Templates](#Prompt-Templates)\n",
    "3. [Basic LLM Call](#Basic-LLM-Call)\n",
    "4. [Streaming Responses](#Streaming-Responses)\n",
    "5. [Asynchronous Processing](#Asynchronous-Processing)\n",
    "6. [JSON Mode](#JSON-Mode)\n",
    "7. [Response Models](#Response-Models)\n",
    "8. [Output Parsers](#Output-Parsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Mirascope supports various LLM providers, including [OpenAI](https://openai.com/), [Anthropic](https://www.anthropic.com/), [Mistral](https://mistral.ai/), [Gemini](https://gemini.google.com), [Groq](https://groq.com/), [Cohere](https://cohere.com/), [LiteLLM](https://www.litellm.ai/), [Azure AI](https://azure.microsoft.com/en-us/solutions/ai), and [Vertex AI](https://cloud.google.com/vertex-ai). For the purposes of this guide, we will be using OpenAI. Let's start by installing Mirascope and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mirascope[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command installs Mirascope along with the necessary packages for the OpenAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "# Set the appropriate API key for the provider you're using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Prompt templates in Mirascope allow you to create dynamic and reusable prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:00:06.593677Z",
     "start_time": "2024-09-06T14:00:06.590420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from mirascope.core import prompt_template\n",
    "\n",
    "\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "def get_capital_prompt(country: str): ...\n",
    "\n",
    "\n",
    "print(get_capital_prompt(\"Japan\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. The `@prompt_template` decorator defines the structure of the prompt.\n",
    "2. `{country}` is a placeholder that gets replaced with the matching argument passed to the function.\n",
    "\n",
    "Here's an example of a multi-line prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:00:08.173878Z",
     "start_time": "2024-09-06T14:00:08.170209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    SYSTEM: You are a helpful assistant that provides information about {topic}.\n",
    "    USER: What are the key points to understand about {subtopic} in {topic}?\n",
    "    \"\"\"\n",
    ")\n",
    "def get_info_prompt(topic: str, subtopic: str): ...\n",
    "\n",
    "\n",
    "print(get_info_prompt(\"artificial intelligence\", \"machine learning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example:\n",
    "1. Includes \"SYSTEM\" and \"USER\" keywords to define messages with different roles in the prompt.\n",
    "2. Uses multiple variables (`{topic}` and `{subtopic}`) to create a more specific and dynamic prompt.\n",
    "\n",
    "Prompt templates improve code readability, maintain consistency in prompts, and make it easier to experiment with and modify prompts for effective prompt engineering.\n",
    "\n",
    "For more detailed information on prompt templates, check out our [Learn documentation on Prompts](https://docs.mirascope.io/learn/prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LLM Call\n",
    "\n",
    "The `call` decorator transforms functions with prompt templates into LLM API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:00:11.288096Z",
     "start_time": "2024-09-06T14:00:10.420541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Japan is Tokyo.\n"
     ]
    }
   ],
   "source": [
    "from mirascope.core import openai\n",
    "\n",
    "# import provider-specific modules to use those providers (e.g. anthropic, gemini, etc.)\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "def get_capital(country: str): ...\n",
    "\n",
    "\n",
    "response = get_capital(\"Japan\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced usage of LLM calls, refer to our [Learn documentation on Calls](https://docs.mirascope.io/learn/calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Streaming allows you to process LLM responses in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:00:16.508709Z",
     "start_time": "2024-09-06T14:00:13.203020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo, the capital city of Japan, is a vibrant metropolis known for its blend of traditional culture and cutting-edge modernity. With a population of over 13 million in the city proper and around 37 million in the Greater Tokyo Area, it is one of the most populous urban agglomerations in the world. Tokyo features a diverse range of neighborhoods, from the historic Asakusa with its ancient temples to the bustling shopping districts of Shibuya and Shinjuku.\n",
      "\n",
      "The city is renowned for its unique architecture, from contemporary skyscrapers to historic shrines and gardens. Tokyo offers a rich tapestry of cuisine, shopping, and entertainment, including world-class dining, bustling markets, and numerous parks. Its efficient public transportation system makes it easy to explore various attractions, such as the iconic Tokyo Tower, the peaceful Meiji Shrine, and the lively Akihabara district known for its otaku culture.\n",
      "\n",
      "Tokyo is also a major economic and cultural hub, hosting numerous international businesses, exhibitions, and events, making it a key player on the global stage."
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\", stream=True)\n",
    "@prompt_template(\"Provide a brief description of {city}.\")\n",
    "def stream_city_info(city: str): ...\n",
    "\n",
    "\n",
    "for chunk, _ in stream_city_info(\"Tokyo\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on streaming, including advanced techniques for processing streamed content, see our [Learn documentation on Streams](https://docs.mirascope.io/learn/streams)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Processing\n",
    "\n",
    "Mirascope supports asynchronous processing for efficient parallel execution of multiple LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:00:17.154267Z",
     "start_time": "2024-09-06T14:00:16.512749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is The capital of France is Paris.\n",
      "The capital of Japan is The capital of Japan is Tokyo.\n",
      "The capital of Brazil is The capital of Brazil is BrasÃ­lia.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "async def get_capital_async(country: str): ...\n",
    "\n",
    "\n",
    "async def main():\n",
    "    countries = [\"France\", \"Japan\", \"Brazil\"]\n",
    "    tasks = [get_capital_async(country) for country in countries]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for country, result in zip(countries, results, strict=False):\n",
    "        print(f\"The capital of {country} is {result.content}\")\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced usage of asynchronous processing, including async streaming, see our [Learn documentation on Async](https://docs.mirascope.io/learn/async)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode\n",
    "\n",
    "JSON mode allows you to directly parse LLM outputs as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:00:22.141802Z",
     "start_time": "2024-09-06T14:00:19.083102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"city\": \"Tokyo\",\n",
      "  \"country\": \"Japan\",\n",
      "  \"population\": 13929286,\n",
      "  \"area_km2\": 2191,\n",
      "  \"landmarks\": [\n",
      "    {\n",
      "      \"name\": \"Tokyo Tower\",\n",
      "      \"type\": \"Tower\",\n",
      "      \"height_m\": 333\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Senso-ji Temple\",\n",
      "      \"type\": \"Temple\",\n",
      "      \"location\": \"Asakusa\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Shibuya Crossing\",\n",
      "      \"type\": \"Intersection\",\n",
      "      \"description\": \"Famous pedestrian scramble\"\n",
      "    }\n",
      "  ],\n",
      "  \"transportation\": {\n",
      "    \"subway_system\": true,\n",
      "    \"major_airports\": [\n",
      "      {\n",
      "        \"name\": \"Narita International Airport\",\n",
      "        \"IATA_code\": \"NRT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Haneda Airport\",\n",
      "        \"IATA_code\": \"HND\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"culture\": {\n",
      "    \"cuisine\": [\n",
      "      \"Sushi\",\n",
      "      \"Ramen\",\n",
      "      \"Tempura\"\n",
      "    ],\n",
      "    \"traditional_events\": [\n",
      "      \"Cherry Blossom Festival\",\n",
      "      \"Gion Matsuri\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\", json_mode=True)\n",
    "@prompt_template(\"Provide information about {city} in JSON format\")\n",
    "def city_info(city: str): ...\n",
    "\n",
    "\n",
    "response = city_info(\"Tokyo\")\n",
    "print(response.content)  # This will be a JSON-formatted string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that not all providers have an explicit JSON mode. For those providers, we attempt to instruct the model to provide JSON; however, there is no guarantee that it will output only JSON (it may start with some text like \"Here is the JSON: ...\"). This is where Output Parsers can be useful.\n",
    "\n",
    "For more information on JSON mode, refer to our [Learn documentation on JSON Mode](https://docs.mirascope.io/learn/json-mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Models\n",
    "\n",
    "Response models allow you to structure and validate the output from LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:00:25.412924Z",
     "start_time": "2024-09-06T14:00:24.661722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Japan is Tokyo\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Capital(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", response_model=Capital)\n",
    "@prompt_template(\"Provide the capital of {country}\")\n",
    "def get_capital_info(country: str): ...\n",
    "\n",
    "\n",
    "result = get_capital_info(\"Japan\")\n",
    "print(f\"The capital of {result.country} is {result.city}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on response models, including advanced validation techniques, check our [Learn documentation on Response Models](https://docs.mirascope.io/learn/response-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output parsers allow you to process LLM responses in custom formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:00:28.482626Z",
     "start_time": "2024-09-06T14:00:27.777878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country_info=CountryInfo(country='Australia', capital='Canberra')\n",
      "The capital of Australia is Canberra\n"
     ]
    }
   ],
   "source": [
    "class CountryInfo(BaseModel):\n",
    "    country: str\n",
    "    capital: str\n",
    "\n",
    "\n",
    "def parse_country_and_capital(response: openai.OpenAICallResponse) -> CountryInfo:\n",
    "    country, capital = response.content.split(\" -> \")\n",
    "    return CountryInfo(country=country, capital=capital)\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", output_parser=parse_country_and_capital)\n",
    "@prompt_template(\n",
    "    \"What is the capital of {country}? Your output MUST follow the format 'country -> capital'.\"\n",
    ")\n",
    "def country_and_capital(country: str): ...\n",
    "\n",
    "\n",
    "country_info = country_and_capital(\"Australia\")\n",
    "print(f\"{country_info=}\")\n",
    "print(f\"The capital of {country_info.country} is {country_info.capital}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on output parsers, see our [Learn documentation on Output Parsers](https://docs.mirascope.io/learn/output-parsers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our Quickstart Guide to Mirascope. We've covered the main features of the library, including prompt templates, basic calls, streaming, asynchronous processing, JSON mode, response models, and output parsers. Each of these features can be combined and customized to create powerful, flexible AI applications.\n",
    "\n",
    "For more detailed information on each of these topics and advanced usage, including dynamic configuration and chaining, please refer to our comprehensive [Learn documentation](https://docs.mirascope.io/learn)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
