{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirascope Quickstart Guide\n",
    "\n",
    "This guide provides a comprehensive introduction to the key features of the Mirascope library, designed for developers who are new to working with Large Language Models (LLMs). Each section includes detailed explanations, code examples, and practical advice.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#Setup)\n",
    "2. [Prompt Templates (prompt)](#Prompt-Templates)\n",
    "3. [Basic LLM Call (call)](#Basic-LLM-Call)\n",
    "4. [Tools](#Tools)\n",
    "5. [Asynchronous Processing (Async)](#Asynchronous-Processing)\n",
    "6. [Streaming Responses (streams)](#Streaming-Responses)\n",
    "7. [Response Models (response_model)](#Response-Models)\n",
    "8. [Dynamic Configuration & Chaining](#Dynamic-Configuration-and-Chaining)\n",
    "9. [JSON Mode](#JSON-Mode)\n",
    "10. [Output Parsers](#Output-Parsers)\n",
    "\n",
    "Let's dive into each of these features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Mirascope supports integration with various LLM providers including OpenAI, Anthropic, and Gemini. Let's start by installing Mirascope and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mirascope[openai]\"\n",
    "\n",
    "# Additional installation for other providers\n",
    "# !pip install \"mirascope[anthropic, gemini]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command installs Mirascope with OpenAI integration. For other providers, you may need to install additional extras (e.g., `pip install mirascope[anthropic]`).\n",
    "\n",
    "Next, let's import the necessary modules and set up the API keys for the providers we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mirascope.core import openai, prompt_template\n",
    "\n",
    "# Import additional modules for other providers\n",
    "# from mirascope.core import anthropic, gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appropriate API key for the provider you're using\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key-here\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"your-google-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting API keys as environment variables is recommended for better security and portability across different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "The `prompt_template` decorator in Mirascope is a powerful tool for creating dynamic and reusable prompts. It makes prompt engineering more flexible and manageable.\n",
    "\n",
    "Here's a basic example of how to define a prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "def get_capital_prompt(country: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "# This function doesn't actually call an LLM yet, it just defines the prompt\n",
    "print(get_capital_prompt(\"Japan\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. The `@prompt_template` decorator defines the structure of the prompt.\n",
    "2. `{country}` is a placeholder that gets replaced with the argument passed to the function.\n",
    "3. The actual function body (`pass`) is ignored; only the prompt structure is defined.\n",
    "\n",
    "You can use more complex prompts with multiple variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@prompt_template(\"\"\"\n",
    "SYSTEM: You are a helpful assistant that provides information about {topic}.\n",
    "USER: What are the key points to understand about {subtopic} in {topic}?\n",
    "\"\"\")\n",
    "def get_info_prompt(topic: str, subtopic: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "print(get_info_prompt(\"artificial intelligence\", \"machine learning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This advanced example:\n",
    "1. Includes \"SYSTEM\" and \"USER\" roles in the prompt, providing different contexts.\n",
    "2. Uses multiple variables (`{topic}` and `{subtopic}`) to create a more specific and dynamic prompt.\n",
    "\n",
    "Prompt templates improve code readability, maintain consistency in prompts, and make it easier to experiment with and modify prompts for effective prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LLM Call\n",
    "\n",
    "Now that we understand prompt templates, let's see how to use them with the `call` decorator to actually interact with an LLM. The `call` decorator in Mirascope transforms regular Python functions with prompt templates into LLM API calls.\n",
    "\n",
    "Here's a basic example combining a prompt template with an LLM call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "def get_capital(country: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "response = get_capital(\"Japan\")\n",
    "print(response.content)\n",
    "\n",
    "# Additional response information\n",
    "print(f\"Model used: {response.model}\")\n",
    "print(f\"Token usage: {response.usage}\")\n",
    "print(f\"Finish reason: {response.finish_reasons}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. The `@prompt_template` decorator defines the structure of the prompt.\n",
    "2. The `@openai.call(\"gpt-4o-mini\")` decorator transforms the function into an LLM call using OpenAI's GPT-4 model.\n",
    "3. When the function is called, it combines the prompt template with the provided arguments and sends the resulting prompt to the LLM.\n",
    "4. The function returns a response object containing the LLM's output and additional metadata.\n",
    "\n",
    "This combination of prompt templates and the call decorator makes it easy to create flexible and reusable LLM interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Tools in Mirascope allow you to extend the capabilities of LLMs by providing access to custom functionality. This feature significantly enhances the LLM's ability to perform complex tasks or generate specific outputs.\n",
    "\n",
    "Here's an example of how to define and use a custom tool, continuing with our capital city theme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mirascope.core import BaseTool, openai, prompt_template\n",
    "\n",
    "\n",
    "class FormatCapital(BaseTool):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "    def call(self) -> str:\n",
    "        return f\"The capital of {self.country} is {self.city}.\"\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", tools=[FormatCapital])\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "def get_capital(country: str): ...\n",
    "\n",
    "\n",
    "response = get_capital(\"France\")\n",
    "if tools := response.tools:\n",
    "    for tool in tools:\n",
    "        print(tool.call())\n",
    "else:\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this example:\n",
    "\n",
    "1. We define a custom tool `FormatCapital` that inherits from `BaseTool`. This tool takes a city and country, and formats them into a sentence stating the capital.\n",
    "\n",
    "2. The `FormatCapital` class has two attributes (`city` and `country`) and a `call` method that returns the formatted string.\n",
    "\n",
    "3. In the `get_capital` function, we use the `@openai.call` decorator and include our `FormatCapital` tool in the `tools` parameter.\n",
    "\n",
    "4. The LLM can now use this tool when generating its response. If it decides to use the tool, it will provide values for `city` and `country`.\n",
    "\n",
    "5. When we call `get_capital`, we check if any tools were used in the response. If so, we call each tool and print its output. Otherwise, we print the direct content of the response.\n",
    "\n",
    "This approach allows the LLM to generate structured data (in this case, a capital city and its country) in a format that's easy for your application to process.\n",
    "\n",
    "Tools can be used for various purposes, such as:\n",
    "- Formatting output in a specific way\n",
    "- Performing calculations (e.g., population density given area and population)\n",
    "- Accessing external data sources (e.g., fetching up-to-date information about a city)\n",
    "- Implementing custom logic that the LLM can leverage (e.g., determining the continent based on the country)\n",
    "\n",
    "By providing tools, you give the LLM additional capabilities that it can use to generate more accurate, formatted, or context-aware responses. In this case, the `FormatCapital` tool ensures that the capital information is always presented in a consistent format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Processing\n",
    "\n",
    "Mirascope supports asynchronous processing, allowing for efficient parallel execution of multiple LLM calls. This is particularly useful when handling numerous queries or when you need to minimize response times.\n",
    "\n",
    "Here's a basic example of asynchronous calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "async def get_capital_async(country: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "async def main():\n",
    "    countries = [\"France\", \"Japan\", \"Brazil\"]\n",
    "    tasks = [get_capital_async(country) for country in countries]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for country, result in zip(countries, results, strict=False):\n",
    "        print(f\"The capital of {country} is {result.content}\")\n",
    "\n",
    "\n",
    "# Use asyncio.run() to execute in the main thread\n",
    "# asyncio.run(main())\n",
    "\n",
    "# Use the following code if running in a Jupyter notebook\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code:\n",
    "1. Defines an asynchronous function using `async def`.\n",
    "2. Uses `asyncio.gather()` to run multiple asynchronous tasks in parallel.\n",
    "3. Processes the results in order, but the API calls themselves are made concurrently, reducing overall execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Streaming allows you to process LLM responses in real-time. This is particularly useful for generating longer responses or providing real-time feedback to users. Let's see how we can use streaming with our capital city example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@openai.call(\"gpt-4o-mini\", stream=True)\n",
    "@prompt_template(\n",
    "    \"Provide detailed information about the capital of {country}. Include facts about its history, culture, and significant landmarks.\"\n",
    ")\n",
    "def stream_capital_info(country: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "for chunk, _ in stream_capital_info(\"France\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "1. We enable streaming mode with the `stream=True` parameter in the `@openai.call` decorator.\n",
    "2. Our prompt asks for detailed information about a capital city, which is likely to generate a longer response.\n",
    "3. The response is returned in small chunks that can be processed immediately.\n",
    "4. We use `end=\"\"` and `flush=True` to display the output immediately without line breaks, creating a smooth streaming effect.\n",
    "\n",
    "This approach has several advantages:\n",
    "\n",
    "1. **Responsiveness**: Users see the beginning of the response immediately, rather than waiting for the entire response to be generated.\n",
    "2. **Handling Long Outputs**: For detailed information like this, streaming allows you to start processing or displaying the response before it's fully generated.\n",
    "3. **Real-time Interaction**: You could potentially process the chunks as they come in, allowing for real-time analysis or formatting of the response.\n",
    "\n",
    "For example, you could modify the processing to format the output in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def format_capital_info(chunk: str) -> str:\n",
    "    # Convert \"History:\" (and similar headers) to bold\n",
    "    chunk = re.sub(r\"(\\w+):\", r\"**\\1:**\", chunk)\n",
    "    # Add bullet points to lists\n",
    "    chunk = re.sub(r\"^(\\s*)(\\d+\\.)\", r\"\\1- \", chunk, flags=re.MULTILINE)\n",
    "    return chunk\n",
    "\n",
    "\n",
    "for chunk, _ in stream_capital_info(\"Japan\"):\n",
    "    formatted_chunk = format_capital_info(chunk.content)\n",
    "    print(formatted_chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modification formats the streamed content in real-time, bold-ing headers and converting numbered lists to bullet points. This demonstrates how streaming can be combined with real-time processing to enhance the presentation of information as it's generated.\n",
    "\n",
    "Streaming is particularly useful when dealing with longer, more detailed responses, such as comprehensive information about capital cities. It allows for a more interactive and responsive user experience, especially in applications where immediate feedback is valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Models\n",
    "\n",
    "Response models allow you to structure and validate the output from LLMs. This feature enhances type safety and makes data manipulation easier.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "\n",
    "class Capital(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "    population: int\n",
    "\n",
    "    @field_validator(\"population\")\n",
    "    @classmethod\n",
    "    def population_must_be_positive(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"Population must be positive\")\n",
    "        return v\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", response_model=Capital)\n",
    "@prompt_template(\"Provide information about the capital of {country}\")\n",
    "def get_capital_info(country: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    result = get_capital_info(\"Japan\")\n",
    "    print(f\"The capital of {result.country} is {result.city}\")\n",
    "    print(f\"Population: {result.population}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. We define a `Capital` model using Pydantic, specifying the structure and types of the expected data.\n",
    "2. We use a field validator to ensure the population is always positive.\n",
    "3. The `response_model` parameter in the `@openai.call` decorator tells Mirascope to parse the LLM's output into this structure.\n",
    "4. We can then access the structured data directly, with type checking and validation already performed.\n",
    "\n",
    "Response models are particularly useful when you need consistent, structured output from your LLM calls, especially for data-driven applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Configuration & Chaining\n",
    "\n",
    "Dynamic configuration allows you to modify LLM call settings at runtime, while chaining enables you to create sequences of LLM calls.\n",
    "\n",
    "Here's an example of dynamic configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"Translate the following to {language}: {text}\")\n",
    "def translate(text: str, language: str):\n",
    "    return {\"temperature\": 0.7 if language == \"French\" else 0.5}\n",
    "\n",
    "\n",
    "response = translate(\"Hello, world!\", \"French\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're dynamically setting the `temperature` parameter based on the target language.\n",
    "\n",
    "Now, let's look at an example of chaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"Summarize the following text: {text}\")\n",
    "def summarize(text: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"Translate the following to French: {text}\")\n",
    "def translate_to_french(text: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "original_text = \"Recently, it's been really hot, so I’m thinking of gathering with friends to enjoy some champagne outdoors. I’m looking for something refreshing and light, especially a champagne that’s enjoyable even on a hot day. If possible, I’d love to know about any recommended champagne that’s perfect for relaxing with everyone.\"\n",
    "summary = summarize(original_text)\n",
    "french_summary = translate_to_french(summary.content)\n",
    "print(french_summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain first summarizes a text, then translates the summary to French, demonstrating how you can combine multiple LLM calls to perform more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode\n",
    "\n",
    "JSON mode allows you to directly parse LLM outputs as JSON. This is useful when you need structured data output.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@openai.call(\"gpt-4o-mini\", json_mode=True)\n",
    "@prompt_template(\"Provide information about {city} in JSON format\")\n",
    "def city_info(city: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "response = city_info(\"Tokyo\")\n",
    "print(response.content)  # This will be a JSON-formatted string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `json_mode=True`, we instruct the LLM to format its response as JSON, which can then be easily parsed and used in your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output parsers allow you to process LLM responses in custom formats. This is useful when you need to extract specific information or transform the output in a particular way.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountryInfo(BaseModel):\n",
    "    country: str\n",
    "    capital: str\n",
    "\n",
    "\n",
    "def parse_country_and_capital(response: openai.OpenAICallResponse) -> CountryInfo:\n",
    "    country, capital = response.content.split(\" -> \")\n",
    "    return CountryInfo(country=country, capital=capital)\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", output_parser=parse_country_and_capital)\n",
    "@prompt_template(\"What is the capital of {country}? the format 'country -> capital'.\")\n",
    "def country_and_capital(country: str): ...\n",
    "\n",
    "\n",
    "country_info = country_and_capital(\"Australia\")\n",
    "print(f\"{country_info=}\")\n",
    "print(f\"The capital of {country_info.country} is {country_info.capital}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a custom parser that splits the LLM's output into a list. The `output_parser` parameter in the `@openai.call` decorator applies this parser to the LLM's response.\n",
    "\n",
    "This concludes our Quickstart Guide to Mirascope. We've covered the main features of the library, including basic calls, prompt templates, tools, asynchronous processing, streaming, response models, dynamic configuration, chaining, JSON mode, and output parsers. Each of these features can be combined and customized to create powerful, flexible AI applications. As you continue to work with Mirascope, you'll discover even more ways to leverage these capabilities in your projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
