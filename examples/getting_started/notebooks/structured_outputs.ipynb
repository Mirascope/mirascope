{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Structured Outputs in Mirascope\n",
    "\n",
    "This notebook provides a detailed introduction to implementing evaluations using Mirascope, focusing on using LLMs as judges and implementing hardcoded evaluation criteria.\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Extracting Structured Outputs](#Extracting-Structured-Outputs)\n",
    "4. [Generating Structured Outputs](#Generating-Structured-Outputs)\n",
    "5. [JSON Mode](#JSON-Mode)\n",
    "6. [Few-Shot Examples](#Few-Shot-Examples)\n",
    "7. [Validating Outputs](#Validating-Outputs)\n",
    "8. [Output Parsers](#Output-Parsers)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Large Language Models (LLMs) generate unstructured text data by default. Structured outputs are essential for building reliable and efficient AI applications, and this notebook demonstrates various techniques for structuring LLM outputs using Mirascope.\n",
    "\n",
    "These methods help ensure consistency, type safety, and easier integration of LLM responses into your application. For more detailed information on structured outputs in Mirascope, refer to the [Response Models](https://docs.mirascope.io/latest/learn/response_models) documentation, [JSON Mode](https://docs.mirascope.io/latest/learn/json_mode) documentation, and [Output Parser](https://docs.mirascope.io/latest/learn/output_parsers) documentation.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up our environment by installing Mirascope and importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:49:47.641064Z",
     "start_time": "2024-09-10T06:49:46.883736Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"mirascope[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Structured Outputs\n",
    "\n",
    "The simplest way to extract structured outputs with Mirascope is using `response_model` to define the output type of the call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='The Name of the Wind' author='Patrick Rothfuss'\n"
     ]
    }
   ],
   "source": [
    "from mirascope.core import openai, prompt_template\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", response_model=Book)\n",
    "@prompt_template()\n",
    "def extract_book(text: str) -> str:\n",
    "    return f\"Extract the book from this text: {text}\"\n",
    "\n",
    "\n",
    "book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n",
    "assert isinstance(book, Book)\n",
    "print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are:\n",
    "1. Defining a response model `Book` as a Pydantic `BaseModel` subclass\n",
    "2. Setting `response_model` equal to our `Book` type.\n",
    "3. Running our LLM API call function as normal, except the output is now a `Book` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Structured Outputs\n",
    "\n",
    "Another common use-case for structured outputs is to generate synthetic data. The interface is the same, requiring only an update to the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='The Name of the Wind' author='Patrick Rothfuss'\n",
      "title='Mistborn: The Final Empire' author='Brandon Sanderson'\n",
      "title='A Darker Shade of Magic' author='V.E. Schwab'\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\", response_model=list[Book])\n",
    "@prompt_template()\n",
    "def recommend_books(genre: str, num: int) -> str:\n",
    "    return f\"Recommend a list of {num} {genre} books\"\n",
    "\n",
    "\n",
    "books = recommend_books(\"fantasy\", 3)\n",
    "for book in books:\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we:\n",
    "1. Updated our prompt to instruct to model to \"recommend\" (i.e. generate) books.\n",
    "2. We set `response_model` equal to `list[Book]` to output multiple books instead of just one.\n",
    "3. We further updated our prompt to enable the user to specify how many books to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode\n",
    "\n",
    "Many LLM providers have JSON Mode to instruct the model to output JSON. Although not all providers offer JSON Mode support officially, Mirascope offers support for all providers. For providers with official support, we simply use the native API feature. For providers without official support, we prompt engineer the model to give us the JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Name of the Wind', 'author': 'Patrick Rothfuss'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", json_mode=True)\n",
    "@prompt_template()\n",
    "def extract_book(text: str) -> str:\n",
    "    return f\"Extract the book title and author from this text: {text}\"\n",
    "\n",
    "\n",
    "response = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n",
    "print(json.loads(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we:\n",
    "1. Set `json_mode=True` to signal we want to use JSON Mode\n",
    "2. Specify the fields that we want in the prompt\n",
    "3. Parse the json string output into a Python dictionary\n",
    "\n",
    "If you want additional validation on the output structure and types, you can use `json_mode` in conjunction with `response_model` to validate your structured outputs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='The Name of the Wind' author='Patrick Rothfuss'\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\n",
    "@prompt_template()\n",
    "def extract_book(text: str) -> str:\n",
    "    return f\"Extract the book from this text: {text}\"\n",
    "\n",
    "\n",
    "book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n",
    "assert isinstance(book, Book)\n",
    "print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Examples\n",
    "\n",
    "Often when guiding an LLM's response, providing few-shot examples can greatly help steer the output in the right direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='THE LORD OF THE RINGS' author='Tolkien, J.R.R.'\n",
      "title='MISTBORN: THE FINAL EMPIRE' author='Sanderson, Brandon'\n",
      "title='GARDENS OF THE MOON' author='Erikson, Steven'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field, ConfigDict\n",
    "\n",
    "\n",
    "class FewShotBook(BaseModel):\n",
    "    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n",
    "    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        json_schema_extra={\n",
    "            \"examples\": [\n",
    "                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"},\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", response_model=list[FewShotBook], json_mode=True)\n",
    "@prompt_template()\n",
    "def recommend_few_shot_books(genre: str, num: int) -> str:\n",
    "    return f\"Recommend a list of {num} {genre} books. Match example format.\"\n",
    "\n",
    "\n",
    "books = recommend_few_shot_books(\"fantasy\", 3)\n",
    "for book in books:\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we:\n",
    "1. Added a few-shot example to each field in our response model.\n",
    "2. Added a few-shot example for the entire response model.\n",
    "3. Set `json_mode=True` because we have found that examples are more effective with this setting.\n",
    "4. Updated the prompt to instruct the LLM to match the format of the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Outputs\n",
    "\n",
    "Since `response_model` relies on Pydantic `BaseModel` types, you can easily add additional validation criteria to your model. This ensures the validation will run on every call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for ValidatedBook\ntitle\n  Assertion failed, All fields must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\nauthor\n  Assertion failed, All fields must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/assertion_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@openai\u001b[39m\u001b[38;5;241m.\u001b[39mcall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, response_model\u001b[38;5;241m=\u001b[39mValidatedBook)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@prompt_template\u001b[39m()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_all_caps_book\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtract the book from this text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m book \u001b[38;5;241m=\u001b[39m \u001b[43mextract_all_caps_book\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe Name of the Wind by Patrick Rothfuss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(book)\n",
      "File \u001b[0;32m~/Mirascope/GitHub/mirascope/mirascope/core/base/_extract.py:132\u001b[0m, in \u001b[0;36mextract_factory.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    131\u001b[0m     e\u001b[38;5;241m.\u001b[39m_response \u001b[38;5;241m=\u001b[39m call_response  \u001b[38;5;66;03m# pyright: ignore [reportAttributeAccessIssue]\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, BaseModel):\n\u001b[1;32m    134\u001b[0m     output\u001b[38;5;241m.\u001b[39m_response \u001b[38;5;241m=\u001b[39m call_response  \u001b[38;5;66;03m# pyright: ignore [reportAttributeAccessIssue]\u001b[39;00m\n",
      "File \u001b[0;32m~/Mirascope/GitHub/mirascope/mirascope/core/base/_extract.py:129\u001b[0m, in \u001b[0;36mextract_factory.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m json_output \u001b[38;5;241m=\u001b[39m get_json_output(call_response, json_mode)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mextract_tool_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    131\u001b[0m     e\u001b[38;5;241m.\u001b[39m_response \u001b[38;5;241m=\u001b[39m call_response  \u001b[38;5;66;03m# pyright: ignore [reportAttributeAccessIssue]\u001b[39;00m\n",
      "File \u001b[0;32m~/Mirascope/GitHub/mirascope/mirascope/core/base/_utils/_extract_tool_return.py:38\u001b[0m, in \u001b[0;36mextract_tool_return\u001b[0;34m(response_model, json_output, allow_partial)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m allow_partial:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m partial(response_model)\u001b[38;5;241m.\u001b[39mmodel_validate(json_obj)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Mirascope/GitHub/mirascope/.venv/lib/python3.10/site-packages/pydantic/main.py:551\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    550\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for ValidatedBook\ntitle\n  Assertion failed, All fields must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\nauthor\n  Assertion failed, All fields must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/assertion_error"
     ]
    }
   ],
   "source": [
    "from pydantic import field_validator\n",
    "\n",
    "\n",
    "class ValidatedBook(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "\n",
    "    @field_validator(\"title\", \"author\")\n",
    "    @classmethod\n",
    "    def must_be_uppercase(cls, v: str) -> str:\n",
    "        assert v.isupper(), \"All fields must be uppercase\"\n",
    "        return v\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", response_model=ValidatedBook)\n",
    "@prompt_template()\n",
    "def extract_all_caps_book(text: str) -> str:\n",
    "    return f\"Extract the book from this text: {text}\"\n",
    "\n",
    "\n",
    "book = extract_all_caps_book(\"The Name of the Wind by Patrick Rothfuss\")\n",
    "print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in this example that the model threw a `ValidationError` on construction because the extracted `title` and `author` fields were not all caps.\n",
    "\n",
    "This example is of course for demonstration purposes. In a real-world example we would ensure we catch such errors and handle them gracefully as well as further engineer the prompt to ensure such errors occur rarely or not at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Mirascope also provides an `output_parser` argument that will run on the call response for every call. This enables writing prompts that have a more specific structure (such as Chain of Thought) while still ensuring the output has the desired structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking: To solve the problem, I need to understand how many tennis balls are in a tube. Typically, a standard tube of tennis balls contains 3 balls. Since Roger starts with 2 tennis balls and then buys 3 tubes, I can calculate how many balls he gets from the tubes and then add that to the 2 he already has. First, I'll calculate the total number of tennis balls from the 3 tubes, which is 3 balls per tube multiplied by 3 tubes. Then, Iâ€™ll add the 2 balls he started with.\n",
      "Output: Roger has a total of 11 tennis balls. (2 starting balls + 9 from 3 tubes)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def parse_cot(response: openai.OpenAICallResponse) -> tuple[str, str]:\n",
    "    pattern = r\"<thinking>(.*?)</thinking>.*?<output>(.*?)</output>\"\n",
    "    match = re.search(pattern, response.content, re.DOTALL)\n",
    "    if not match:\n",
    "        return \"\", response.content\n",
    "    else:\n",
    "        return match.group(1).strip(), match.group(2).strip()\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", output_parser=parse_cot)\n",
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    SYSTEM:\n",
    "    First, output your thought process in <thinking> tags.\n",
    "    Then, provide your final output in <output> tags.\n",
    "\n",
    "    USER: {query}\n",
    "    \"\"\"\n",
    ")\n",
    "def cot(query: str): ...\n",
    "\n",
    "\n",
    "output = cot(\n",
    "    \"How many tennis balls does Roger have if he started with 2 and bought 3 tubes?\"\n",
    ")\n",
    "print(f\"Thinking: {output[0]}\")\n",
    "print(f\"Output: {output[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These techniques provide a solid foundation for structuring outputs in your LLM applications. As you continue to work with LLMs and develop more complex applications, robust prompt engineering and validation will be crucial for ensuring the quality and reliability of your models and outputs.\n",
    "\n",
    "For more advanced topics and best practices, refer to the Mirascope [Response Models](https://docs.mirascope.io/latest/learn/response_models) documentation, [JSON Mode](https://docs.mirascope.io/latest/learn/json_mode) documentation, and [Output Parsers](https://docs.mirascope.io/latest/learn/output_parsers) documentation.\n",
    "\n",
    "We also recommend taking a look at our [Tenacity Integration](https://docs.mirascope.io/latest/integrations/tenacity/) to learn how to build more robust pipelines that automatically re-insert validation errors into a subsequent call, enabling the LLM to learn from its mistakes and (hopefully) output the correct answer on the next attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
