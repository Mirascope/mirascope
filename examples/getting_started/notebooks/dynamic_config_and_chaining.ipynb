{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Configuration and Chaining in Mirascope\n",
    "\n",
    "This notebook provides a detailed introduction to using Dynamic Configuration and Chaining in Mirascope. We'll cover various examples ranging from basic usage to more complex chaining techniques.\n",
    "\n",
    "For more information on these topics, refer to the following documentation:\n",
    "- [Dynamic Configuration](https://docs.mirascope.io/learn/dynamic_configuration/)\n",
    "- [Chaining](https://docs.mirascope.io/learn/chaining/)\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "2. [Dynamic Configuration](#Dynamic-Configuration)\n",
    "   - [Basic Usage](#Basic-Usage)\n",
    "   - [Computed Fields](#Computed-Fields)\n",
    "   - [Dynamic Tools](#Dynamic-Tools)\n",
    "3. [Chaining](#Chaining)\n",
    "   - [Function-based Chaining](#Function-based-Chaining)\n",
    "   - [Chaining with Computed Fields](#Chaining-with-Computed-Fields)\n",
    "4. [Advanced Techniques](#Advanced-Techniques)\n",
    "   - [Combining Dynamic Config and Chaining](#Combining-Dynamic-Config-and-Chaining)\n",
    "   - [Error Handling in Chains](#Error-Handling-in-Chains)\n",
    "5. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install Mirascope and set up our environment. We'll use OpenAI for our examples, but you can adapt these to other providers supported by Mirascope. For more information on supported providers, see the [Calls documentation](https://docs.mirascope.io/learn/calls/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mirascope[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Configuration\n",
    "\n",
    "Dynamic Configuration in Mirascope allows you to modify the behavior of LLM calls at runtime based on input arguments or other conditions. For more details, see the [Dynamic Configuration documentation](https://docs.mirascope.io/learn/dynamic_configuration/).\n",
    "\n",
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T07:36:01.762693Z",
     "start_time": "2024-09-11T07:35:57.078707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low creativity: I recommend \"The Guest List\" by Lucy Foley. This gripping psychological thriller is set on a remote Irish island where a glamorous wedding takes a dark turn when a murder occurs. The story is told from multiple perspectives, revealing secrets and lies among the guests. With its intricate plot and well-drawn characters, it's a captivating read for any mystery lover. Enjoy!\n",
      "High creativity: One highly recommended mystery book is **\"The Girl with the Dragon Tattoo\"** by Stieg Larsson. This gripping novel follows journalist Mikael Blomkvist as he investigates the decades-old disappearance of a wealthy industrialist's niece, with the help of the enigmatic hacker Lisbeth Salander. The story weaves together themes of family secrets, corruption, and personal redemption, making it a compelling read for mystery enthusiasts. If you're looking for something more recent, consider **\"The Last House on Needless Street\"** by Catriona Ward—a psychological thriller with plenty of twists and a unique narrative style. Happy reading!\n"
     ]
    }
   ],
   "source": [
    "from mirascope.core import BaseDynamicConfig, Messages, openai, prompt_template\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"Recommend a {genre} book\")\n",
    "def recommend_book(genre: str, creativity: float) -> BaseDynamicConfig:\n",
    "    return {\n",
    "        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n",
    "        \"call_params\": {\"temperature\": creativity},\n",
    "    }\n",
    "\n",
    "\n",
    "# Low creativity recommendation\n",
    "response = recommend_book(\"mystery\", 0.2)\n",
    "print(\"Low creativity:\", response.content)\n",
    "\n",
    "# High creativity recommendation\n",
    "response = recommend_book(\"mystery\", 0.8)\n",
    "print(\"High creativity:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computed Fields\n",
    "\n",
    "When using the `Messages.Type` return for writing prompt templates, you can inject computed fields directly into the formatted strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recommend **\"Six of Crows\" by Leigh Bardugo**. This fantasy novel features a diverse cast of characters and is set in a richly built world full of intricate politics, magic, and heists. The story follows a group of outcasts and misfits as they plan a dangerous heist, and it combines action, adventure, and character-driven storytelling. It's highly regarded in the young adult fantasy genre and is the first book in a duology, making it a great choice for fans of fantasy and thrilling plots.\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def recommend_book_by_age(genre: str, age: int) -> Messages.Type:\n",
    "    reading_level = \"adult\"\n",
    "    if age < 12:\n",
    "        reading_level = \"elementary\"\n",
    "    elif age < 18:\n",
    "        reading_level = \"young adult\"\n",
    "    return f\"Recommend a {genre} book with a reading level of {reading_level}\"\n",
    "\n",
    "\n",
    "response = recommend_book_by_age(\"fantasy\", 15)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using string templates, you can use computed fields to dynamically generate or modify template variables used in your prompt. For more information on prompt templates, see the [Prompts documentation](https://docs.mirascope.io/learn/prompts/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T07:36:06.480670Z",
     "start_time": "2024-09-11T07:36:05.149070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recommend **\"Throne of Glass\" by Sarah J. Maas**. It's a captivating young adult fantasy novel featuring a strong female protagonist, Celaena Sardothien, who is an assassin in a corrupt kingdom. The story is filled with intrigue, magic, and romance, making it a great choice for young adult readers who enjoy adventurous and character-driven tales. The book is the first in a series, so if you enjoy it, there's plenty more to explore!\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"Recommend a {genre} book with a reading level of {reading_level}\")\n",
    "def recommend_book_by_age(genre: str, age: int) -> openai.OpenAIDynamicConfig:\n",
    "    reading_level = \"adult\"\n",
    "    if age < 12:\n",
    "        reading_level = \"elementary\"\n",
    "    elif age < 18:\n",
    "        reading_level = \"young adult\"\n",
    "    return {\"computed_fields\": {\"reading_level\": reading_level}}\n",
    "\n",
    "\n",
    "response = recommend_book_by_age(\"fantasy\", 15)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Tools\n",
    "\n",
    "You can dynamically configure which tools are available to the LLM based on runtime conditions. Here's a simple example using a basic tool function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T07:36:09.151739Z",
     "start_time": "2024-09-11T07:36:08.392083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Girl with the Dragon Tattoo by Stieg Larsson (Mystery)\n"
     ]
    }
   ],
   "source": [
    "def format_book(title: str, author: str, genre: str) -> str:\n",
    "    \"\"\"Format a book recommendation.\"\"\"\n",
    "    return f\"{title} by {author} ({genre})\"\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def recommend_book_with_tool(genre: str) -> openai.OpenAIDynamicConfig:\n",
    "    return {\n",
    "        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n",
    "        \"tools\": [format_book],\n",
    "    }\n",
    "\n",
    "\n",
    "response = recommend_book_with_tool(\"mystery\")\n",
    "if response.tool:\n",
    "    print(response.tool.call())\n",
    "else:\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced usage of tools, including the `BaseToolKit` class, please refer to the [Tools documentation](https://docs.mirascope.io/latest/learn/tools/) and the [Tools and Agents Getting Started Guide](https://docs.mirascope.io/latest/cookbook/agents/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "Chaining in Mirascope allows you to combine multiple LLM calls or operations in a sequence to solve complex tasks. Let's explore two main approaches to chaining: function-based chaining and chaining with computed fields.\n",
    "\n",
    "### Function-based Chaining\n",
    "\n",
    "In function-based chaining, you call multiple functions in sequence, passing the output of one function as input to the next. This approach requires you to manage the sequence of calls manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T07:36:16.594586Z",
     "start_time": "2024-09-11T07:36:15.294731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bien sûr ! Veuillez fournir le texte que vous aimeriez que je résume.\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def summarize(text: str) -> str:\n",
    "    return f\"Summarize this text: {text}\"\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def translate(text: str, language: str) -> str:\n",
    "    return f\"Translate this text to {language}: {text}\"\n",
    "\n",
    "\n",
    "original_text = \"Long English text here...\"\n",
    "summary = summarize(original_text)\n",
    "translation = translate(summary.content, \"french\")\n",
    "print(translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Chaining\n",
    "\n",
    "You can easily create a single function that calls the entire chain simply by calling each part of the chain in the function body of the corresponding parent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bien sûr ! Veuillez fournir le texte que vous souhaitez que je résume.\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def summarize(text: str) -> str:\n",
    "    return f\"Summarize this text: {text}\"\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def summarize_and_translate(text: str, language: str) -> str:\n",
    "    summary = summarize(original_text)\n",
    "    return f\"Translate this text to {language}: {summary.content}\"\n",
    "\n",
    "\n",
    "original_text = \"Long English text here...\"\n",
    "translation = summarize_and_translate(original_text, \"french\")\n",
    "print(translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining with Computed Fields\n",
    "\n",
    "Chaining with computed fields allows you to better trace your nested chains since the full chain of operations will exist in the response of the single function (rather than having to call and track each part of the chain separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T07:36:20.903239Z",
     "start_time": "2024-09-11T07:36:19.838736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: Veuillez fournir le texte que vous aimeriez que je résume.\n",
      "\n",
      "Computed fields (including summary): {'summary': OpenAICallResponse(metadata={}, response=ChatCompletion(id='chatcmpl-AAkXulav2CQsWjfwmDoNcsK0BKSVB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Please provide the text you would like me to summarize.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727125566, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_1bb46167f9', usage=CompletionUsage(completion_tokens=11, prompt_tokens=18, total_tokens=29, completion_tokens_details={'reasoning_tokens': 0})), tool_types=None, prompt_template=None, fn_args={'text': 'Long English text here...'}, dynamic_config={'messages': [BaseMessageParam(role='user', content='Summarize this text: Long English text here...')]}, messages=[{'role': 'user', 'content': 'Summarize this text: Long English text here...'}], call_params={}, call_kwargs={'model': 'gpt-4o-mini', 'messages': [{'role': 'user', 'content': 'Summarize this text: Long English text here...'}]}, user_message_param={'content': 'Summarize this text: Long English text here...', 'role': 'user'}, start_time=1727125566298.787, end_time=1727125566887.317, message_param={'content': 'Please provide the text you would like me to summarize.', 'refusal': None, 'role': 'assistant', 'tool_calls': None}, tools=None, tool=None)}\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def summarize(text: str) -> str:\n",
    "    return f\"Summarize this text: {text}\"\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"Translate this text to {language}: {summary}\")\n",
    "def summarize_and_translate(text: str, language: str) -> BaseDynamicConfig:\n",
    "    return {\"computed_fields\": {\"summary\": summarize(text)}}\n",
    "\n",
    "\n",
    "response = summarize_and_translate(\"Long English text here...\", \"french\")\n",
    "print(\"Translation:\", response.content)\n",
    "print(\n",
    "    \"\\nComputed fields (including summary):\", response.dynamic_config[\"computed_fields\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with computed fields, you get access to both the final translation and the intermediate summary in a single response. This approach provides better traceability and can be particularly useful for debugging and understanding the entire chain of operations without the need to manage multiple separate function calls.\n",
    "\n",
    "Of course, you can always put the computed fields in the dynamic configuration without using string templates for the same effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: Bien sûr ! Veuillez fournir le texte que vous aimeriez que je résume.\n",
      "\n",
      "Computed fields (including summary): {'summary': OpenAICallResponse(metadata={}, response=ChatCompletion(id='chatcmpl-AAkYkYoxPqIaDWqaM5MLCUvlGlGUz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Sure! Please provide the text that you would like me to summarize.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727125618, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_1bb46167f9', usage=CompletionUsage(completion_tokens=14, prompt_tokens=18, total_tokens=32, completion_tokens_details={'reasoning_tokens': 0})), tool_types=None, prompt_template=None, fn_args={'text': 'Long English text here...'}, dynamic_config={'messages': [BaseMessageParam(role='user', content='Summarize this text: Long English text here...')]}, messages=[{'role': 'user', 'content': 'Summarize this text: Long English text here...'}], call_params={}, call_kwargs={'model': 'gpt-4o-mini', 'messages': [{'role': 'user', 'content': 'Summarize this text: Long English text here...'}]}, user_message_param={'content': 'Summarize this text: Long English text here...', 'role': 'user'}, start_time=1727125618634.048, end_time=1727125619128.29, message_param={'content': 'Sure! Please provide the text that you would like me to summarize.', 'refusal': None, 'role': 'assistant', 'tool_calls': None}, tools=None, tool=None)}\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def summarize(text: str) -> str:\n",
    "    return f\"Summarize this text: {text}\"\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"Translate this text to {language}: {summary}\")\n",
    "def summarize_and_translate(text: str, language: str) -> BaseDynamicConfig:\n",
    "    summary = summarize(text)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            Messages.User(f\"Translate this text to {language}: {summary.content}\"),\n",
    "        ],\n",
    "        \"computed_fields\": {\"summary\": summary},\n",
    "    }\n",
    "\n",
    "\n",
    "response = summarize_and_translate(\"Long English text here...\", \"french\")\n",
    "print(\"Translation:\", response.content)\n",
    "print(\n",
    "    \"\\nComputed fields (including summary):\", response.dynamic_config[\"computed_fields\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling in Chains\n",
    "\n",
    "Implementing robust error handling is crucial in complex chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T01:08:04.218737Z",
     "start_time": "2024-09-12T01:08:01.204558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Result: Bien sûr ! Veuillez fournir le texte que vous souhaitez résumer, et je serai heureux de vous aider.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAIError\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def summarize(text: str) -> str:\n",
    "    return f\"Summarize this text: {text}\"\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template()\n",
    "def translate(text: str, language: str) -> str:\n",
    "    return f\"Translate this text to {language}: {text}\"\n",
    "\n",
    "\n",
    "def process_text_with_error_handling(text: str, target_language: str):\n",
    "    try:\n",
    "        summary = summarize(text).content\n",
    "    except OpenAIError as e:\n",
    "        print(f\"Error during summarization: {e}\")\n",
    "        summary = text  # Fallback to original text if summarization fails\n",
    "\n",
    "    try:\n",
    "        translation = translate(summary, target_language).content\n",
    "        return translation\n",
    "    except OpenAIError as e:\n",
    "        print(f\"Error during translation: {e}\")\n",
    "        return summary  # Fallback to summary if translation fails\n",
    "\n",
    "\n",
    "result = process_text_with_error_handling(\"Long text here...\", \"French\")\n",
    "print(\"Processed Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated various techniques for using Dynamic Configuration and Chaining in Mirascope. These powerful features allow you to create flexible, efficient, and complex LLM-powered applications. By combining these techniques, you can build sophisticated AI systems that can adapt to different inputs and requirements while maintaining robustness and traceability.\n",
    "\n",
    "Remember to always consider error handling, especially in complex chains, to ensure your applications are resilient to potential issues that may arise during LLM calls or processing steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
