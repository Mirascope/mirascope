{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7638ee3862c99f60",
   "metadata": {},
   "source": [
    "# Web Search Agent\n",
    "\n",
    "In this recipe, we'll explore using Mirascope to enhance our Large Language Model (LLM) ‚Äî specifically OpenAI's GPT-4o mini ‚Äî by providing it with access to the web and its contents. We will be using DuckDuckGo's API as a tool for our Agentic workflow.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "<p class=\"admonition-title\">Mirascope Concepts Used</p>\n",
    "<ul>\n",
    "<li><a href=\"../../../learn/prompts/\">Prompts</a></li>\n",
    "<li><a href=\"../../../learn/calls/\">Calls</a></li>\n",
    "<li><a href=\"../../../learn/agents/\">Agents</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "<p class=\"admonition-title\">Background</p>\n",
    "<p>\n",
    "In the past, users had to rely on search engines and manually browse through multiple web pages to find answers to their questions. Large Language Models (LLMs) have revolutionized this process. They can efficiently utilize search engine results pages (SERPs) and extract relevant content from websites. By leveraging this information, LLMs can quickly provide accurate answers to user queries, eliminating the need for active searching. Users can simply pose their questions and let the LLM work in the background, significantly streamlining the information retrieval process.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9fcd2a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To set up our environment, first let's install all of the packages we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de520d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mirascope[openai]\" beautifulsoup4  duckduckgo-search requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "# Set the appropriate API key for the provider you're using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b874cbd2c154a6",
   "metadata": {},
   "source": [
    "## Add Web Search and Content Extraction\n",
    "\n",
    "We'll use two pre-made tools from `mirascope.tools`:\n",
    "\n",
    "1. `DuckDuckGoSearch`: For performing web searches through DuckDuckGo\n",
    "2. `ParseURLContent`: For extracting content from webpages\n",
    "\n",
    "The `DuckDuckGoSearch` tool provides search results with URLs while `ParseURLContent` handles the content extraction. We save our search results into `search_history` to provide context for future searches.\n",
    "\n",
    "## Add Q&A Functionality\n",
    "\n",
    "Now that we have imported our tools, we can create our `prompt_template` and `_stream` function. We engineer the prompt to first use the `DuckDuckGoSearch` tool to find relevant pages, then `ParseURLContent` to extract content before answering the user question based on the retrieved information:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b1b2950246b768fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:48:59.190625Z",
     "start_time": "2024-10-30T08:48:59.174098Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from mirascope.core import openai, prompt_template, BaseMessageParam\n",
    "from mirascope.tools import DuckDuckGoSearch, ParseURLContent\n",
    "\n",
    "\n",
    "class WebAssistantBaseWithStream(BaseModel):\n",
    "    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "    search_history: list[str] = []\n",
    "    max_results_per_query: int = 2\n",
    "\n",
    "    @openai.call(model=\"gpt-4o-mini\", stream=True)\n",
    "    @prompt_template(  # noqa: F821\n",
    "        \"\"\"\n",
    "        SYSTEM:\n",
    "        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\n",
    "        The current date is {current_date}.\n",
    "\n",
    "        You have access to the following tools:\n",
    "        - `_web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:\n",
    "            1. There is a previous search context: {self.search_history}\n",
    "            2. There is the current user query: {question}\n",
    "            3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query. \n",
    "                Even if the connection isn't immediately clear, consider how they might be related.\n",
    "        - `extract_content`: Parse the content of a webpage.\n",
    "\n",
    "        When calling the `_web_search` tool, the `body` is simply the body of the search\n",
    "        result. You MUST then call the `extract_content` tool to get the actual content\n",
    "        of the webpage. It is up to you to determine which search results to parse.\n",
    "\n",
    "        Once you have gathered all of the information you need, generate a writeup that\n",
    "        strikes the right balance between brevity and completeness based on the context of the user's query.\n",
    "\n",
    "        MESSAGES: {self.messages}\n",
    "        USER: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "    async def _stream(self, question: str) -> openai.OpenAIDynamicConfig:\n",
    "        return {\n",
    "            \"tools\": [DuckDuckGoSearch, ParseURLContent],\n",
    "            \"computed_fields\": {\n",
    "                \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            },\n",
    "        }\n",
    "\n",
    "    async def _step(self, question: str):\n",
    "        print(self.messages)\n",
    "        response = await self._stream(question)\n",
    "        tools_and_outputs = []\n",
    "        async for chunk, tool in response:\n",
    "            if tool:\n",
    "                print(f\"using {tool._name()} tool with args: {tool.args}\")\n",
    "                tools_and_outputs.append((tool, tool.call()))\n",
    "            else:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "        if response.user_message_param:\n",
    "            self.messages.append(response.user_message_param)\n",
    "        self.messages.append(response.message_param)\n",
    "        if tools_and_outputs:\n",
    "            self.messages += response.tool_message_params(tools_and_outputs)\n",
    "            await self._step(\"\")\n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            question = input(\"(User): \")\n",
    "            if question == \"exit\":\n",
    "                break\n",
    "            print(f\"(User): {question}\", flush=True)\n",
    "            print(\"(Assistant): \", end=\"\", flush=True)\n",
    "            await self._step(question)\n",
    "            print()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "90937bb00d1f6334",
   "metadata": {},
   "source":[
    "There are a few things to note:\n",
    "\n",
    "1. We set our `@openai.call()` to `stream=True` to provide a more responsive user experience.\n",
    "2. We give the LLM the current date so the user does not need to provide that.\n",
    "3. We instruct our LLM on how to use the pre-made tools we've imported.\n",
    "    * User queries can often times be ambiguous so giving as much context to the LLM when it generates the search query is crucial.\n",
    "    * Multiple search queries are generated for user queries that might rely on previous context.\n",
    "\n",
    "By utilizing Mirascope's pre-made tools, we avoid having to implement web search and content extraction functionality ourselves. This allows us to focus on engineering the prompt and agent behavior rather than low-level implementation details.\n",
    "\n",
    "### Example search queries\n",
    "\n",
    "Let's look at how the search queries work together as context builds up...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "80dd7c3a3e94a37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:50:00.570430Z",
     "start_time": "2024-10-30T08:49:03.313765Z"
    }
   },
   "source": [
    "await WebAssistantBaseWithStream().run()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(User): I am a SWE looking for a LLM dev tool library\n",
      "(Assistant): []\n",
      "using DuckDuckGoSearch tool with args: {'queries': ['best LLM development tools', 'LLM libraries for software engineers', 'open source LLM development libraries', 'machine learning development tools for LLMs']}\n",
      "[{'role': 'user', 'content': 'I am a SWE looking for a LLM dev tool library'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"best LLM development tools\",\"LLM libraries for software engineers\",\"open source LLM development libraries\",\"machine learning development tools for LLMs\"]}', 'name': 'DuckDuckGoSearch'}, 'id': 'call_PKBPb8KVSWRZMzsJsPdQvR2N'}]}, {'role': 'tool', 'content': \"<class 'duckduckgo_search.exceptions.RatelimitException'>: Failed to search the web for text\", 'tool_call_id': 'call_PKBPb8KVSWRZMzsJsPdQvR2N', 'name': 'DuckDuckGoSearch'}]\n",
      "using DuckDuckGoSearch tool with args: {'queries': ['best LLM development tools', 'LLM libraries for software engineers', 'open source LLM development libraries', 'machine learning development tools for LLMs']}\n",
      "[{'role': 'user', 'content': 'I am a SWE looking for a LLM dev tool library'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"best LLM development tools\",\"LLM libraries for software engineers\",\"open source LLM development libraries\",\"machine learning development tools for LLMs\"]}', 'name': 'DuckDuckGoSearch'}, 'id': 'call_PKBPb8KVSWRZMzsJsPdQvR2N'}]}, {'role': 'tool', 'content': \"<class 'duckduckgo_search.exceptions.RatelimitException'>: Failed to search the web for text\", 'tool_call_id': 'call_PKBPb8KVSWRZMzsJsPdQvR2N', 'name': 'DuckDuckGoSearch'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"best LLM development tools\",\"LLM libraries for software engineers\",\"open source LLM development libraries\",\"machine learning development tools for LLMs\"]}', 'name': 'DuckDuckGoSearch'}, 'id': 'call_yD03cS6yuifOu1tNbX9j0SWU'}]}, {'role': 'tool', 'content': 'Title: Comparing 10+ LLMOps Tools: A Comprehensive Vendor Benchmark - AIMultiple\\nURL: https://research.aimultiple.com/llmops-tools/\\nSnippet: Figure 1: The increasing number of LLMs since 2019 2. LLMOps tools can reduce these costs by facilitating LLM management. However, LLMOps is a relatively recent solution and most business leaders are not aware of the leading players in this market. This article explains the LLMOps market and compares available tools.\\n\\nTitle: GitHub - tensorchord/Awesome-LLMOps: An awesome & curated list of best ...\\nURL: https://github.com/tensorchord/awesome-llmops\\nSnippet: A developer first, lightweight, user-friendly experiment tracking and visualization tool for machine learning projects, streamlining collaboration and simplifying MLOps. W&B excels at tracking LLM-powered applications, featuring W&B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and ...\\n\\nTitle: Demystifying LLM-based Software Engineering Agents\\nURL: https://arxiv.org/html/2407.01489v2\\nSnippet: To solve the challenging real-world software development problems from SWE-bench, inspired by the Devin AI Software Engineer [], there has been a significant body of work from both academia and industry focusing on developing agent-based approaches [109; 38; 102; 31; 65; 30].While there is not a fixed definition for agent-based approaches, they generally equip LLMs with a set of tools and ...\\n\\nTitle: GitHub - xinyi-hou/LLM4SE_SLR: Large Language Models for Software ...\\nURL: https://github.com/xinyi-hou/LLM4SE_SLR\\nSnippet: This repository extends from our recent work, \"Large Language Models for Software Engineering: A Systematic Literature Review\". It includes necessary information for our research and a curated collection of LLM4SE papers and other resources (datasets, tutorials, etc.). The focus is primarily on ...\\n\\nTitle: GitHub - tensorchord/Awesome-LLMOps: An awesome & curated list of best ...\\nURL: https://github.com/tensorchord/awesome-llmops\\nSnippet: Open Source LLM Engineering Platform: Traces, evals, prompt management and metrics to debug and improve your LLM application. LangKit Out-of-the-box LLM telemetry collection library that extracts features and profiles prompts, responses and metadata about how your LLM is performing over time to find problems at scale.\\n\\nTitle: 8 Top Open-Source LLMs for 2024 and Their Uses - DataCamp\\nURL: https://www.datacamp.com/blog/top-open-source-llms\\nSnippet: The open-source LLM space is rapidly expanding. Today, there are many more open-source LLMs than proprietary ones, and the performance gap may be bridged soon as developers worldwide collaborate to upgrade current LLMs and design more optimized ones. In this vibrant and exciting context, it may be difficult to choose the right open-source LLM ...\\n\\nTitle: GitHub - tensorchord/Awesome-LLMOps: An awesome & curated list of best ...\\nURL: https://github.com/tensorchord/awesome-llmops\\nSnippet: OpenAI compatible API for LLMs and embeddings (LLaMA, Vicuna, ChatGLM and many others) ... Experiment tracking, ML developer tools. MLRun: Machine Learning automation and tracking. Kedro-Viz: Kedro-Viz is an interactive development tool for building data science pipelines with Kedro. Kedro-Viz also allows users to view and compare different ...\\n\\nTitle: Introduction to Large Language Models | Machine Learning | Google for ...\\nURL: https://developers.google.com/machine-learning/resources/intro-llms\\nSnippet: Define language models and large language models (LLMs). Define key LLM concepts, including Transformers and self-attention. Describe the costs and benefits of LLMs, along with common use cases. What is a language model? A language model is a machine learning model that aims to predict and generate plausible language. Autocomplete is a language ...', 'tool_call_id': 'call_yD03cS6yuifOu1tNbX9j0SWU', 'name': 'DuckDuckGoSearch'}]\n",
      "using ParseURLContent tool with args: {'url': 'https://research.aimultiple.com/llmops-tools/'}\n",
      "using ParseURLContent tool with args: {'url': 'https://github.com/tensorchord/awesome-llmops'}\n",
      "using ParseURLContent tool with args: {'url': 'https://www.datacamp.com/blog/top-open-source-llms'}\n",
      "[{'role': 'user', 'content': 'I am a SWE looking for a LLM dev tool library'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"best LLM development tools\",\"LLM libraries for software engineers\",\"open source LLM development libraries\",\"machine learning development tools for LLMs\"]}', 'name': 'DuckDuckGoSearch'}, 'id': 'call_PKBPb8KVSWRZMzsJsPdQvR2N'}]}, {'role': 'tool', 'content': \"<class 'duckduckgo_search.exceptions.RatelimitException'>: Failed to search the web for text\", 'tool_call_id': 'call_PKBPb8KVSWRZMzsJsPdQvR2N', 'name': 'DuckDuckGoSearch'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"best LLM development tools\",\"LLM libraries for software engineers\",\"open source LLM development libraries\",\"machine learning development tools for LLMs\"]}', 'name': 'DuckDuckGoSearch'}, 'id': 'call_yD03cS6yuifOu1tNbX9j0SWU'}]}, {'role': 'tool', 'content': 'Title: Comparing 10+ LLMOps Tools: A Comprehensive Vendor Benchmark - AIMultiple\\nURL: https://research.aimultiple.com/llmops-tools/\\nSnippet: Figure 1: The increasing number of LLMs since 2019 2. LLMOps tools can reduce these costs by facilitating LLM management. However, LLMOps is a relatively recent solution and most business leaders are not aware of the leading players in this market. This article explains the LLMOps market and compares available tools.\\n\\nTitle: GitHub - tensorchord/Awesome-LLMOps: An awesome & curated list of best ...\\nURL: https://github.com/tensorchord/awesome-llmops\\nSnippet: A developer first, lightweight, user-friendly experiment tracking and visualization tool for machine learning projects, streamlining collaboration and simplifying MLOps. W&B excels at tracking LLM-powered applications, featuring W&B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and ...\\n\\nTitle: Demystifying LLM-based Software Engineering Agents\\nURL: https://arxiv.org/html/2407.01489v2\\nSnippet: To solve the challenging real-world software development problems from SWE-bench, inspired by the Devin AI Software Engineer [], there has been a significant body of work from both academia and industry focusing on developing agent-based approaches [109; 38; 102; 31; 65; 30].While there is not a fixed definition for agent-based approaches, they generally equip LLMs with a set of tools and ...\\n\\nTitle: GitHub - xinyi-hou/LLM4SE_SLR: Large Language Models for Software ...\\nURL: https://github.com/xinyi-hou/LLM4SE_SLR\\nSnippet: This repository extends from our recent work, \"Large Language Models for Software Engineering: A Systematic Literature Review\". It includes necessary information for our research and a curated collection of LLM4SE papers and other resources (datasets, tutorials, etc.). The focus is primarily on ...\\n\\nTitle: GitHub - tensorchord/Awesome-LLMOps: An awesome & curated list of best ...\\nURL: https://github.com/tensorchord/awesome-llmops\\nSnippet: Open Source LLM Engineering Platform: Traces, evals, prompt management and metrics to debug and improve your LLM application. LangKit Out-of-the-box LLM telemetry collection library that extracts features and profiles prompts, responses and metadata about how your LLM is performing over time to find problems at scale.\\n\\nTitle: 8 Top Open-Source LLMs for 2024 and Their Uses - DataCamp\\nURL: https://www.datacamp.com/blog/top-open-source-llms\\nSnippet: The open-source LLM space is rapidly expanding. Today, there are many more open-source LLMs than proprietary ones, and the performance gap may be bridged soon as developers worldwide collaborate to upgrade current LLMs and design more optimized ones. In this vibrant and exciting context, it may be difficult to choose the right open-source LLM ...\\n\\nTitle: GitHub - tensorchord/Awesome-LLMOps: An awesome & curated list of best ...\\nURL: https://github.com/tensorchord/awesome-llmops\\nSnippet: OpenAI compatible API for LLMs and embeddings (LLaMA, Vicuna, ChatGLM and many others) ... Experiment tracking, ML developer tools. MLRun: Machine Learning automation and tracking. Kedro-Viz: Kedro-Viz is an interactive development tool for building data science pipelines with Kedro. Kedro-Viz also allows users to view and compare different ...\\n\\nTitle: Introduction to Large Language Models | Machine Learning | Google for ...\\nURL: https://developers.google.com/machine-learning/resources/intro-llms\\nSnippet: Define language models and large language models (LLMs). Define key LLM concepts, including Transformers and self-attention. Describe the costs and benefits of LLMs, along with common use cases. What is a language model? A language model is a machine learning model that aims to predict and generate plausible language. Autocomplete is a language ...', 'tool_call_id': 'call_yD03cS6yuifOu1tNbX9j0SWU', 'name': 'DuckDuckGoSearch'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"url\": \"https://research.aimultiple.com/llmops-tools/\"}', 'name': 'ParseURLContent'}, 'id': 'call_4e6UDTsZyXFqAkZcmpQDXiMV'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://github.com/tensorchord/awesome-llmops\"}', 'name': 'ParseURLContent'}, 'id': 'call_dfULUM0OKBERrMSN6JtLPQk2'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://www.datacamp.com/blog/top-open-source-llms\"}', 'name': 'ParseURLContent'}, 'id': 'call_L1Xy8nuy2wRu2TpvU3DcEa0o'}]}, {'role': 'tool', 'content': \"We follow\\nethical norms\\n&\\nour process\\nfor objectivity.\\nThis research is not\\nfunded\\nby any sponsors.\\nTable of contents\\nLLMOps Landscape\\nCompare the top LLMOps platforms in 2023\\nTools for secure and complaint LLMs\\nDisclaimer about current categorization approach\\nWhich LLMOps tool is the best choice for your business?\\nFurther reading\\nMLOps\\nLLM\\nUpdated on\\nOct 9, 2024\\nComparing 10+ LLMOps Tools: A Comprehensive Vendor Benchmark\\nBy\\nCem Dilmegani\\nThe number of\\nlarge language models (LLMs)\\nhas been increasing since 2019 due to the models‚Äô extensive application areas and capabilities (See Figure 1).\\nYet, the estimates show that designing a new foundation model can cost up to $90 million while fine-tuning or enhancing existing large language models can cost $1 million to $100 thousand.\\n1\\nThese costs result from:\\nComputational costs like hardware usage for training runs\\nData gathering and labelling costs\\nEngineering and R&D costs\\nFigure 1: The increasing number of LLMs since 2019\\n2\\nLLMOps\\ntools can reduce these costs by facilitating LLM management. However, LLMOps is a relatively recent solution and most business leaders are not aware of the leading players in this market. This article explains the LLMOps market and compares available tools.\\nLLMOps Landscape\\nThere are 20+ tools that claim to be LLMOps solutions, which can be evaluated under 6 main categories:\\n1. LLMOps Platforms\\nThese are either designed specifically for LLMOps or are MLOps platforms that started offering LLMOps capabilities. They include features that allow carrying out these operations on LLMs:\\nFinetuning\\nVersioning\\nDeploying\\nThese LLM platforms can offer different levels of flexibility and ease of use:\\nNo-code LLM platforms:\\nSome of these platforms are no-code and low-code, which facilitate LLM adoption. However, these tools typically have limited flexibility.\\nCode-first platforms:\\nThese platforms target machine learning engineers and data scientists. They tend to offer a higher level of flexibility.\\n2. Integration frameworks\\nThese tools are built to facilitate developing\\nLLM applications\\nsuch as document analyzers, code analyzers, chatbots etc.\\n3.) Vector databases (VD)\\nVDs store high-dimensional data vectors, such as patient data covering symptoms, blood test results, behaviors, and general health. Some VD software like deep lake can facilitate LLM operations.\\n4.) Fine-tuning tools\\nFine-tuning tools are frameworks, or platforms for fine-tuning pre-trained models. These tools provide a streamlined workflow to modify,\\nretrain\\n, and optimize pre-trained models for natural language processing, computer vision, and more tasks. Some libraries are also designed for fine-tuning, such as Hugging Face Transformers, PyTorch, and TensorFlow.\\n5.) RLHF tools\\nReinforcement learning from human feedback\\n, or RLHF, is a way for AI to learn the best actions by listening to human input. Typically, Reinforcement learning includes an RL algorithm to learn by interacting with the environment and receiving rewards or penalties based on its actions.\\nIn contrast, RLHF tools (e.g. Clickworker or Appen) include human feedback in the learning loop. RLHF can be useful to:\\nEnhance LLM fine-tuning by large data labeling\\nImplement AI governance by reducing biases in LLM responses and moderating content\\nCustomize model\\nImprove contextual understanding.\\n6.) LLM testing tools\\nLLM testing tools evaluate and assess LLMs by testing model performance, capabilities, and potential biases in various language-related tasks and applications, such as natural language understanding and generation. Testing tools may include:\\nTesting frameworks\\nBenchmark datasets\\nEvaluation metrics.\\n7.) LLM monitoring and observability\\nLLM monitoring and observability tools ensure their proper functioning, user safety, and brand protection. LLM monitoring includes activities like:\\nFunctional monitoring\\n: Keeping track of factors like response time, token usage, number of requests, costs and error rates.\\nPrompt monitoring\\n: Checking user inputs and prompts to evaluate toxic content in responses, measure embedding distances, and identify malicious prompt injections.\\nResponse monitoring:\\nAnalyzing to discover\\nhallucinatory\\nbehavior, topic divergence, tone and sentiment in the responses.\\nCompare the top LLMOps platforms in 2023\\nIn this section, we focus on LLMOps platforms and excluded integration frameworks and other tools. LLMOps platforms can be examined in these categories:\\n1. MLOps platforms\\nSome\\nMLOps platforms\\noffer LLMOps toolkits.\\nMachine Learning Operations (MLOps)\\nmanages and optimizes the end-to-end machine learning lifecycle. Since LLMs are also machine learning models, MLOps vendors are naturally expanding into this domain.\\n2. LLM platforms\\nSome\\nLLM providers\\n, especially OpenAI, are also providing LLMOps capabilities to fine-tune, integrate and deploy their models.\\n3. Data and cloud platforms\\nData or cloud platforms are starting to offer LLMOps capabilities that allow their users to leverage their own data to build and finetune LLMs. For example, Databricks acquired MosaicML for $1.3 billion.\\n3\\nCloud platforms\\nCloud leaders Amazon, Azure and Google have all launched their LLMOps offering which allows users to deploy models from different providers with ease\\n4. LLMOPs frameworks\\nThis category includes tools that exclusively focus on optimizing and managing LLM operations. The table below shows the Github stars, B2B reviews and average B2B score from B2B review pages (Trustradius, Gartner & G2) for some of these LLMOps tools:\\nLLMOps Tools\\nGithub Stars\\nNumber of B2B Reviews*\\nAverage Review Score**\\nNemo by Nvidia\\n7,900\\nNA\\nNA\\nDeep Lake\\n6,600\\nNA\\nNA\\nFine-Tuner AI\\n6,000\\nNA\\nNA\\nSnorkel AI\\n5,500\\nNA\\nNA\\nZen ML\\n3,000\\nNA\\nNA\\nLamini AI\\n2100\\nNA\\nNA\\nComet\\n54\\nNA\\nNA\\nTitan ML\\n47\\nNA\\nNA\\nDeepset AI\\n6\\nNA\\nNA\\nValohai\\nNot open source\\n20\\n4.9\\nHere is a brief explanation for each tool in alphabetical order:\\nComet:\\nComet streamlines the ML lifecycle, tracking experiments and production models. Suited for large enterprise teams, it offers various deployment strategies. It supports private cloud, hybrid, and on-premise setups.\\nFigure 2:\\nComet LLMops platform\\n4\\nDeep Lake:\\nDeep Lake combines the capabilities of Data Lakes and Vector Databases to create, refine, and implement high-quality LLMs and MLOps solutions for businesses. Deep Lake allows users to visualize and manipulate datasets in their browser or Jupyter notebook, swiftly accessing different versions and generating new ones through queries, all compatible with PyTorch and TensorFlow.\\nDeepset AI:\\nDeepset AI is a comprehensive platform that allows users to integrate their data with LLMs to build and deploy customized LLM features in their applications. Deepset supports Retrieval-augmented generation (RAG) and Enterprise knowledge search, as well.\\nLamini AI:\\nLamini AI provides an easy method for training LLMs through both prompt-tuning and base model training. Lamini AI users can write custom code, integrate their own data, and host the resulting LLM on their infrastructure.\\nNemo by Nvidia:\\nNvidia offers an end-to-end, cloud-native enterprise framework to develop, customize, and employ generative AI models and LLM applications. The framework can execute various tasks required to train LLMs, such as token classification, prompt learning and question answering.\\nSnorkel AI:\\nSnorkel AI empowers enterprises to construct or customize foundation models (FMs) and large language models (LLMs) to achieve remarkable precision on domain-specific datasets and use cases. Snorkel AI introduces programmatic labelling, enabling data-centric AI development with automated processes.\\nFigure 3:\\nSnorkel AI LLMOps platform\\n5\\n6.\\nTitan ML:\\nTitanML is an NLP development platform that aims to allow businesses to swiftly build and implement smaller, more economical deployments of large language models. It offers proprietary, automated, efficient fine-tuning and inference optimization techniques. This way, it allows businesses to create and roll out large language models in-house.\\n7.\\nValohai:\\nValohai streamlines MLOps and LLMs, automating data extraction to model deployment. It can store models, experiments, and artefacts, making monitoring and deployment easier. Valohai creates an efficient workflow from code to deployment, supporting notebooks, scripts, and Git projects.\\n8.\\nZen ML:\\nZenML primarily focuses on machine learning operations (MLOps) and the management of the machine learning workflow, including data preparation, experimentation, and model deployment.\\nTools for secure and complaint LLMs\\nSome LLMOps integrate with AI governance and\\nLLM security\\ntechnologies to ensure safe, unbiased, and ethical LLM deployment and operation. Check out more on these:\\nCompare Top 25 AI Governance Tools: A Vendor Benchmark\\nCompare 20 LLM Security Tools & Open-Source Frameworks\\nDisclaimer about current categorization approach\\nWe recognize different ways to categorize these tools. Some include technologies like containerization or edge computing, which, while useful for LLM performance, are not essential for model design or monitoring, so we exclude them. Traditional categorizations focus on license type or pre-trained models. While relevant, we believe functionality is more critical. For instance, whether an LLM is open source affects fine-tuning, but since most LLMOps users don‚Äôt modify the code, open source status is less important for these tools.\\nWhich LLMOps tool is the best choice for your business?\\nWe now provide relatively generic recommendations on choosing these tools. We will make these more specific as we explore LLMOps platforms in more detail and as the market matures.\\nHere are a few steps you must complete in your selection process:\\nDefine goals:\\nClearly outline your business goals to establish a solid foundation for your LLMOps tool selection process. For example, if your goal requires training a model from scratch vs fine-tuning an existing model, this will have important implications to your LLMOps stack.\\nDefine requirements:\\nBased on your goal, certain requirements will become more important. For example, if you aim to enable business users to use LLMs, you may want to include no code in your list of requirements.\\nPrepare a shortlist\\n: Consider user reviews and feedback to gain insights into real-world experiences with different LLMOps tools. Rely on this market data to prepare a shortlist.\\nCompare functionality:\\nUtilize free trials and demos provided by various LLMOps tools to compare their features and functionalities firsthand.\\nWhat is LLMOps?\\nLarge Language Models (LLMs) are advanced machine learning models designed to understand and generate human-like text based on the patterns and information they‚Äôve learned from training data. These models are built using deep learning models to capture intricate linguistic nuances and context.\\nLLMOps refer to techniques and tools used for the operational model management of LLMs in production environments.\\nKey components of LLMOps tools\\nLarge Language Model Operations (LLMOps) tools encompass crucial components for efficient management and deployment of large language models (LLMs). These tools typically include features such as:\\n‚Äì\\nPrompt Engineering:\\nCreating effective prompt templates for improved model performance.\\n‚Äì\\nData Management:\\nHandling vast datasets, ensuring proper data versioning, and facilitating exploratory data analysis.\\n‚Äì\\nModel Fine Tuning:\\nFine-tuning LLMs to specific tasks and refining models for optimal performance.\\n‚Äì\\nModel Monitoring:\\nContinuous tracking of model outcomes, detection of accuracy degradation, and addressing\\nmodel drift\\n.\\nWhat are LLMOps benefits?\\nLLMOps delivers significant advantages to machine learning projects leveraging large language models:\\n1.) Increased Accuracy:\\nEnsuring high-quality data for training and reliable deployment enhances model accuracy.\\n2.)\\nReduced Latency:\\nEfficient deployment strategies lead to reduced latency in LLMs, enabling faster data retrieval.\\n3.) Fairness Promotion:\\nStriving to eliminate\\nAI bias\\nprevents discrimination and other\\nAI ethics\\ndilemmas while ensuring\\nresponsible AI\\npractices.\\nLLMOps challenges & solutions\\nChallenges in large language model operations require robust solutions to maintain optimal performance:\\n1.) Data Management Challenges:\\nHandling vast datasets and sensitive data necessitates efficient data collection and versioning.\\n2.)\\nModel Monitoring Solutions:\\nImplementing model monitoring tools to track model outcomes, detect accuracy degradation, and address model drift.\\n3.)\\nScalable Deployment:\\nDeploying scalable infrastructure and utilizing cloud-native technologies to meet computational power requirements.\\n4.)\\nOptimizing Models:\\nEmploying model compression techniques and refining models to enhance overall efficiency.\\nLLMOps tools are pivotal in overcoming challenges and delivering higher quality models in the dynamic landscape of large language models.\\nFurther reading\\nExplore more on LLMs, MLOps and AIOps by checking out our articles:\\nMLOps Tools & Platforms Landscape: In-Depth Guide\\n15 Best AiOps Platforms: Streamline IT Ops with AI\\nChatGPT AIOps in IT Automation: 8 Powerful Examples\\nIf you still have questions about LLMOps tools and landscape, we would like to help:\\nFind the Right Vendors\\nExternal sources\\n1.  ‚Äú\\nThe CEO‚Äôs\\xa0Roadmap on Generative AI\\n‚Äù BCG. March 2023. Revisited August 11, 2023.\\n2.  ‚Äú\\nA Survey of Large Language Models.\\n‚Äù\\nGithub\\n. March 2023. Revisited August 11, 2023.\\n3. ‚Äú\\nDatabricks Signs Definitive Agreement to Acquire MosaicML, a Leading Generative AI Platform\\n‚Äú.\\nDatabricks\\n. June 26, 2023. Retrieved August 24, 2023.\\n4.  ‚Äú\\nDebugging Large Language Models with Comet LLMOps Tools\\n.‚Äù\\nComet\\n. Revisited August 16, 2023.\\n5.  Harvey, N(March 20, 2023). ‚Äú\\nSnorkel Flow Spring 2023: warm starts and foundation models.\\n‚Äù\\nSnorkelAI\\n. Revisited August 16, 2023.\\nShare This Article\\nCem Dilmegani\\nFollow on\\nCem has been the principal analyst at AIMultiple since 2017. AIMultiple informs hundreds of thousands of businesses (as per similarWeb) including 55% of Fortune 500 every month.\\nCem's work has been cited by leading global publications including Business Insider,  Forbes, Washington Post, global firms like Deloitte, HPE and NGOs like World Economic Forum and supranational organizations like European Commission. You can see more reputable companies and resources that referenced AIMultiple.\\nThroughout his career, Cem served as a tech consultant, tech buyer and tech entrepreneur. He advised enterprises on their technology decisions at McKinsey & Company and Altman Solon for more than a decade. He also published a McKinsey report on digitalization.\\nHe led technology strategy and procurement of a telco while reporting to the CEO. He has also led commercial growth of deep tech company Hypatos that reached a 7 digit annual recurring revenue and a 9 digit valuation from 0 within 2 years. Cem's work in Hypatos was covered by leading technology publications like TechCrunch and Business Insider.\\nCem regularly speaks at international technology conferences. He graduated from Bogazici University as a computer engineer and holds an MBA from Columbia Business School.\\nFollow on\\nNext to Read\\nWhy Should You Use Cloud Inference (Inference as a Service) ?\\nOct 21\\n3 min read\\nWhat is LLMOps, Why It Matters & 7 Best Practices\\nSep 6\\n6 min read\\nLarge Language Models: Complete Guide\\nOct 17\\n4 min read\\nComments\\nYour email address will not be published. All fields are required.\\n0\\nComments\\nPost Comment\\nRelated research\\nLLM Data Guide & 6 Methods of Collection\\nOct 25\\n6 min read\\nGuide to RLHF LLMs : Benefits & Top Vendors\\nOct 27\\n5 min read\", 'tool_call_id': 'call_4e6UDTsZyXFqAkZcmpQDXiMV', 'name': 'ParseURLContent'}, {'role': 'tool', 'content': 'tensorchord\\n/\\nAwesome-LLMOps\\nPublic\\nNotifications\\nYou must be signed in to change notification settings\\nFork\\n373\\nStar\\n3.9k\\nAn awesome & curated list of best LLMOps tools for developers\\nLicense\\nCC0-1.0 license\\n3.9k\\nstars\\n373\\nforks\\nBranches\\nTags\\nActivity\\nStar\\nNotifications\\nYou must be signed in to change notification settings\\ntensorchord/Awesome-LLMOps\\nmain\\nBranches\\nTags\\nGo to file\\nCode\\nFolders and files\\nName\\nName\\nLast commit message\\nLast commit date\\nLatest commit\\nHistory\\n156 Commits\\nscripts\\nscripts\\n.gitignore\\n.gitignore\\nLICENSE\\nLICENSE\\nREADME.md\\nREADME.md\\ncontributing.md\\ncontributing.md\\nView all files\\nRepository files navigation\\nAwesome LLMOps\\nAn awesome & curated list of the best LLMOps tools for developers.\\nContribute\\nContributions are most welcome, please adhere to the\\ncontribution guidelines\\n.\\nTable of Contents\\nTable of Contents\\nModel\\nLarge Language Model\\nCV Foundation Model\\nAudio Foundation Model\\nServing\\nLarge Model Serving\\nFrameworks/Servers for Serving\\nObservability\\nSecurity\\nLLMOps\\nSearch\\nVector search\\nCode AI\\nTraining\\nIDEs and Workspaces\\nFoundation Model Fine Tuning\\nFrameworks for Training\\nExperiment Tracking\\nVisualization\\nModel Editing\\nData\\nData Management\\nData Storage\\nData Tracking\\nFeature Engineering\\nData/Feature enrichment\\nLarge Scale Deployment\\nML Platforms\\nWorkflow\\nScheduling\\nModel Management\\nPerformance\\nML Compiler\\nProfiling\\nAutoML\\nOptimizations\\nFederated ML\\nAwesome Lists\\nModel\\nLarge Language Model\\nProject\\nDetails\\nRepository\\nAlpaca\\nCode and documentation to train Stanford\\'s Alpaca models, and generate the data.\\nBELLE\\nA 7B Large Language Model fine-tune by 34B Chinese Character Corpus, based on LLaMA and Alpaca.\\nBloom\\nBigScience Large Open-science Open-access Multilingual Language Model\\ndolly\\nDatabricks‚Äô Dolly, a large language model trained on the Databricks Machine Learning Platform\\nFalcon 40B\\nFalcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.\\nFastChat (Vicuna)\\nAn open platform for training, serving, and evaluating large language models. Release repo for Vicuna and FastChat-T5.\\nGemma\\nGemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models.\\nGLM-6B (ChatGLM)\\nAn Open Bilingual Pre-Trained Model, quantization of ChatGLM-130B, can run on consumer-level GPUs.\\nChatGLM2-6B\\nChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model\\nChatGLM-6B\\n.\\nGLM-130B (ChatGLM)\\nAn Open Bilingual Pre-Trained Model (ICLR 2023)\\nGPT-NeoX\\nAn implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.\\nLuotuo\\nA Chinese LLM, Based on LLaMA and fine tune by Stanford Alpaca, Alpaca LoRA, Japanese-Alpaca-LoRA.\\nMixtral-8x7B-v0.1\\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\\nStableLM\\nStableLM: Stability AI Language Models\\n‚¨Ü back to ToC\\nCV Foundation Model\\nProject\\nDetails\\nRepository\\ndisco-diffusion\\nA frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.\\nmidjourney\\nMidjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.\\nsegment-anything (SAM)\\nproduces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image.\\nstable-diffusion\\nA latent text-to-image diffusion model\\nstable-diffusion v2\\nHigh-Resolution Image Synthesis with Latent Diffusion Models\\n‚¨Ü back to ToC\\nAudio Foundation Model\\nProject\\nDetails\\nRepository\\nbark\\nBark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects.\\nwhisper\\nRobust Speech Recognition via Large-Scale Weak Supervision\\nServing\\nLarge Model Serving\\nProject\\nDetails\\nRepository\\nAlpaca-LoRA-Serve\\nAlpaca-LoRA as Chatbot service\\nCTranslate2\\nfast inference engine for Transformer models in C++\\nClip-as-a-service\\nserving the OpenAI CLIP model\\nDeepSpeed-MII\\nMII makes low-latency and high-throughput inference possible, powered by DeepSpeed.\\nFaster Whisper\\nfast inference engine for whisper in C++ using CTranslate2.\\nFlexGen\\nRunning large language models on a single GPU for throughput-oriented scenarios.\\nFlowise\\nDrag & drop UI to build your customized LLM flow using LangchainJS.\\nllama.cpp\\nPort of Facebook\\'s LLaMA model in C/C++\\nInfinity\\nRest API server for serving text-embeddings\\nModelz-LLM\\nOpenAI compatible API for LLMs and embeddings (LLaMA, Vicuna, ChatGLM and many others)\\nOllama\\nServe Llama 2 and other large language models locally from command line or through a browser interface.\\nTensorRT-LLM\\nInference engine for TensorRT on Nvidia GPUs\\ntext-generation-inference\\nLarge Language Model Text Generation Inference\\ntext-embeddings-inference\\nInference for text-embedding models\\ntokenizers\\nüí• Fast State-of-the-Art Tokenizers optimized for Research and Production\\nvllm\\nA high-throughput and memory-efficient inference and serving engine for LLMs.\\nwhisper.cpp\\nPort of OpenAI\\'s Whisper model in C/C++\\nx-stable-diffusion\\nReal-time inference for Stable Diffusion - 0.88s latency. Covers AITemplate, nvFuser, TensorRT, FlashAttention.\\n‚¨Ü back to ToC\\nFrameworks/Servers for Serving\\nProject\\nDetails\\nRepository\\nBentoML\\nThe Unified Model Serving Framework\\nJina\\nBuild multimodal AI services via cloud native technologies ¬∑ Model Serving ¬∑ Generative AI ¬∑ Neural Search ¬∑ Cloud Native\\nMosec\\nA machine learning model serving framework with dynamic batching and pipelined stages, provides an easy-to-use Python interface.\\nTFServing\\nA flexible, high-performance serving system for machine learning models.\\nTorchserve\\nServe, optimize and scale PyTorch models in production\\nTriton Server (TRTIS)\\nThe Triton Inference Server provides an optimized cloud and edge inferencing solution.\\nlangchain-serve\\nServerless LLM apps on Production with Jina AI Cloud\\nlanarky\\nFastAPI framework to build production-grade LLM applications\\nray-llm\\nLLMs on Ray - RayLLM\\nXinference\\nReplace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you\\'re empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.\\n‚¨Ü back to ToC\\nSecurity\\nFrameworks for LLM security\\nProject\\nDetails\\nRepository\\nPlexiglass\\nA Python Machine Learning Pentesting Toolbox for Adversarial Attacks. Works with LLMs, DNNs, and other machine learning algorithms.\\n‚¨Ü back to ToC\\nObservability\\nProject\\nDetails\\nRepository\\nAzure OpenAI Logger\\n\"Batteries included\" logging solution for your Azure OpenAI instance.\\nDeepchecks\\nTests for Continuous Validation of ML Models & Data. Deepchecks is a Python package for comprehensively validating your machine learning models and data with minimal effort.\\nEvidently\\nAn open-source framework to evaluate, test and monitor ML and LLM-powered systems.\\nFiddler AI\\nEvaluate, monitor, analyze, and improve machine learning and generative models from pre-production to production. Ship more ML and LLMs into production, and monitor ML and LLM metrics like hallucination, PII, and toxicity.\\nGiskard\\nTesting framework dedicated to ML models, from tabular to LLMs. Detect risks of biases, performance issues and errors in 4 lines of code.\\nGreat Expectations\\nAlways know what to expect from your data.\\nwhylogs\\nThe open standard for data logging\\n‚¨Ü back to ToC\\nLLMOps\\nProject\\nDetails\\nRepository\\nagenta\\nThe LLMOps platform to build robust LLM apps. Easily experiment and evaluate different prompts, models, and workflows to build robust apps.\\nAI studio\\nA Reliable Open Source AI studio to build core infrastructure stack for your LLM Applications. It allows you to gain visibility, make your application reliable, and prepare it for production with features such as caching, rate limiting, exponential retry, model fallback, and more.\\nArize-Phoenix\\nML observability for LLMs, vision, language, and tabular models.\\nBudgetML\\nDeploy a ML inference service on a budget in less than 10 lines of code.\\ndeeplake\\nStream large multimodal datasets to achieve near 100% GPU utilization. Query, visualize, & version control data. Access data w/o the need to recompute the embeddings for the model finetuning.\\nDify\\nOpen-source framework aims to enable developers (and even non-developers) to quickly build useful applications based on large language models, ensuring they are visual, operable, and improvable.\\nDstack\\nCost-effective LLM development in any cloud (AWS, GCP, Azure, Lambda, etc).\\nEmbedchain\\nFramework to create ChatGPT like bots over your dataset.\\nEvidently\\nAn open-source framework to evaluate, test and monitor ML and LLM-powered systems.\\nFiddler AI\\nEvaluate, monitor, analyze, and improve MLOps and LLMOps from pre-production to production.\\nGlide\\nCloud-Native LLM Routing Engine. Improve LLM app resilience and speed.\\ngotoHuman\\nBring a\\nhuman into the loop\\nin your LLM-based and agentic workflows. Prompt users to approve actions, select next steps, or review and validate generated results.\\nGPTCache\\nCreating semantic cache to store responses from LLM queries.\\nGPUStack\\nAn open-source GPU cluster manager for running and managing LLMs\\nHaystack\\nQuickly compose applications with LLM Agents, semantic search, question-answering and more.\\nHelicone\\nOpen-source LLM observability platform for logging, monitoring, and debugging AI applications. Simple 1-line integration to get started.\\nIzlo\\nPrompt management tools for teams. Store, improve, test, and deploy your prompts in one unified workspace.\\nKeywords AI\\nA unified DevOps platform for AI software. Keywords AI makes it easy for developers to build LLM applications.\\nlangchain\\nBuilding applications with LLMs through composability\\nLangFlow\\nAn effortless way to experiment and prototype LangChain flows with drag-and-drop components and a chat interface.\\nLangfuse\\nOpen Source LLM Engineering Platform: Traces, evals, prompt management and metrics to debug and improve your LLM application.\\nLangKit\\nOut-of-the-box LLM telemetry collection library that extracts features and profiles prompts, responses and metadata about how your LLM is performing over time to find problems at scale.\\nLiteLLM üöÖ\\nA simple & light 100 line package to\\nstandardize LLM API calls\\nacross OpenAI, Azure, Cohere, Anthropic, Replicate API Endpoints\\nLiteral AI\\nMulti-modal LLM observability and evaluation platform. Create prompt templates, deploy prompts versions, debug LLM runs, create datasets, run evaluations, monitor LLM metrics and collect human feedback.\\nLlamaIndex\\nProvides a central interface to connect your LLMs with external data.\\nLLMApp\\nLLM App is a Python library that helps you build real-time LLM-enabled data pipelines with few lines of code.\\nLLMFlows\\nLLMFlows is a framework for building simple, explicit, and transparent LLM applications such as chatbots, question-answering systems, and agents.\\nLLMonitor\\nObservability and monitoring for AI apps and agents. Debug agents with powerful tracing and logging. Usage analytics and dive deep into the history of your requests. Developer friendly modules with plug-and-play integration into LangChain.\\nmagentic\\nSeamlessly integrate LLMs as Python functions. Use type annotations to specify structured output. Mix LLM queries and function calling with regular Python code to create complex LLM-powered functionality.\\nManag.ai\\nYour all-in-one prompt management and observability platform. Craft, track, and perfect your LLM prompts with ease.\\nMirascope\\nIntuitive convenience tooling for lightning-fast, efficient development and ensuring quality in LLM-based applications\\nOpenLIT\\nOpenLIT is an OpenTelemetry-native GenAI and LLM Application Observability tool and provides OpenTelmetry Auto-instrumentation for monitoring LLMs, VectorDBs and Frameworks. It provides valuable insights into token & cost usage, user interaction, and performance related metrics.\\nOpik\\nConfidently evaluate, test, and ship LLM applications with a suite of observability tools to calibrate language model outputs across your dev and production lifecycle.\\nParea AI\\nPlatform and SDK for AI Engineers providing tools for LLM evaluation, observability, and a version-controlled enhanced prompt playground.\\nPezzo üïπÔ∏è\\nPezzo is the open-source LLMOps platform built for developers and teams. In just two lines of code, you can seamlessly troubleshoot your AI operations, collaborate and manage your prompts in one place, and instantly deploy changes to any environment.\\nPromptHub\\nFull stack prompt management tool designed to be usable by technical and non-technical team members. Test, version, collaborate, deploy, and monitor, all from one place.\\npromptfoo\\nOpen-source tool for testing & evaluating prompt quality. Create test cases, automatically check output quality and catch regressions, and reduce evaluation cost.\\nPromptFoundry\\nThe simple prompt engineering and evaluation tool designed for developers building AI applications.\\nPromptLayer üç∞\\nPrompt Engineering platform. Collaborate, test, evaluate, and monitor your LLM applications\\nPromptMage\\nOpen-source tool to simplify the process of creating and managing LLM workflows and prompts as a self-hosted solution.\\nPrompteams\\nPrompt management system. Version, test, collaborate, and retrieve prompts through real-time APIs. Have GitHub style with repos, branches, and commits (and commit history).\\nprompttools\\nOpen-source tools for testing and experimenting with prompts. The core idea is to enable developers to evaluate prompts using familiar interfaces like code and notebooks. In just a few lines of codes, you can test your prompts and parameters across different models (whether you are using OpenAI, Anthropic, or LLaMA models). You can even evaluate the retrieval accuracy of vector databases.\\nTreeScale\\nAll In One Dev Platform For LLM Apps. Deploy LLM-enhanced APIs seamlessly using tools for prompt optimization, semantic querying, version management, statistical evaluation, and performance tracking. As a part of the developer friendly API implementation TreeScale offers Elastic LLM product, which makes a unified API Endpoint for all major LLM providers and open source models.\\nTrueFoundry\\nDeploy LLMOps tools like Vector DBs, Embedding server etc on your own Kubernetes (EKS,AKS,GKE,On-prem) Infra including deploying, Fine-tuning, tracking Prompts and serving Open Source LLM Models with full Data Security and Optimal GPU Management. Train and Launch your LLM Application at Production scale with best Software Engineering practices.\\nReliableGPT üí™\\nHandle OpenAI Errors (overloaded OpenAI servers, rotated keys, or context window errors) for your production LLM Applications.\\nPortkey\\nControl Panel with an observability suite & an AI gateway ‚Äî to ship fast, reliable, and cost-efficient apps.\\nVellum\\nAn AI product development platform to experiment with, evaluate, and deploy advanced LLM apps.\\nWeights & Biases (Prompts)\\nA suite of LLMOps tools within the developer-first W&B MLOps platform. Utilize W&B Prompts for visualizing and inspecting LLM execution flow, tracking inputs and outputs, viewing intermediate results, securely managing prompts and LLM chain configurations.\\nWordware\\nA web-hosted IDE where non-technical domain experts work with AI Engineers to build task-specific AI agents. It approaches prompting as a new programming language rather than low/no-code blocks.\\nxTuring\\nBuild and control your personal LLMs with fast and efficient fine-tuning.\\nZenML\\nOpen-source framework for orchestrating, experimenting and deploying production-grade ML solutions, with built-in\\nlangchain\\n&\\nllama_index\\nintegrations.\\n‚¨Ü back to ToC\\nSearch\\nVector search\\nProject\\nDetails\\nRepository\\nAquilaDB\\nAn easy to use Neural Search Engine. Index latent vectors along with JSON metadata and do efficient k-NN search.\\nAwadb\\nAI Native database for embedding vectors\\nChroma\\nthe open source embedding database\\nInfinity\\nThe AI-native database built for LLM applications, providing incredibly fast vector and full-text search\\nLancedb\\nDeveloper-friendly, serverless vector database for AI applications. Easily add long-term memory to your LLM apps!\\nMarqo\\nTensor search for humans.\\nMilvus\\nVector database for scalable similarity search and AI applications.\\nPinecone\\nThe Pinecone vector database makes it easy to build high-performance vector search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles.\\npgvector\\nOpen-source vector similarity search for Postgres.\\npgvecto.rs\\nVector database plugin for Postgres, written in Rust, specifically designed for LLM.\\nQdrant\\nVector Search Engine and Database for the next generation of AI applications. Also available in the cloud\\ntxtai\\nBuild AI-powered semantic search applications\\nVald\\nA Highly Scalable Distributed Vector Search Engine\\nVearch\\nA distributed system for embedding-based vector retrieval\\nVectorDB\\nA Python vector database you just need - no more, no less.\\nVellum\\nA managed service for ingesting documents and performing hybrid semantic/keyword search across them. Comes with out-of-box support for OCR, text chunking, embedding model experimentation, metadata filtering, and production-grade APIs.\\nWeaviate\\nWeaviate is an open source vector search engine that stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance and scalability of a cloud-native database, all accessible through GraphQL, REST, and various language clients.\\n‚¨Ü back to ToC\\nCode AI\\nProject\\nDetails\\nRepository\\nCodeGeeX\\nCodeGeeX: An Open Multilingual Code Generation Model (KDD 2023)\\nCodeGen\\nCodeGen is an open-source model for program synthesis. Trained on TPU-v4. Competitive with OpenAI Codex.\\nCodeT5\\nOpen Code LLMs for Code Understanding and Generation.\\nContinue\\n‚è© the open-source autopilot for software development‚Äîbring the power of ChatGPT to VS Code\\nfauxpilot\\nAn open-source alternative to GitHub Copilot server\\ntabby\\nSelf-hosted AI coding assistant. An opensource / on-prem alternative to GitHub Copilot.\\nTraining\\nIDEs and Workspaces\\nProject\\nDetails\\nRepository\\ncode server\\nRun VS Code on any machine anywhere and access it in the browser.\\nconda\\nOS-agnostic, system-level binary package manager and ecosystem.\\nDocker\\nMoby is an open-source project created by Docker to enable and accelerate software containerization.\\nenvd\\nüèïÔ∏è Reproducible development environment for AI/ML.\\nJupyter Notebooks\\nThe Jupyter notebook is a web-based notebook environment for interactive computing.\\nKurtosis\\nA build, packaging, and run system for ephemeral multi-container environments.\\nWordware\\nA web-hosted IDE where non-technical domain experts work with AI Engineers to build task-specific AI agents. It approaches prompting as a new programming language rather than low/no-code blocks.\\n‚¨Ü back to ToC\\nFoundation Model Fine Tuning\\nProject\\nDetails\\nRepository\\nalpaca-lora\\nInstruct-tune LLaMA on consumer hardware\\nfinetuning-scheduler\\nA PyTorch Lightning extension that accelerates and enhances foundation model experimentation with flexible fine-tuning schedules.\\nFlyflow\\nOpen source, high performance fine tuning as a service for GPT4 quality models with 5x lower latency and 3x lower cost\\nLMFlow\\nAn Extensible Toolkit for Finetuning and Inference of Large Foundation Models\\nLora\\nUsing Low-rank adaptation to quickly fine-tune diffusion models.\\npeft\\nState-of-the-art Parameter-Efficient Fine-Tuning.\\np-tuning-v2\\nAn optimized prompt tuning strategy achieving comparable performance to fine-tuning on small/medium-sized models and sequence tagging challenges.\\n(ACL 2022)\\nQLoRA\\nEfficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.\\nTRL\\nTrain transformer language models with reinforcement learning.\\n‚¨Ü back to ToC\\nFrameworks for Training\\nProject\\nDetails\\nRepository\\nAccelerate\\nüöÄ A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision.\\nApache MXNet\\nLightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler.\\naxolotl\\nA tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.\\nCaffe\\nA fast open framework for deep learning.\\nCandle\\nMinimalist ML framework for Rust .\\nColossalAI\\nAn integrated large-scale model training system with efficient parallelization techniques.\\nDeepSpeed\\nDeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\\nHorovod\\nDistributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\\nJax\\nAutograd and XLA for high-performance machine learning research.\\nKedro\\nKedro is an open-source Python framework for creating reproducible, maintainable and modular data science code.\\nKeras\\nKeras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.\\nLightGBM\\nA fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\\nMegEngine\\nMegEngine is a fast, scalable and easy-to-use deep learning framework, with auto-differentiation.\\nmetric-learn\\nMetric Learning Algorithms in Python.\\nMindSpore\\nMindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\\nOneflow\\nOneFlow is a performance-centered and open-source deep learning framework.\\nPaddlePaddle\\nMachine Learning Framework from Industrial Practice.\\nPyTorch\\nTensors and Dynamic neural networks in Python with strong GPU acceleration.\\nPyTorch Lightning\\nDeep learning framework to train, deploy, and ship AI products Lightning fast.\\nXGBoost\\nScalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library.\\nscikit-learn\\nMachine Learning in Python.\\nTensorFlow\\nAn Open Source Machine Learning Framework for Everyone.\\nVectorFlow\\nA minimalist neural network library optimized for sparse data and single machine environments.\\n‚¨Ü back to ToC\\nExperiment Tracking\\nProject\\nDetails\\nRepository\\nAim\\nan easy-to-use and performant open-source experiment tracker.\\nClearML\\nAuto-Magical CI/CD to streamline your ML workflow. Experiment Manager, MLOps and Data-Management\\nComet\\nComet is an MLOps platform that offers experiment tracking, model production management, a model registry, and full data lineage from training straight through to production. Comet plays nicely with all your favorite tools, so you don\\'t have to change your existing workflow. Comet Opik to confidently evaluate, test, and ship LLM applications with a suite of observability tools to calibrate language model outputs across your dev and production lifecycle!\\nGuild AI\\nExperiment tracking, ML developer tools.\\nMLRun\\nMachine Learning automation and tracking.\\nKedro-Viz\\nKedro-Viz is an interactive development tool for building data science pipelines with Kedro. Kedro-Viz also allows users to view and compare different runs in the Kedro project.\\nLabNotebook\\nLabNotebook is a tool that allows you to flexibly monitor, record, save, and query all your machine learning experiments.\\nSacred\\nSacred is a tool to help you configure, organize, log and reproduce experiments.\\nWeights & Biases\\nA developer first, lightweight, user-friendly experiment tracking and visualization tool for machine learning projects, streamlining collaboration and simplifying MLOps. W&B excels at tracking LLM-powered applications, featuring W&B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and LLM chain configurations.\\n‚¨Ü back to ToC\\nVisualization\\nProject\\nDetails\\nRepository\\nFiddler AI\\nRich dashboards, reports, and UMAP to perform root cause analysis, pinpoint problem areas, like correctness, safety, and privacy issues, and improve LLM outcomes.\\nManiford\\nA model-agnostic visual debugging tool for machine learning.\\nnetron\\nVisualizer for neural network, deep learning, and machine learning models.\\nOpenOps\\nBring multiple data streams into one dashboard.\\nTensorBoard\\nTensorFlow\\'s Visualization Toolkit.\\nTensorSpace\\nNeural network 3D visualization framework, build interactive and intuitive model in browsers, support pre-trained deep learning models from TensorFlow, Keras, TensorFlow.js.\\ndtreeviz\\nA python library for decision tree visualization and model interpretation.\\nZetane Viewer\\nML models and internal tensors 3D visualizer.\\nZeno\\nAI evaluation platform for interactively exploring data and model outputs.\\nModel Editing\\nProject\\nDetails\\nRepository\\nFastEdit\\nFastEdit aims to assist developers with injecting fresh and customized knowledge into large language models efficiently using one single command.\\n‚¨Ü back to ToC\\nData\\nData Management\\nProject\\nDetails\\nRepository\\nArtiVC\\nA version control system to manage large files. Lake is a dataset format with a simple API for creating, storing, and collaborating on AI datasets of any size.\\nDolt\\nGit for Data.\\nDVC\\nData Version Control - Git for Data & Models - ML Experiments Management.\\nDelta-Lake\\nStorage layer that brings scalable, ACID transactions to Apache Spark and other engines.\\nPachyderm\\nPachyderm is a version control system for data.\\nQuilt\\nA self-organizing data hub for S3.\\n‚¨Ü back to ToC\\nData Storage\\nProject\\nDetails\\nRepository\\nJuiceFS\\nA distributed POSIX file system built on top of Redis and S3.\\nLakeFS\\nGit-like capabilities for your object storage.\\nLance\\nModern columnar data format for ML implemented in Rust.\\n‚¨Ü back to ToC\\nData Tracking\\nProject\\nDetails\\nRepository\\nPiperider\\nA CLI tool that allows you to build data profiles and write assertion tests for easily evaluating and tracking your data\\'s reliability over time.\\nLUX\\nA Python library that facilitates fast and easy data exploration by automating the visualization and data analysis process.\\n‚¨Ü back to ToC\\nFeature Engineering\\nProject\\nDetails\\nRepository\\nFeatureform\\nThe Virtual Feature Store. Turn your existing data infrastructure into a feature store.\\nFeatureTools\\nAn open source python framework for automated feature engineering\\n‚¨Ü back to ToC\\nData/Feature enrichment\\nProject\\nDetails\\nRepository\\nUpgini\\nFree automated data & feature enrichment library for machine learning: automatically searches through thousands of ready-to-use features from public and community shared data sources and enriches your training dataset with only the accuracy improving features\\nFeast\\nAn open source feature store for machine learning.\\ndistilabel\\n‚öóÔ∏è distilabel is a framework for synthetic data and AI feedback for AI engineers that require high-quality outputs, full data ownership, and overall efficiency.\\n‚¨Ü back to ToC\\nLarge Scale Deployment\\nML Platforms\\nProject\\nDetails\\nRepository\\nComet\\nComet is an MLOps platform that offers experiment tracking, model production management, a model registry, and full data lineage from training straight through to production. Comet plays nicely with all your favorite tools, so you don\\'t have to change your existing workflow. Comet Opik to confidently evaluate, test, and ship LLM applications with a suite of observability tools to calibrate language model outputs across your dev and production lifecycle!\\nClearML\\nAuto-Magical CI/CD to streamline your ML workflow. Experiment Manager, MLOps and Data-Management.\\nHopsworks\\nHopsworks is a MLOps platform for training and operating large and small ML systems, including fine-tuning and serving LLMs. Hopsworks includes both a feature store and vector database for RAG.\\nOpenLLM\\nAn open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.\\nMLflow\\nOpen source platform for the machine learning lifecycle.\\nMLRun\\nAn open MLOps platform for quickly building and managing continuous ML applications across their lifecycle.\\nModelFox\\nModelFox is a platform for managing and deploying machine learning models.\\nKserve\\nStandardized Serverless ML Inference Platform on Kubernetes\\nKubeflow\\nMachine Learning Toolkit for Kubernetes.\\nPAI\\nResource scheduling and cluster management for AI.\\nPolyaxon\\nMachine Learning Management & Orchestration Platform.\\nPrimehub\\nAn effortless infrastructure for machine learning built on the top of Kubernetes.\\nOpenModelZ\\nOne-click machine learning deployment (LLM, text-to-image and so on) at scale on any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine).\\nSeldon-core\\nAn MLOps framework to package, deploy, monitor and manage thousands of production machine learning models\\nStarwhale\\nAn MLOps/LLMOps platform for model building, evaluation, and fine-tuning.\\nTrueFoundry\\nA PaaS to deploy, Fine-tune and serve LLM Models on a company‚Äôs own Infrastructure with Data Security and Optimal GPU and Cost Management. Launch your LLM Application at Production scale with best DevSecOps practices.\\nWeights & Biases\\nA lightweight and flexible platform for machine learning experiment tracking, dataset versioning, and model management, enhancing collaboration and streamlining MLOps workflows. W&B excels at tracking LLM-powered applications, featuring W&B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and LLM chain configurations.\\n‚¨Ü back to ToC\\nWorkflow\\nProject\\nDetails\\nRepository\\nAirflow\\nA platform to programmatically author, schedule and monitor workflows.\\naqueduct\\nAn Open-Source Platform for Production Data Science\\nArgo Workflows\\nWorkflow engine for Kubernetes.\\nFlyte\\nKubernetes-native workflow automation platform for complex, mission-critical data and ML processes at scale.\\nHamilton\\nA lightweight framework to represent ML/language model pipelines as a series of python functions.\\nKubeflow Pipelines\\nMachine Learning Pipelines for Kubeflow.\\nLangFlow\\nAn effortless way to experiment and prototype LangChain flows with drag-and-drop components and a chat interface.\\nMetaflow\\nBuild and manage real-life data science projects with ease!\\nPloomber\\nThe fastest way to build data pipelines. Develop iteratively, deploy anywhere.\\nPrefect\\nThe easiest way to automate your data.\\nVDP\\nAn open-source unstructured data ETL tool to streamline the end-to-end unstructured data processing pipeline.\\nZenML\\nMLOps framework to create reproducible pipelines.\\n‚¨Ü back to ToC\\nScheduling\\nProject\\nDetails\\nRepository\\nKueue\\nKubernetes-native Job Queueing.\\nPAI\\nResource scheduling and cluster management for AI (Open-sourced by Microsoft).\\nSlurm\\nA Highly Scalable Workload Manager.\\nVolcano\\nA Cloud Native Batch System (Project under CNCF).\\nYunikorn\\nLight-weight, universal resource scheduler for container orchestrator systems.\\n‚¨Ü back to ToC\\nModel Management\\nProject\\nDetails\\nRepository\\nComet\\nComet is an MLOps platform that offers Model Production Management, a Model Registry, and full model lineage from training straight through to production. Use Comet for model reproducibility, model debugging, model versioning, model visibility, model auditing, model governance, and model monitoring.\\ndvc\\nML Experiments Management - Data Version Control - Git for Data & Models\\nModelDB\\nOpen Source ML Model Versioning, Metadata, and Experiment Management\\nMLEM\\nA tool to package, serve, and deploy any ML model on any platform.\\normb\\nDocker for Your ML/DL Models Based on OCI Artifacts\\n‚¨Ü back to ToC\\nPerformance\\nML Compiler\\nProject\\nDetails\\nRepository\\nONNX-MLIR\\nCompiler technology to transform a valid Open Neural Network Exchange (ONNX) graph into code that implements the graph with minimum runtime support.\\nbitsandbytes\\nAccessible large language models via k-bit quantization for PyTorch.\\nTVM\\nOpen deep learning compiler stack for cpu, gpu and specialized accelerators\\n‚¨Ü back to ToC\\nProfiling\\nProject\\nDetails\\nRepository\\noctoml-profile\\noctoml-profile is a python library and cloud service designed to provide the simplest experience for assessing and optimizing the performance of PyTorch models on cloud hardware with state-of-the-art ML acceleration technology.\\nscalene\\na high-performance, high-precision CPU, GPU, and memory profiler for Python\\n‚¨Ü back to ToC\\nAutoML\\nProject\\nDetails\\nRepository\\nArchai\\na platform for Neural Network Search (NAS) that allows you to generate efficient deep networks for your applications.\\nautoai\\nA framework to find the best performing AI/ML model for any AI problem.\\nAutoGL\\nAn autoML framework & toolkit for machine learning on graphs\\nAutoGluon\\nAutoML for Image, Text, and Tabular Data.\\nautoml-gs\\nProvide an input CSV and a target field to predict, generate a model + code to run it.\\nAutoRAG\\nAutoML tool for RAG - Boost your LLM app performance with your own data\\nautokeras\\nAutoML library for deep learning.\\nAuto-PyTorch\\nAutomatic architecture search and hyperparameter optimization for PyTorch.\\nauto-sklearn\\nan automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.\\nDragonfly\\nAn open source python library for scalable Bayesian optimisation.\\nDetermined\\nscalable deep learning training platform with integrated hyperparameter tuning support; includes Hyperband, PBT, and other search methods.\\nDEvol (DeepEvolution)\\na basic proof of concept for genetic architecture search in Keras.\\nEvalML\\nAn open source python library for AutoML.\\nFEDOT\\nAutoML framework for the design of composite pipelines.\\nFLAML\\nFast and lightweight AutoML (\\npaper\\n).\\nGoptuna\\nA hyperparameter optimization framework, inspired by Optuna.\\nHpBandSter\\na framework for distributed hyperparameter optimization.\\nHPOlib2\\na library for hyperparameter optimization and black box optimization benchmarks.\\nHyperband\\nopen source code for tuning hyperparams with Hyperband.\\nHypernets\\nA General Automated Machine Learning Framework.\\nHyperopt\\nDistributed Asynchronous Hyperparameter Optimization in Python.\\nhyperunity\\nA toolset for black-box hyperparameter optimisation.\\nIntelli\\nA framework to connect a flow of ML models by applying graph theory.\\nKatib\\nKatib is a Kubernetes-native project for automated machine learning (AutoML).\\nKeras Tuner\\nHyperparameter tuning for humans.\\nlearn2learn\\nPyTorch Meta-learning Framework for Researchers.\\nLudwig\\na toolbox built on top of TensorFlow that allows to train and test deep learning models without the need to write code.\\nMOE\\na global, black box optimization engine for real world metric optimization by Yelp.\\nModel Search\\na framework that implements AutoML algorithms for model architecture search at scale.\\nNASGym\\na proof-of-concept OpenAI Gym environment for Neural Architecture Search (NAS).\\nNNI\\nAn open source AutoML toolkit for automate machine learning lifecycle, including feature engineering, neural architecture search, model compression and hyper-parameter tuning.\\nOptuna\\nA hyperparameter optimization framework.\\nPycaret\\nAn open-source, low-code machine learning library in Python that automates machine learning workflows.\\nRay Tune\\nScalable Hyperparameter Tuning.\\nREMBO\\nBayesian optimization in high-dimensions via random embedding.\\nRoBO\\na Robust Bayesian Optimization framework.\\nscikit-optimize(skopt)\\nSequential model-based optimization with a\\nscipy.optimize\\ninterface.\\nSpearmint\\na software package to perform Bayesian optimization.\\nTPOT\\none of the very first AutoML methods and open-source software packages.\\nTorchmeta\\nA Meta-Learning library for PyTorch.\\nVegas\\nan AutoML algorithm tool chain by Huawei Noah\\'s Arb Lab.\\n‚¨Ü back to ToC\\nOptimizations\\nProject\\nDetails\\nRepository\\nFeatherCNN\\nFeatherCNN is a high performance inference engine for convolutional neural networks.\\nForward\\nA library for high performance deep learning inference on NVIDIA GPUs.\\nNCNN\\nncnn is a high-performance neural network inference framework optimized for the mobile platform.\\nPocketFlow\\nuse AutoML to do model compression.\\nTensorFlow Model Optimization\\nA suite of tools that users, both novice and advanced, can use to optimize machine learning models for deployment and execution.\\nTNN\\nA uniform deep learning inference framework for mobile, desktop and server.\\noptimum-tpu\\nGoogle TPU optimizations for transformers models\\n‚¨Ü back to ToC\\nFederated ML\\nProject\\nDetails\\nRepository\\nEasyFL\\nAn Easy-to-use Federated Learning Platform\\nFATE\\nAn Industrial Grade Federated Learning Framework\\nFedML\\nThe federated learning and analytics library enabling secure and collaborative machine learning on decentralized data anywhere at any scale. Supporting large-scale cross-silo federated learning, cross-device federated learning on smartphones/IoTs, and research simulation.\\nFlower\\nA Friendly Federated Learning Framework\\nHarmonia\\nHarmonia is an open-source project aiming at developing systems/infrastructures and libraries to ease the adoption of federated learning (abbreviated to FL) for researches and production usage.\\nTensorFlow Federated\\nA framework for implementing federated learning\\n‚¨Ü back to ToC\\nAwesome Lists\\nProject\\nDetails\\nRepository\\nAwesome Argo\\nA curated list of awesome projects and resources related to Argo\\nAwesome AutoDL\\nAutomated Deep Learning: Neural Architecture Search Is Not the End (a curated list of AutoDL resources and an in-depth analysis)\\nAwesome AutoML\\nCurating a list of AutoML-related research, tools, projects and other resources\\nAwesome AutoML Papers\\nA curated list of automated machine learning papers, articles, tutorials, slides and projects\\nAwesome-Code-LLM\\nüë®\\u200düíª An awesome and curated list of best code-LLM for research.\\nAwesome Federated Learning Systems\\nA curated list of Federated Learning Systems related academic papers, articles, tutorials, slides and projects.\\nAwesome Federated Learning\\nA curated list of federated learning publications, re-organized from Arxiv (mostly)\\nawesome-federated-learning\\nacc\\nAll materials you need for Federated Learning: blogs, videos, papers, and softwares, etc.\\nAwesome Open MLOps\\nThis is the Fuzzy Labs guide to the universe of free and open source MLOps tools.\\nAwesome Production Machine Learning\\nA curated list of awesome open source libraries to deploy, monitor, version and scale your machine learning\\nAwesome Tensor Compilers\\nA list of awesome compiler projects and papers for tensor computation and deep learning.\\nkelvins/awesome-mlops\\nA curated list of awesome MLOps tools.\\nvisenger/awesome-mlops\\nMachine Learning Operations - An awesome list of references for MLOps\\ncurrentslab/awesome-vector-search\\nA curated list of awesome vector search framework/engine, library, cloud service and research papers to vector similarity search.\\npleisto/flappy\\nProduction-Ready LLM Agent SDK for Every Developer\\n‚¨Ü back to ToC\\nAbout\\nAn awesome & curated list of best LLMOps tools for developers\\nTopics\\nawesome-list\\nmlops\\nai-development-tools\\nllmops\\nResources\\nReadme\\nLicense\\nCC0-1.0 license\\nActivity\\nCustom properties\\nStars\\n3.9k\\nstars\\nWatchers\\n65\\nwatching\\nForks\\n373\\nforks\\nReport repository\\nReleases\\nNo releases published\\nPackages\\n0\\nNo packages published\\nContributors\\n84\\n+ 70 contributors\\nLanguages\\nShell\\n86.2%\\nPython\\n13.8%', 'tool_call_id': 'call_dfULUM0OKBERrMSN6JtLPQk2', 'name': 'ParseURLContent'}, {'role': 'tool', 'content': 'Failed to fetch content from URL: 403 Client Error: Forbidden for url: https://www.datacamp.com/blog/top-open-source-llms', 'tool_call_id': 'call_L1Xy8nuy2wRu2TpvU3DcEa0o', 'name': 'ParseURLContent'}]\n",
      "As a software engineer (SWE) looking for tools and libraries to develop with large language models (LLMs), there are several options available that can cater to various aspects of LLM development. Here are some notable libraries and development tools to consider:\n",
      "\n",
      "### 1. **LLMOps Tools**\n",
      "These tools help manage and operationalize the deployment and fine-tuning of LLMs. Some popular LLMOps platforms include:\n",
      "\n",
      "- **Weights & Biases (W&B)**: A comprehensive platform for tracking machine learning experiments, including LLMs. It provides tools for visualizing execution flows, managing prompts, and monitoring model performance.\n",
      "- **Valohai**: A platform that automates the machine learning lifecycle, making it easy to deploy and monitor models.\n",
      "- **ZenML**: An open-source framework to build reproducible ML pipelines with specific focus on MLOps and LLMs deployment.\n",
      "\n",
      "### 2. **Fine-Tuning Libraries**\n",
      "Libraries that enable you to fine-tune pre-trained LLMs for specific tasks:\n",
      "- **Hugging Face Transformers**: One of the most popular libraries for working with transformer models, it allows easy access to various pre-trained models and simple methods for fine-tuning.\n",
      "- **NVIDIA NeMo**: An open-source toolkit for building, training, and fine-tuning state-of-the-art neural network models, particularly LLMs.\n",
      "\n",
      "### 3. **Integration and Serving Frameworks**\n",
      "Tools that facilitate model deployment and integration into applications:\n",
      "- **Langchain**: A library designed for building applications that are powered by language models. It provides an easy way to chain together different components such as data loading, pre-processing, and model interaction.\n",
      "- **FastAPI**: Although not specific to LLMs, it allows for rapid development of APIs, making it easier to deploy LLMs as RESTful services.\n",
      "\n",
      "### 4. **Experiment Tracking and Management**\n",
      "A crucial aspect of ML development is tracking experiments and models:\n",
      "- **Comet**: A platform that provides tracking tools for experiments, metrics, and models in a collaborative environment.\n",
      "- **ClearML**: An open-source MLOps framework that helps in tracking and managing ML experiments and deployments.\n",
      "\n",
      "### 5. **Open-Source LLMs**\n",
      "If you're looking for open-source LLMs to work with, consider:\n",
      "- **LLaMA**: Developed by Meta, it is known for its efficiency and performance.\n",
      "- **GPT-NeoX**: A community-driven project to create open, large-scale LLMs.\n",
      "\n",
      "### 6. **Community Curated Lists**\n",
      "Websites like [Awesome LLMOps](https://github.com/tensorchord/awesome-llmops) on GitHub curate various tools and libraries helpful for LLM applications, covering everything from model training to deployment.\n",
      "\n",
      "### Additional Resources\n",
      "- **AIMultiple's Comprehensive Vendor Benchmark**: This article offers insights into top LLMOps tools and how they compare based on functionalities such as fine-tuning, deployment, model monitoring, and more.\n",
      "- **Open Source Libraries**: Explore curated lists like the [Awesome LLMOps](https://github.com/tensorchord/awesome-llmops) that contain various libraries and frameworks useful for development.\n",
      "\n",
      "### Conclusion\n",
      "Whether you're focusing on building, deploying, or managing LLMs, these tools provide a solid foundation to support your development process. Choosing the right combination of these tools can greatly enhance your workflow and allow for efficient LLM application development.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "e31ded9b1ef58436",
   "metadata": {},
   "source": [
    "\n",
    "Search Queries:\n",
    "\n",
    "* LLM development tool libraries\n",
    "* best libraries for LLM development\n",
    "* software engineering tools for LLM\n",
    "* open source LLM libraries for developers\n",
    "* programming libraries for large language models\n",
    "\n",
    "By prompting the LLM to generate multiple queries, the LLM has access to a wide range of relevant information, including both open-source and commercial products, which it would have a significantly lower chance of doing with a single query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "92e052d8f125dc84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:53:16.127288Z",
     "start_time": "2024-10-30T08:52:37.988663Z"
    }
   },
   "source": [
    "await WebAssistantBaseWithStream().run()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(User): What is Mirascope library?\n",
      "(Assistant): []\n",
      "using DuckDuckGoSearch tool with args: {'queries': ['Mirascope library', 'Mirascope library GitHub', 'Mirascope Python', 'What is Mirascope library used for']}\n",
      "using DuckDuckGoSearch tool with args: {'queries': ['Mirascope library overview', 'Mirascope library features', 'Mirascope library documentation']}\n",
      "[{'role': 'user', 'content': 'What is Mirascope library?'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\": [\"Mirascope library\", \"Mirascope library GitHub\", \"Mirascope Python\", \"What is Mirascope library used for\"]}', 'name': 'DuckDuckGoSearch'}, 'id': 'call_gord7bRkL9CHVP1xnApVzWjG'}, {'type': 'function', 'function': {'arguments': '{\"queries\": [\"Mirascope library overview\", \"Mirascope library features\", \"Mirascope library documentation\"]}', 'name': 'DuckDuckGoSearch'}, 'id': 'call_nheFZBlZk6hi7hCZpoLqwiXk'}]}, {'role': 'tool', 'content': \"Title: Mirascope | LLM abstractions that aren't obstructions\\nURL: https://www.mirascope.com/\\nSnippet: Vince Trost. Co-Founder / Plastic Labs. Mirascope's simplicity made it the natural next step from a provider's API (OpenAI) ‚Äî all without fighting unnecessary complexity of other tools like LangChain. We have all the bells and whistles necessary for production while maintaining ease of use and easy onboarding of new team members.\\n\\nTitle: Mirascope/mirascope: LLM abstractions that aren't obstructions - GitHub\\nURL: https://github.com/mirascope/mirascope\\nSnippet: Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, Vertex AI, and Bedrock.. Whether you're generating text, extracting structured information, or developing complex AI-driven agent ...\\n\\nTitle: Mirascope/mirascope: LLM abstractions that aren't obstructions - GitHub\\nURL: https://github.com/mirascope/mirascope\\nSnippet: Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, Vertex AI, and Bedrock.. Whether you're generating text, extracting structured information, or developing complex AI-driven agent ...\\n\\nTitle: Mirascope - GitHub\\nURL: https://github.com/mirascope\\nSnippet: Building the future of LLM developer tools. Mirascope has 6 repositories available. Follow their code on GitHub.\\n\\nTitle: Mirascope | LLM abstractions that aren't obstructions\\nURL: https://www.mirascope.com/\\nSnippet: Mirascope is a Python toolkit that provides the right level of abstraction for building LLM agents your way. Modular. Extensible. Reliable. Open Source. pip install mirascope. ... Mirascope's simplicity made it the natural next step from a provider's API (OpenAI) ‚Äî all without fighting unnecessary complexity of other tools like LangChain. ...\\n\\nTitle: Mirascope/mirascope: LLM abstractions that aren't obstructions - GitHub\\nURL: https://github.com/mirascope/mirascope\\nSnippet: Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, Vertex AI, and Bedrock.. Whether you're generating text, extracting structured information, or developing complex AI-driven agent ...\\n\\nTitle: Welcome to Mirascope - Mirascope\\nURL: https://www.mirascope.com/docs/\\nSnippet: Mirascope¬∂. LLM abstractions that aren't obstructions. Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, Vertex AI, and Bedrock.. Whether you're generating text, extracting ...\\n\\nTitle: mirascope/README.md at main ¬∑ Mirascope/mirascope - GitHub\\nURL: https://github.com/Mirascope/mirascope/blob/main/README.md\\nSnippet: LLM abstractions that aren't obstructions. Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM ...\", 'tool_call_id': 'call_gord7bRkL9CHVP1xnApVzWjG', 'name': 'DuckDuckGoSearch'}, {'role': 'tool', 'content': \"<class 'duckduckgo_search.exceptions.RatelimitException'>: Failed to search the web for text\", 'tool_call_id': 'call_nheFZBlZk6hi7hCZpoLqwiXk', 'name': 'DuckDuckGoSearch'}]\n",
      "using ParseURLContent tool with args: {'url': 'https://www.mirascope.com/'}\n",
      "using ParseURLContent tool with args: {'url': 'https://github.com/mirascope/mirascope'}\n",
      "[{'role': 'user', 'content': 'What is Mirascope library?'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\": [\"Mirascope library\", \"Mirascope library GitHub\", \"Mirascope Python\", \"What is Mirascope library used for\"]}', 'name': 'DuckDuckGoSearch'}, 'id': 'call_gord7bRkL9CHVP1xnApVzWjG'}, {'type': 'function', 'function': {'arguments': '{\"queries\": [\"Mirascope library overview\", \"Mirascope library features\", \"Mirascope library documentation\"]}', 'name': 'DuckDuckGoSearch'}, 'id': 'call_nheFZBlZk6hi7hCZpoLqwiXk'}]}, {'role': 'tool', 'content': \"Title: Mirascope | LLM abstractions that aren't obstructions\\nURL: https://www.mirascope.com/\\nSnippet: Vince Trost. Co-Founder / Plastic Labs. Mirascope's simplicity made it the natural next step from a provider's API (OpenAI) ‚Äî all without fighting unnecessary complexity of other tools like LangChain. We have all the bells and whistles necessary for production while maintaining ease of use and easy onboarding of new team members.\\n\\nTitle: Mirascope/mirascope: LLM abstractions that aren't obstructions - GitHub\\nURL: https://github.com/mirascope/mirascope\\nSnippet: Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, Vertex AI, and Bedrock.. Whether you're generating text, extracting structured information, or developing complex AI-driven agent ...\\n\\nTitle: Mirascope/mirascope: LLM abstractions that aren't obstructions - GitHub\\nURL: https://github.com/mirascope/mirascope\\nSnippet: Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, Vertex AI, and Bedrock.. Whether you're generating text, extracting structured information, or developing complex AI-driven agent ...\\n\\nTitle: Mirascope - GitHub\\nURL: https://github.com/mirascope\\nSnippet: Building the future of LLM developer tools. Mirascope has 6 repositories available. Follow their code on GitHub.\\n\\nTitle: Mirascope | LLM abstractions that aren't obstructions\\nURL: https://www.mirascope.com/\\nSnippet: Mirascope is a Python toolkit that provides the right level of abstraction for building LLM agents your way. Modular. Extensible. Reliable. Open Source. pip install mirascope. ... Mirascope's simplicity made it the natural next step from a provider's API (OpenAI) ‚Äî all without fighting unnecessary complexity of other tools like LangChain. ...\\n\\nTitle: Mirascope/mirascope: LLM abstractions that aren't obstructions - GitHub\\nURL: https://github.com/mirascope/mirascope\\nSnippet: Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, Vertex AI, and Bedrock.. Whether you're generating text, extracting structured information, or developing complex AI-driven agent ...\\n\\nTitle: Welcome to Mirascope - Mirascope\\nURL: https://www.mirascope.com/docs/\\nSnippet: Mirascope¬∂. LLM abstractions that aren't obstructions. Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, Vertex AI, and Bedrock.. Whether you're generating text, extracting ...\\n\\nTitle: mirascope/README.md at main ¬∑ Mirascope/mirascope - GitHub\\nURL: https://github.com/Mirascope/mirascope/blob/main/README.md\\nSnippet: LLM abstractions that aren't obstructions. Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM ...\", 'tool_call_id': 'call_gord7bRkL9CHVP1xnApVzWjG', 'name': 'DuckDuckGoSearch'}, {'role': 'tool', 'content': \"<class 'duckduckgo_search.exceptions.RatelimitException'>: Failed to search the web for text\", 'tool_call_id': 'call_nheFZBlZk6hi7hCZpoLqwiXk', 'name': 'DuckDuckGoSearch'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"url\": \"https://www.mirascope.com/\"}', 'name': 'ParseURLContent'}, 'id': 'call_u3s9q0XG450PfTD2ttR5H4Tl'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://github.com/mirascope/mirascope\"}', 'name': 'ParseURLContent'}, 'id': 'call_XJCo6imHPHCRZL1hc16L31xj'}]}, {'role': 'tool', 'content': 'Join our beta list!\\nGet updates and early access to try out new features as a beta tester.\\nThank you! Your submission has been received!\\nOops! Something went wrong while submitting the form.\\nCore Components\\nCalls\\nStreams\\nTools\\nStreaming Tools\\nStructured Outputs\\nStructured Streaming\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n\"gpt-4o-mini\"\\n)\\n4\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n5\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n6\\n7\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n8\\nprint\\n(response)\\n9\\n# > Sure! I would recommend The Name of the Wind by...\\n1\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n)\\n2\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n3\\n\"\"\"Recommend a {genre} book.\"\"\"\\n4\\n5\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n6\\nfor\\nchunk, _\\nin\\nstream:\\n7\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n8\\n# > Sure! I would recommend...\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\ndef\\nformat_book\\n(\\ntitle:\\nstr\\n, author:\\nstr\\n):\\n4\\nreturn\\nf\"\\n{title}\\nby\\n{author}\\n\"\\n5\\n6\\n@openai.call(\\n7\\n\"gpt-4o-mini\"\\n,\\n8\\ntools=[format_book],\\n9\\ntool_choice=\\n\"required\"\\n,\\n10\\n)\\n11\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n12\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n13\\n14\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n15\\nif\\ntool := response.tool\\n16\\nprint\\n(tool.call())\\n17\\n# > The Name of the Wind by Patrick Rothfuss\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n4\\n\"gpt-4o-mini\"\\n,\\n5\\nstream=\\nTrue\\n,\\n6\\ntools=[format_book],\\n7\\ntool_choice=\\n\"required\"\\n8\\n)\\n9\\n@prompt_template(\\n\"Recommend two (2) {genre} books\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\nchunk, tool\\nin\\nstream:\\n14\\nif\\ntool:\\n15\\nprint\\n(tool.call())\\n16\\nelse\\n:\\n17\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n18\\n# > The Name of the Wind by Patrick Rothfuss\\n19\\n# > Mistborn: The Final Empire by Brandon Sanderson\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n11\\n12\\nbook = recommend_book(\\n\"fantasy\"\\n)\\n13\\nassert\\nisinstance\\n(book, Book)\\n14\\nprint\\n(book)\\n15\\n# > title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nbook_stream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\npartial_book\\nin\\nbook_stream:\\n14\\nprint\\n(partial_book)\\n15\\n# > title=None author=None\\n16\\n# > title=\\'The Name\\' author=None\\n17\\n# > title=\\'The Name of the Wind\\' author=None\\n18\\n# > title=\\'The Name of the Wind\\' author=\\'Patrick\\'\\n19\\n# > title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n...and more!\\nExamples\\nYou\\'re in control\\nSimple and transparent abstractions ensure you\\'re in the driver\\'s seat. Abstractions must be useful without getting in your way and blinding your view.\\nType hints you\\'ll love\\nWe√¢\\x80\\x99ve taken care of all of the annoying Python typing so that you can have proper type hints in as simple an interface as possible.\\nBuild\\nyour\\nway\\nSimple, clean building blocks enable fast and reliable development without forcing you to do things our way. While we have recommendations and best practices, you\\'re still always able to do things\\nyour\\nway to best suit\\nyour\\nneeds.\\nTrusted by engineers building the next generation of AI-native applications\\nThe Pydantic inspired LLM framework the space has been missing. Simple, modular, extensible...helps where you need it, stays out of your way when you don\\'t.\\nVince Trost\\nCo-Founder / Plastic Labs\\nMirascope\\'s simplicity made it the natural next step from a provider\\'s API (OpenAI) √¢\\x80\\x94 all without fighting unnecessary complexity of other tools like LangChain. We have all the bells and whistles necessary for production while maintaining ease of use and easy onboarding of new team members.\\nJake Duth\\nCo-Founder &√Ç\\xa0CTO / Reddy\\nAs the author of Mirascope, I\\'m certainly biased, but there\\'s a reason I\\'ve worked day and night on this package √¢\\x80\\x94 it solves the problems I have with existing developer tools. Mirascope is simple and easy to use, and honestly building with Mirascope is the most fun I\\'ve had coding in my life.\\nWilliam Bakst\\nFounder / Mirascope\\nStart Building With Mirascope\\nMirascope can help improve your application development experience no matter how simple or complex.\\nIt will feel like writing the Python you already know.\\nRead the Docs\\nGitHub | √¢\\xad\\x90 ${starCount}\\nJoin the Community', 'tool_call_id': 'call_u3s9q0XG450PfTD2ttR5H4Tl', 'name': 'ParseURLContent'}, {'role': 'tool', 'content': 'Mirascope\\n/\\nmirascope\\nPublic\\nNotifications\\nYou must be signed in to change notification settings\\nFork\\n47\\nStar\\n741\\nLLM abstractions that aren\\'t obstructions\\nmirascope.com/docs\\nLicense\\nMIT license\\n741\\nstars\\n47\\nforks\\nBranches\\nTags\\nActivity\\nStar\\nNotifications\\nYou must be signed in to change notification settings\\nMirascope/mirascope\\nmain\\nBranches\\nTags\\nGo to file\\nCode\\nFolders and files\\nName\\nName\\nLast commit message\\nLast commit date\\nLatest commit\\nHistory\\n2,704 Commits\\n.github\\n.github\\n.vscode\\n.vscode\\ndocs\\ndocs\\nexamples\\nexamples\\nmirascope\\nmirascope\\ntests\\ntests\\n.gitignore\\n.gitignore\\n.pre-commit-config.yaml\\n.pre-commit-config.yaml\\nLICENSE\\nLICENSE\\nREADME.md\\nREADME.md\\nmkdocs.yml\\nmkdocs.yml\\npyproject.toml\\npyproject.toml\\nuv.lock\\nuv.lock\\nView all files\\nRepository files navigation\\nMirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including\\nOpenAI\\n,\\nAnthropic\\n,\\nMistral\\n,\\nGemini\\n,\\nGroq\\n,\\nCohere\\n,\\nLiteLLM\\n,\\nAzure AI\\n,\\nVertex AI\\n, and\\nBedrock\\n.\\nWhether you\\'re generating text, extracting structured information, or developing complex AI-driven agent systems, Mirascope provides the tools you need to streamline your development process and create powerful, robust applications.\\n30 Second Quickstart\\nInstall Mirascope, specifying the provider(s) you intend to use, and set your API key:\\npip install\\n\"\\nmirascope[openai]\\n\"\\nexport\\nOPENAI_API_KEY=XXXXX\\nMake your first call to an LLM to extract the title and author of a book from unstructured text:\\nfrom\\nmirascope\\n.\\ncore\\nimport\\nopenai\\nfrom\\npydantic\\nimport\\nBaseModel\\nclass\\nBook\\n(\\nBaseModel\\n):\\ntitle\\n:\\nstr\\nauthor\\n:\\nstr\\n@\\nopenai\\n.\\ncall\\n(\\n\"gpt-4o-mini\"\\n,\\nresponse_model\\n=\\nBook\\n)\\ndef\\nextract_book\\n(\\ntext\\n:\\nstr\\n)\\n->\\nstr\\n:\\nreturn\\nf\"Extract\\n{\\ntext\\n}\\n\"\\nbook\\n=\\nextract_book\\n(\\n\"The Name of the Wind by Patrick Rothfuss\"\\n)\\nassert\\nisinstance\\n(\\nbook\\n,\\nBook\\n)\\nprint\\n(\\nbook\\n)\\n# Output: title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\nTutorials\\nCheck out our\\nquickstart tutorial\\nand many other tutorials for an interactive way to getting started with Mirascope.\\nUsage\\nFor a complete guide on how to use all of the various features Mirascope has to offer, read through our\\nLearn\\ndocumentation.\\nVersioning\\nMirascope uses\\nSemantic Versioning\\n.\\nLicence\\nThis project is licensed under the terms of the\\nMIT License\\n.\\nAbout\\nLLM abstractions that aren\\'t obstructions\\nmirascope.com/docs\\nTopics\\npython\\nartificial-intelligence\\ndeveloper-tools\\nhacktoberfest\\nllm\\nprompt-engineering\\nllm-agent\\nllm-tools\\nfunction-calling\\npydantic-v2\\nResources\\nReadme\\nLicense\\nMIT license\\nActivity\\nCustom properties\\nStars\\n741\\nstars\\nWatchers\\n13\\nwatching\\nForks\\n47\\nforks\\nReport repository\\nReleases\\n85\\nv1.6.1\\nLatest\\nOct 26, 2024\\n+ 84 releases\\nContributors\\n20\\n+ 6 contributors\\nLanguages\\nPython\\n100.0%', 'tool_call_id': 'call_XJCo6imHPHCRZL1hc16L31xj', 'name': 'ParseURLContent'}]\n",
      "Mirascope is a powerful and user-friendly Python library designed for simplifying the development and deployment of applications that utilize large language models (LLMs). It provides a unified interface that supports multiple model providers, including popular ones like OpenAI, Anthropic, Mistral, and Azure AI. Here are some key features and details about Mirascope:\n",
      "\n",
      "### Key Features\n",
      "1. **Unified Interface**: Mirascope allows developers to work with various LLMs through a single, consistent API, making it easier to switch between different providers without extensive code changes.\n",
      "2. **Modular Design**: The library is designed for extensibility, allowing developers to build their applications using simple, clean building blocks. This modularity helps in maintaining flexibility during development.\n",
      "3. **Structured Outputs**: Mirascope enables the extraction of structured information from unstructured text, which is particularly useful for tasks like summarization and information retrieval.\n",
      "4. **Developer-Friendly**: The library incorporates Python type hints and integrates Pydantic for model validation, making it intuitive and straightforward for Python developers to use.\n",
      "5. **Open Source**: It is openly accessible under the MIT License, and developers can contribute to its ongoing development via its GitHub repository.\n",
      "\n",
      "### Use Cases\n",
      "- **Text Generation**: Mirascope can generate creative text, such as stories or articles, leveraging the capabilities of the integrated LLMs.\n",
      "- **Information Extraction**: Users can easily extract specific information (like book titles and authors) from raw text, making it useful for applications in data analysis or content aggregation.\n",
      "- **AI-driven Agents**: The library facilitates the creation of AI agents that can perform sophisticated tasks, driven by the capabilities of the underlying LLMs.\n",
      "\n",
      "### Getting Started\n",
      "To begin using Mirascope, you would typically install it via `pip`, setting your API keys for the required providers, and then you can start integrating LLM capabilities into your applications. For example, it simplifies making calls to LLMs and handling responses.\n",
      "\n",
      "### Documentation and Community\n",
      "For more detailed documentation, tutorials, and examples, you can visit the official website or the [GitHub repository](https://github.com/mirascope/mirascope). The community around Mirascope appears to be active, fostering collaboration and improvements.\n",
      "\n",
      "You can find further details and installation instructions on their [official site](https://www.mirascope.com/).\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "3023adf49ca37aaf",
   "metadata": {},
   "source": [
    "\n",
    "Search Queries:\n",
    "\n",
    "* Mirascope library\n",
    "* Mirascope library GitHub\n",
    "* Mirascope Python\n",
    "* What is Mirascope library used for\n",
    "* Mirascope library overview\n",
    "* Mirascope library features\n",
    "* Mirascope library documentation\n",
    "\n",
    "The LLM can gather information regarding the Mirascope library but has no context beyond that.\n",
    "\n",
    "Let's take a look at what happens when we call the user queries together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "333b15e2f52a5fac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:06:27.575342Z",
     "start_time": "2024-09-30T17:06:22.536808Z"
    }
   },
   "outputs": [],
   "source": [
    "await WebAssistantBaseWithStream().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f181f6e0a3e7b",
   "metadata": {},
   "source": [
    "\n",
    "Search Queries:\n",
    "\n",
    "* Mirascope library\n",
    "* Mirascope LLM development\n",
    "* Mirascope open source\n",
    "* Mirascope Python library\n",
    "* LLM tools Mirascope\n",
    "\n",
    "By giving the LLM search history, these search queries now connect the Mirascope library specifically to LLM development tools,\n",
    "providing a more cohesive set of results.\n",
    "\n",
    "We can now create our `_step` and `run` functions which will call our `_stream` and `_step` functions respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e265cea2c2711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebAssistant(WebAssistantBaseWithStream):\n",
    "    async def _step(self, question: str):\n",
    "        print(self.messages)\n",
    "        response = await self._stream(question)\n",
    "        tools_and_outputs = []\n",
    "        async for chunk, tool in response:\n",
    "            if tool:\n",
    "                print(f\"using {tool._name()} tool with args: {tool.args}\")\n",
    "                tools_and_outputs.append((tool, tool.call()))\n",
    "            else:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "        if response.user_message_param:\n",
    "            self.messages.append(response.user_message_param)\n",
    "        self.messages.append(response.message_param)\n",
    "        if tools_and_outputs:\n",
    "            self.messages += response.tool_message_params(tools_and_outputs)\n",
    "            await self._step(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aae19c824b5a90",
   "metadata": {},
   "source": [
    "The `run` function will keep running until the LLM feels that the users question can be answered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e57b88407592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_assistant = WebAssistant()\n",
    "await web_assistant.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f46b3ed952908",
   "metadata": {},
   "source": [
    "Note that by giving the LLM the current date, it'll automatically search for the most up-to-date information.\n",
    "\n",
    "Check out [Evaluating Web Search Agent](../evals/evaluating_web_search_agent.ipynb) for an in-depth guide on how we evaluate the quality of our agent.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "<p class=\"admonition-title\">Additional Real-World Applications</p>\n",
    "<ol>\n",
    "<li><p><b>Advanced Research Assistant</b></p>\n",
    "<ul>\n",
    "<li>Stay updated on latest developments in rapidly evolving fields</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><p><b>Personalized Education</b></p>\n",
    "<ul>\n",
    "<li>Create customized learning materials based on current curricula</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><p><b>Business Intelligence</b></p>\n",
    "<ul>\n",
    "<li>Assist in data-driven decision making with real-time insights</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><p><b>Technical Support and Troubleshooting</b></p>\n",
    "<ul>\n",
    "<li>Assist in debugging by referencing current documentation</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><p><b>Travel Planning</b></p>\n",
    "<ul>\n",
    "<li>Provide updates on travel restrictions, local events, and weather</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><p><b>Journalism and Fact-Checking</b></p>\n",
    "<ul>\n",
    "<li>Help identify and combat misinformation</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><p><b>Environmental Monitoring</b></p>\n",
    "<ul>\n",
    "<li>Track and analyze current climate data</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "When adapting this recipe, consider:\n",
    "\n",
    "* Optimizing the search by utilizing `async` to increase parallelism.\n",
    "* When targeting specific websites for scraping purposes, use `response_model` to extract the specific information you're looking for across websites with similar content.\n",
    "* Implement a feedback loop so the LLM can rewrite the query for better search results.\n",
    "* Reduce the number of tokens used by storing the extracted webpages as embeddings in a vectorstore and retrieving only what is necessary.\n",
    "* Make a more specific web search agent for your use-case rather than a general purpose web search agent.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
