---
title: Prompts
description: Learn how to use the @llm.prompt decorator for creating reusable prompt templates.
---

# Prompts

When working programmatically with LLMs, we often want reusable functions that encapsulate constructing messages and getting an LLM response. Using just what we've already learned about [Messages](/docs/mirascope/v2/messages) and [Models](/docs/mirascope/v2/models), we might write code like this:

<CodeExample file="examples/prompts/basic_without_prompt.py" />

Since this is such a common pattern in LLM-powered development, Mirascope provides the `@llm.prompt` decorator to streamline things:

<CodeExample file="examples/prompts/basic.py" />

You write a *prompt function* that returns the content to send to the LLM. The decorator converts it into an `llm.Prompt` that you can call with any model.

## Prompt Return Types

A prompt function can return content in several forms:

- **A string** — Converted to a user message
- **A list of content parts** — `llm.Text`, `llm.Image`, `llm.Audio`, combined into a user message
- **A list of messages** — Passed directly to the model for full control

<CodeExample file="examples/prompts/return_types.py" lines="3-" />

## Calling Prompts

A decorated prompt takes a model as its first argument, followed by your function's original arguments. If you don't need custom params, you can just pass a model id string:

<CodeExample file="examples/prompts/basic.py" />

When you need to configure parameters like `temperature`, pass an `llm.Model` instance:

<CodeExample file="examples/prompts/with_params.py" lines="3-" />

<Note>
The `Prompt` doesn't reference the model context manager, so it will always use the model that you pass in as the first argument.
</Note>

## Inspecting Messages

Use `.messages()` to see what messages a prompt generates without calling an LLM:

<CodeExample file="examples/prompts/messages_method.py" lines="3-" />

## Decorator Arguments

The `@llm.prompt` decorator accepts optional arguments for tools and structured output:

| Argument | Description |
| --- | --- |
| `tools` | List of tools the LLM can call. See [Tools](/docs/mirascope/v2/tools). |
| `format` | Response format for structured output. See [Structured Output](/docs/mirascope/v2/structured-output). |

We cover these arguments in their respective guides.
When no arguments are needed, the parentheses are optional: `@llm.prompt` and `@llm.prompt()` are equivalent.

## Next Steps

- [Calls](/docs/mirascope/v2/calls) — Bundle a model with your prompt function
- [Tools](/docs/mirascope/v2/tools) — Let LLMs call functions
- [Structured Output](/docs/mirascope/v2/structured-output) — Parse responses into structured data
