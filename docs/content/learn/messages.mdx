---
title: Messages
description: Learn how to use Messages to communicate with Large Language Models (LLMs) in Mirascope.
---

# Messages

Messages are the fundamental building blocks of Large Language Model (LLM) interactions. They are a shared abstraction underpinning LLM usage: you provide context to the LLM via messages, and it responds by generating messages of its own.

Here is a simple example:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/manual.py" lines="1-4" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/manual.ts" lines="1-4" />
</Tab>
</TabbedSection>

Each message is represented by a [Message](/docs/api/messages#message) class. The class has two required properties: `role`, which specifies what kind of entity is providing the message, and `content`, which specifies the content of the message. There's also an optional `name` field, which is helpful if there are multiple participants with the same role.

## Message Roles

Every message is associated with one of three roles: "system", "user", or "assistant". Roles are foundational to how messages are interpreted, so Mirascope provides role-specific shorthand functions for constructing messages:
[`llm.messages.system`](/docs/api/messages#system),
[`llm.messages.user`](/docs/api/messages#user),
and [`llm.messages.assistant`](/docs/api/messages#assistant).

### System Messages

System messages are instructions from the application developer, and take priority over user messages. The system message may include specific directives, a persona the LLM should adopt, or detailed examples of intended behavior. Context engineering often focuses on improving the system message.

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/shorthand.py" lines="3-5" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/shorthand.ts" lines="3-5" />
</Tab>
</TabbedSection>

### User Messages

User messages represent input from the person interacting with the LLM. These are questions, requests, or any input the LLM should respond to.

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/shorthand.py" lines="6-6" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/shorthand.ts" lines="6-6" />
</Tab>
</TabbedSection>

### Assistant Messages

Assistant messages are responses from the LLM. You typically don't create these by hand, but receive them as part of LLM responses.

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/shorthand.py" lines="7-9" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/shorthand.ts" lines="7-9" />
</Tab>
</TabbedSection>

<Note title="Manual Message Construction" collapsible={true} defaultOpen={false}>
Instead of using the shorthand functions, you can also construct the `Message` class directly:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/manual.py" lines="6-14" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/manual.ts" lines="6-17" />
</Tab>
</TabbedSection>

Both approaches create identical messages, so we recommend the shorthand functions for convenience.

</Note>

<Info title="Message Names" collapsible={true} defaultOpen={false}>

Messages have an optional `name` parameter, which may be provided to the
shorthand function. The name is useful when there are multiple participants in a conversation; for example multiple users, or (as shown below) multiple specialized assistants.

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/prompts.py" lines="15-25" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/prompts.ts" lines="13-24" />
</Tab>
</TabbedSection>

</Info>

## Message Content Types

Messages can contain much more than just text. Modern LLMs support multimodal content, allowing you to include multiple media types in a conversation, or even within a single message. Mirascope allows 8 different content types, although support may vary depending on your chosen provider.

| Content Type                                 | Anthropic | Google | OpenAI |
| -------------------------------------------- | :-------: | :----: | :----: |
| `Text`                                       |     ✓     |   ✓    |   ✓    |
| [`Image`](/docs/api/content#image)           |     ✓     |   ✓    |   ✓    |
| [`Audio`](/docs/api/content#audio)           |     —     |   ✓    |   ✓    |
| [`Video`](/docs/api/content#video)           |     —     |   ✓    |   —    |
| [`Document`](/docs/api/content#document)     |     ✓     |   ✓    |   ✓    |
| [`Thinking`](/docs/api/content#thinking)     |     ✓     |   ✓    |   ✓    |
| [`ToolCall`](/docs/api/content#toolcall)     |     ✓     |   ✓    |   ✓    |
| [`ToolOutput`](/docs/api/content#tooloutput) |     ✓     |   ✓    |   ✓    |

_TODO: Replace this with centralized compatibility table (#1031)_

### Text Content

The most common type of content is text, represented as plain strings:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/content.py" lines="3-3" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/content.ts" lines="3-3" />
</Tab>
</TabbedSection>

### Media Content

Depending on provider support, you can supply `Image`, `Audio`, or even `Video` content.

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/content.py" lines="5-10" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/content.ts" lines="5-14" />
</Tab>
</TabbedSection>
<br/>
<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/content.py" lines="12-18" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/content.ts" lines="16-23" />
</Tab>
</TabbedSection>
<br/>
<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/content.py" lines="20-26" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/content.ts" lines="25-35" />
</Tab>
</TabbedSection>

<Info>
**Media content** (`Image`, `Audio`, `Video`) can be provided as file paths, URLs, base64-encoded strings, or raw bytes.

Additional options:

- `id`: Optional unique identifier for tracking and referencing with a conversation.
- `transcript`: Optional text transcript for `Audio` and `Video` content.

</Info>

### Document Content

`Document` content allows you to embed structured document files, such as PDFs. Like media content, it can be specified via a file path, a URL, a base64-encoded string, or raw bytes.

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/content.py" lines="28-34" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/content.ts" lines="37-45" />
</Tab>
</TabbedSection>

### Tool Interactions

`ToolCall` and `ToolOutput` content is relevant when the LLM is calling external functions, also known as [Tools](./tools). The `ToolCall` content allows the LLM to specify that it wants to use a tool, and the `ToolOutput` content gives the LLM access to the output from that computation. By convention, the `ToolCall` is included in an assistant message, and the `ToolOutput` is provided as a user message.

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/content.py" lines="36-47" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/content.ts" lines="47-60" />
</Tab>
</TabbedSection>

### Thinking Content

Some models support thinking, in which case they create intermediate reasoning content that informs their final response. The `Thinking` content gives insight into the model's thinking process. Depending on the provider, this may be the raw thinking text, or a generated summary.

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/content.py" lines="49-59" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/content.ts" lines="62-72" />
</Tab>
</TabbedSection>

### Content Sequences

A message can contain a sequence of content objects, potentially of multiple types:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/content.py" lines="61-70" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/content.ts" lines="74-82" />
</Tab>
</TabbedSection>

## Building Prompts

LLMs operate on sequences of messages, which we call "prompts", and which provide the context the LLM uses to generate its response. Here, we show how some common prompting patterns can be constructed using the Mirascope message APIs.

**One-shot:**

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/prompts.py" lines="3-5" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/prompts.ts" lines="3-5" />
</Tab>
</TabbedSection>

**With system instructions:**

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/prompts.py" lines="7-12" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/prompts.ts" lines="7-12" />
</Tab>
</TabbedSection>

**Multi-turn Conversation:**

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/messages/prompts.py" lines="14-23" />
</Tab>
<Tab value="Typescript">
<CodeExample file="typescript/examples/messages/prompts.ts" lines="14-23" />
</Tab>
</TabbedSection>

## Next Steps

The message structure we've covered here works consistently across all LLM providers that Mirascope supports, giving you a unified interface regardless of whether you're using OpenAI, Anthropic, Google, or any other supported provider.

In the next section on [Prompt Templates](./prompt-templates), we will describe how Mirascope provides a reusable functional interface for constructing prompts. This sets you up for success when context engineering your LLM [Calls](./calls).
