---
title: Calls
description: Learn how to call LLMs and get responses, learning how to move from context engineering to LLM powered results.
---

# Calls

<Note>
  If you haven't already, we recommend reading the sections on [Messages](/docs/learn/messages) and [Prompt Templates](/docs/learn/prompt_templates).
</Note>

Mirascope calls are how you actually execute your prompts and get responses from LLMs. They provide a provider-agnostic interface that works consistently across OpenAI, Anthropic, Google, [and other providers](/docs/compatibility). As with [prompt templates](/docs/learn/prompt_templates), Mirascope approaches calls through a parameterized, functional interface.

Let's dive into an actual example! Here's a LLM call that gets a genre-aware book recommendation:

<TabbedSection>
<Tab value="Functional">
<CodeExample file="examples/calls/intro_functional.py" />

</Tab>
<Tab value="Spec">
<CodeExample file="examples/calls/intro_spec.py"/>


</Tab>
</TabbedSection>

<Note title="Functional vs Spec Style" collapsible={true} defaultOpen={false}>

By clicking the tabs, you can choose between the [functional](/docs/learn/prompt_templates#functional-style-templates) and [spec](/docs/learn/prompt_templates#spec-style-templates) styles for prompt templating. Both approaches produce the same result.

We're showing both styles in the intro; however for the rest of the doc, we'll use whichever style is most convenient for the example at hand.
</Note>

As you can see, `recommend_book` is a function which, when called, calls the LLM for us. We choose a model and define the prompt, and we get back a `llm.Response` object that has the actual response content from the LLM.


## Model and Provider Selection

Mirascope makes it easy to switch to a different LLM model, or even an entirely different provider; you can just update the model choice string. See our [compatibility page](/docs/compatibility) to see all the available model choices, as well as what features they support.

## Working with Response Objects

Regardless of which provider you use, you'll get back a consistent object: an [llm.Response](/docs/api/responses#response). Let's explore how to work with responses, starting with the most common use cases.

### Getting Response Content

| Property | Type | Description |
|----------|------|-------------|
| `text` | str | Returns all text content concatenated with newlines, or empty string if none. This is usually what you want for text-only responses. |
| `image` | [Image](/docs/api/content#image) \| None | Returns the first image in the response content, if any. |
| `audio` | [Audio](/docs/api/content#audio) \| None | Returns the first audio in the response content, if any. |
| `video` | [Video](/docs/api/content#video) \| None | Returns the first video in the response content, if any. |
| `thinking` | [Thinking](/docs/api/content#thinking) \| None | Returns the first thinking content in the response, if any. |
| `tool` | [Tool](/docs/api/tools#tool) \| [ContextTool](/docs/api/tools#contexttool) \| None | Returns the first tool called by the LLM, if any. See our [Tools guide](/docs/learn/tools) for details. |
| `content` | Sequence[[ResponseContent](/docs/api/responses#responsecontent)] | Access all content generated by the LLM as a sequence. `ToolCall` response content is automatically converted into `Tool` instances for easy calling. |
| `texts` | Sequence[str] | All text content as individual strings (empty sequence if none). |
| `images` | Sequence[[Image](/docs/api/content#image)] | All image content (empty sequence if none). |
| `audios` | Sequence[[Audio](/docs/api/content#audio)] | All audio content (empty sequence if none). |
| `videos` | Sequence[[Video](/docs/api/content#video)] | All video content (empty sequence if none). |
| `thinkings` | Sequence[[Thinking](/docs/api/content#thinking)] | All thinking content (empty sequence if none). |
| `tools` | Sequence[[Tool](/docs/api/tools#tool)] \| Sequence[[ContextTool](/docs/api/tools#contexttool)] | All tools called by the LLM (empty sequence if none). See our [Tools guide](/docs/learn/tools) for details. |

### Response Metadata

Information about the response quality and cost:

| Property | Type | Description |
|----------|------|-------------|
| `finish_reason` | [FinishReason](/docs/api/responses#finishreason) | The reason why the LLM finished generating a response (e.g., "stop", "length"). |
| `usage` | [Usage](/docs/api/responses#usage) \| None | The token usage statistics for the request to the LLM, if available. |
| `cost` | float \| None | The cost of the request to the LLM, if available. |

### Call Information

Details about how the call was made:

| Property | Type | Description |
|----------|------|-------------|
| `provider` | str | The provider of the LLM (e.g. "openai", "anthropic"). |
| `model` | str | The model identifier that generated the response (e.g. "openai:gpt-4", "anthropic:claude-3-5-sonnet-latest"). |
| `args` | dict[str, Any] | The arguments used to generate the response. |
| `spec` | str \| None | The string spec used to define the messages array, if a spec-style prompt was used. |
| `messages` | list[[Message](/docs/api/messages#message)] | The messages used to generate the response. This will not include the system message. |
| `raw` | Any | The raw response from the LLM. This contains the original, unprocessed response object from the provider. |

### Response Formatting

For structured outputs, you can format responses according to a schema:

| Method | Type | Description |
|--------|------|-------------|
| `format()` | T | Format the response according to a response format parser. This method is only available if the call was created with a response format type. See our [Response Models guide](/docs/learn/response_models) for details. |

## Error Handling

When making LLM calls, it's important to handle potential errors. Mirascope provides a unified exception hierarchy that wraps provider-specific errors, giving you a consistent interface while preserving access to the original provider exceptions.

### Exception Hierarchy

Mirascope defines the following exception types:

- [`llm.MirascopeError`](/docs/api/exceptions#mirascopeerror): Base exception for all Mirascope errors
- [`llm.APIError`](/docs/api/exceptions#apierror): Base class for API-related errors
- [`llm.ConnectionError`](/docs/api/exceptions#connectionerror): Network connectivity issues or timeouts
- [`llm.AuthenticationError`](/docs/api/exceptions#authenticationerror): Authentication failures (401, invalid API keys)
- [`llm.PermissionError`](/docs/api/exceptions#permissionerror): Permission/authorization failures (403)
- [`llm.BadRequestError`](/docs/api/exceptions#badrequesterror): Malformed requests (400, 422)
- [`llm.NotFoundError`](/docs/api/exceptions#notfounderror): Requested resource not found (404)
- [`llm.RateLimitError`](/docs/api/exceptions#ratelimiterror): Rate limits exceeded (429)
- [`llm.ServerError`](/docs/api/exceptions#servererror): Server-side errors (500+)
- [`llm.TimeoutError`](/docs/api/exceptions#timeouterror): Request timeouts or deadline exceeded

### Basic Error Handling

<CodeExample file="examples/calls/exceptions.py" />

## Call Params

Mirascope supports common parameters like `temperature` and `max_tokens` that work consistently across all providers:

<CodeExample file="examples/calls/params.py" />

These parameters are part of the base interface, so you can use them with any model without worrying about provider-specific differences.


## Custom Clients

If you want to access provider-specific client configuration, you may pass a custom client tied to that specific provider, as below:

<TabbedSection>
<Tab value="OpenAI">
<CodeExample file="examples/calls/custom_client_openai.py" />
</Tab>
<Tab value="Anthropic">
<CodeExample file="examples/calls/custom_client_anthropic.py" />
</Tab>
<Tab value="Google">
<CodeExample file="examples/calls/custom_client_google.py" />
</Tab>
</TabbedSection>

