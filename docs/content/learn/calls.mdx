---
title: Calls
description: Learn how to call LLMs and get responses, learning how to move from context engineering to LLM powered results.
---

# Calls

<Note>
  If you haven't already, we recommend reading the sections on [Messages](/docs/learn/messages) and [Prompt Templates](/docs/learn/prompt_templates).
</Note>

A Mirascope "call" is a powerful tool that enables you to transform [prompt templates](/docs/learn/prompt_templates) into LLM API calls with minimal boilerplate code while providing type safety and consistency across OpenAI, Anthropic, Google, [and other providers we support](/docs/compatibility).

Let's dive into an actual example! Here's an LLM call that generates a genre-aware book recommendation:

<TabbedSection>
<Tab value="Functional">
<CodeExample file="examples/calls/intro_functional.py" />

</Tab>
<Tab value="Spec">
<CodeExample file="examples/calls/intro_spec.py"/>


</Tab>
</TabbedSection>

<Note title="Functional vs Spec Style" collapsible={true} defaultOpen={false}>

By clicking the tabs, you can choose between the [functional](/docs/learn/prompt_templates#functional-style-templates) and [spec](/docs/learn/prompt_templates#spec-style-templates) styles for prompt templating. Both approaches produce the same result.

We're showing both styles in the intro; however, for the rest of the doc, we'll use whichever style is most convenient for the example at hand.
</Note>

As you can see, `recommend_book` is a prompt template that's been transformed into an LLM function. Calling it returns a `llm.Response` object generated by your chosen LLM.


## Model and Provider Selection

Mirascope makes switching to a different LLM, or even an entirely different provider, as easy as updating a single string. See our [compatibility page](/docs/compatibility) to see all the `provider:model` choices, as well as what features they support. Try switching the provider in the right sidebar to update examples with your preferred provider.

## Call Params

Mirascope supports common parameters like `temperature` and `max_tokens` that work consistently across all providers:

<CodeExample file="examples/calls/params.py" />

These parameters are part of the base interface, so you can use them with any model without worrying about provider-specific differences.

## Working with Response Objects

Regardless of which provider you use, you'll get back a consistent object: an [llm.Response](/docs/api/responses#response). Let's explore how to work with responses, starting with the most common use cases.

### Getting Response Content

| Property | Type | Description |
|----------|------|-------------|
| `content` | Sequence[[ResponseContent](/docs/api/responses#responsecontent)] | Access all content generated by the LLM as a sequence. `ToolCall` response content is automatically converted into `Tool` instances for easy calling. |
| `text` | str \| None  | Returns the first text content as a string, if any. |
| `texts` | Sequence[str] | All text content as individual strings (empty sequence if none). |
| `image` | [Image](/docs/api/content#image) \| None | Returns the first image in the response content, if any. |
| `images` | Sequence[[Image](/docs/api/content#image)] | All image content (empty sequence if none). |
| `audio` | [Audio](/docs/api/content#audio) \| None | Returns the first audio in the response content, if any. |
| `audios` | Sequence[[Audio](/docs/api/content#audio)] | All audio content (empty sequence if none). |
| `video` | [Video](/docs/api/content#video) \| None | Returns the first video in the response content, if any. |
| `videos` | Sequence[[Video](/docs/api/content#video)] | All video content (empty sequence if none). |
| `tool` | [Tool](/docs/api/tools#tool) \| [ContextTool](/docs/api/tools#contexttool) \| None | Returns the first tool called by the LLM, if any. See our [Tools guide](/docs/learn/tools) for details. |
| `tools` | Sequence[[Tool](/docs/api/tools#tool)] \| Sequence[[ContextTool](/docs/api/tools#contexttool)] | All tools called by the LLM (empty sequence if none). See our [Tools guide](/docs/learn/tools) for details. |
| `thinking` | [Thinking](/docs/api/content#thinking) \| None | Returns the first thinking content in the response, if any. |
| `thinkings` | Sequence[[Thinking](/docs/api/content#thinking)] | All thinking content (empty sequence if none). |

### Response Metadata

| Property | Type | Description |
|----------|------|-------------|
| `finish_reason` | [FinishReason](/docs/api/responses#finishreason) | The reason why the LLM finished generating a response (e.g., "stop", "length"). See the [reliability docs](/docs/learn/reliability) to learn how to continue responses that finished due to token length constraints. |
| `usage` | [Usage](/docs/api/responses#usage) \| None | The token usage statistics for the request to the LLM, if available. |
| `cost` | [Decimal](https://docs.python.org/3/library/decimal.html) \| None | The USD cost of the request to the LLM, if available. |

### Call Information

| Property | Type | Description |
|----------|------|-------------|
| `model` | str | The model identifier that generated the response (e.g. "openai:gpt-4", "anthropic:claude-3-5-sonnet-latest"). |
| `args` | dict[str, Any] | The arguments used to generate the response. |
| `spec` | str \| None | The string spec used to define the messages array, if a spec-style prompt was used. |
| `messages` | list[[Message](/docs/api/messages#message)] | The messages used to generate the response. |
| `raw` | Any | The raw response from the LLM. This contains the original, unprocessed response object from the provider. |

### Response Formatting

For structured outputs, you can format responses according to a schema:

| Method | Type | Description |
|--------|------|-------------|
| `format()` | T | Format the response according to a response format parser. This method is only available if the call was created with a response format type. See our [Response Models guide](/docs/learn/structured_outputs) for details. |

## Error Handling

When making LLM calls, it's important to handle potential errors. Mirascope provides a unified exception hierarchy that wraps provider-specific errors, giving you a consistent interface while preserving access to the original provider exceptions.

### Exception Hierarchy

Mirascope defines the following exception types:

- [`llm.MirascopeError`](/docs/api/exceptions#mirascopeerror): Base exception for all Mirascope errors
- [`llm.APIError`](/docs/api/exceptions#apierror): Base class for API-related errors
- [`llm.ConnectionError`](/docs/api/exceptions#connectionerror): Network connectivity issues or timeouts
- [`llm.AuthenticationError`](/docs/api/exceptions#authenticationerror): Authentication failures (401, invalid API keys)
- [`llm.PermissionError`](/docs/api/exceptions#permissionerror): Permission/authorization failures (403)
- [`llm.BadRequestError`](/docs/api/exceptions#badrequesterror): Malformed requests (400, 422)
- [`llm.NotFoundError`](/docs/api/exceptions#notfounderror): Requested resource not found (404)
- [`llm.RateLimitError`](/docs/api/exceptions#ratelimiterror): Rate limits exceeded (429)
- [`llm.ServerError`](/docs/api/exceptions#servererror): Server-side errors (500+)
- [`llm.TimeoutError`](/docs/api/exceptions#timeouterror): Request timeouts or deadline exceeded

### Basic Error Handling

<CodeExample file="examples/calls/exceptions.py" />


## Custom Clients

If you want to access provider-specific client configuration, you may pass a custom client tied to that specific provider:

<CodeExample file="examples/calls/custom_client.py" />
