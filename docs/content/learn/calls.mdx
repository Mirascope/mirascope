---
title: Calls
description: Learn how to call LLMs and get responses, learning how to move from context engineering to LLM powered results.
---

# Calls

<Note>
  If you haven't, we recommend starting by reading about [Messages](/docs/learn/messages).
</Note>

Working with Large Language Models (LLMs) involves "prompting" them with context that the LLM uses to generate its response. In Mirascope, you do this through **calls**. Calls are powerful tools that allow you to transform functional prompts into LLM API calls with minimal boilerplate, all while maintaining type safety and consistency across providers like OpenAI, Anthropic, Google, [and others](/docs/compatibility).

Let's explore a simple example, which showcases how easy it is to get started using Mirascope:

<CodeExample file="examples/calls/intro.py" />

This example has three important parts. The **prompt** (the undecorated `recommend_book` function) is a function that returns context we'll feed to the LLM. The `@llm.call` decorator transforms the prompt into a LLM-powered **call**. Finally, the **response** contains the content generated by the LLM, as well as call metadata (like token usage). Let's explore each piece in turn

## Prompts

In Mirascope, a prompt is any function which returns context that we can feed into a LLM. The simplest prompt is a function that just returns a string:

<CodeExample file="examples/calls/prompts.py" lines="4-6" />

Since LLMs operate on lists of [messages](/docs/learn/messages), under the hood Mirascope converts string prompts into lists of messages. It does so by creating a single user message with the specified content:

<CodeExample file="examples/calls/prompts.py" lines="8-8" />

While strings are the most common case, a prompt can return any valid `llm.Content`. Here is an example in which the prompt returns an audio file, perhaps for a speech-based user interface:

<CodeExample file="examples/calls/prompts.py" lines="11-12" />

<Info title="Refresher on content types" collapsible={true} defaultOpen={false}>

Mirascope supports [8 different content types](/docs/learn/messages#message-content-types), which includes things like tool calls that you likely won't specify in the prompt. For purposes of writing prompts, besides strings, you likely want images, audio, video, and documents. Content support varies by provider, so be sure to take a look at our [compatibility reference](/docs/compatibiliy).

| Content Type                                 |
| -------------------------------------------- |
| `str`                                        |
| [`Image`](/docs/api/content#image)           |
| [`Audio`](/docs/api/content#audio)           |
| [`Video`](/docs/api/content#video)           |
| [`Document`](/docs/api/content#document)     |

</Info>

A prompt can also return a sequence of content, which are all combined into one user message. This is convenient when dealing with multimodal prompts:

<CodeExample file="examples/calls/prompts.py" lines="15-22" />

Sometimes you will want to directly construct the messages; for example, when providing a system message. In that case, the prompt will return the list of messages directly:

<CodeExample file="examples/calls/prompts.py" lines="24-30" />

This can also be useful when you want to include past messages as part of a prompt.

<CodeExample file="examples/calls/prompts.py" lines="40-47" />

## Calls

Regardless of the style of prompt, you will want to transform it into a call. That is as simple as choosing a model string (which takes the form `provider:model`, e.g. `openai:gpt-4o-mini`), and providing it to the `@llm.call` decorator. The decorator transforms the decorated function from a prompt (which returns context) to a call (which returns llm responses). We revisit our simple example:

<CodeExample file="examples/calls/intro.py" lines="4-9" />

<Note title="On prompt return type signatures" collapsible={true} defaultOpen={false}>

In the example above, we've intentionally left out the return type signature on `recommend_book`. The correct typing is a bit counter-intuitive: `recommend_book` is still defined as a function that returns a string:

<CodeExample file="examples/calls/return_type.py" lines="4-9" />

This can be confusing, because when we actually call `recommend_book()`, we get back a `llm.Response`, not a string. The reason is that the call decorator transforms `recommend_book`, from a prompt which returns a string, into a call which returns a response. Going forward, we'll leave out the prompt return types to avoid potential confusion.

</Note>

### Model and Provider Selection

Mirascope makes switching to a different LLM, or even an entirely different provider, as easy as updating a single string. See our [compatibility page](/docs/compatibility) to see all the `provider:model` choices, as well as what features they support. Try switching the provider in the right sidebar to update examples with your preferred provider.

### Call Params

You can provide call params like `max_tokens` or `temperature` which affect the LLM's behavior on the prompt. 

<CodeExample file="examples/calls/params.py" lines="4-10"/>

Mirascope abstracts these into common params, so you can use them across all supported providers. Here's a reference on supported common call params:

| Param | Type | Description |
| --- | --- | --- |
| temperature | float | Controls randomness in response generation. Lower values (closer to 0) make responses more deterministic, higher values (closer to 1) make them more creative |
| max_tokens | int | Maximum number of tokens to generate in the response |

### Custom Call Params & Clients

It's also possible to use a provider-specific custom params:

<CodeExample file="examples/calls/params.py" lines="3-8"/>

Or a custom client:

<CodeExample file="examples/calls/custom_client.py" lines="3-11" />


## Responses

Once you invoke the call, you'll get back an an [llm.Response](/docs/api/responses#response). Under the hood, the LLM produces an assistant message, and the response wraps the content from that message. 
The response has a `__repr__` method that produces a string representation of all the response's content, so you can easily print it (or call `str(response)`). If you know that the message will contain a single piece of text, you can also conveniently access it via `response.text`.

<CodeExample file="examples/calls/simple_response.py" lines="4-13" />

When getting a multi-media response, it will consist of multiple content parts. In this case, you'll want to either iterate over the response content, or use the type-specific accessor properties:

<CodeExample file="examples/calls/multimedia_response.py" lines="4-42" />


### Response Content Properties

| Property | Type | Description |
|----------|------|-------------|
| `content` | Sequence[[ResponseContent](/docs/api/responses#responsecontent)] | Access all content generated by the LLM as a sequence. `ToolCall` response content is automatically converted into `Tool` instances for easy calling. |
| `text` | str \| None  | Returns the first text content as a string, if any. |
| `texts` | Sequence[str] | All text content as individual strings (empty sequence if none). |
| `image` | [Image](/docs/api/content#image) \| None | Returns the first image in the response content, if any. |
| `images` | Sequence[[Image](/docs/api/content#image)] | All image content (empty sequence if none). |
| `audio` | [Audio](/docs/api/content#audio) \| None | Returns the first audio in the response content, if any. |
| `audios` | Sequence[[Audio](/docs/api/content#audio)] | All audio content (empty sequence if none). |
| `video` | [Video](/docs/api/content#video) \| None | Returns the first video in the response content, if any. |
| `videos` | Sequence[[Video](/docs/api/content#video)] | All video content (empty sequence if none). |
| `tool` | [Tool](/docs/api/tools#tool) \| [ContextTool](/docs/api/tools#contexttool) \| None | Returns the first tool called by the LLM, if any. See our [Tools guide](/docs/learn/tools) for details. |
| `tools` | Sequence[[Tool](/docs/api/tools#tool)] \| Sequence[[ContextTool](/docs/api/tools#contexttool)] | All tools called by the LLM (empty sequence if none). See our [Tools guide](/docs/learn/tools) for details. |
| `thinking` | [Thinking](/docs/api/content#thinking) \| None | Returns the first thinking content in the response, if any. |
| `thinkings` | Sequence[[Thinking](/docs/api/content#thinking)] | All thinking content (empty sequence if none). |

### Response Metadata

| Property | Type | Description |
|----------|------|-------------|
| `finish_reason` | [FinishReason](/docs/api/responses#finishreason) | The reason why the LLM finished generating a response (e.g., "stop", "length"). See the [reliability docs](/docs/learn/reliability) to learn how to continue responses that finished due to token length constraints. |
| `usage` | [Usage](/docs/api/responses#usage) \| None | The token usage statistics for the request to the LLM, if available. |
| `cost` | [Decimal](https://docs.python.org/3/library/decimal.html) \| None | The USD cost of the request to the LLM, if available. |

### Call Information

| Property | Type | Description |
|----------|------|-------------|
| `model` | str | The model identifier that generated the response (e.g. "openai:gpt-4", "anthropic:claude-3-5-sonnet-latest"). |
| `args` | dict[str, Any] | The arguments used to generate the response. |
| `spec` | str \| None | The string spec used to define the messages array, if a spec-style prompt was used. |
| `messages` | list[[Message](/docs/api/messages#message)] | The messages used to generate the response. |
| `raw` | Any | The raw response from the LLM. This contains the original, unprocessed response object from the provider. |

### Response Formatting

For structured outputs, you can format responses according to a schema:

| Method | Type | Description |
|--------|------|-------------|
| `format()` | T | Format the response according to a response format parser. This method is only available if the call was created with a response format to generate [structured outputs](/docs/learn/structured_outputs). |

## Error Handling

When making LLM calls, it's important to handle potential errors. Mirascope provides a unified exception hierarchy that wraps provider-specific errors, giving you a consistent interface while preserving access to the original provider exceptions.

### Exception Hierarchy

Mirascope defines the following exception types:

- [`llm.MirascopeError`](/docs/api/exceptions#mirascopeerror): Base exception for all Mirascope errors
- [`llm.APIError`](/docs/api/exceptions#apierror): Base class for API-related errors
- [`llm.ConnectionError`](/docs/api/exceptions#connectionerror): Network connectivity issues or timeouts
- [`llm.AuthenticationError`](/docs/api/exceptions#authenticationerror): Authentication failures (401, invalid API keys)
- [`llm.PermissionError`](/docs/api/exceptions#permissionerror): Permission/authorization failures (403)
- [`llm.BadRequestError`](/docs/api/exceptions#badrequesterror): Malformed requests (400, 422)
- [`llm.NotFoundError`](/docs/api/exceptions#notfounderror): Requested resource not found (404)
- [`llm.RateLimitError`](/docs/api/exceptions#ratelimiterror): Rate limits exceeded (429)
- [`llm.ServerError`](/docs/api/exceptions#servererror): Server-side errors (500+)
- [`llm.TimeoutError`](/docs/api/exceptions#timeouterror): Request timeouts or deadline exceeded

### Basic Error Handling

<CodeExample file="examples/calls/exceptions.py" />


## Next Steps

By mastering calls in Mirascope, you'll be well-equipped to build robust, flexible, and reusable LLM applications.

Next, we recommend choosing one of:

- [Prompt Templates](/docs/learn/prompt_templates) for a convenient, template-based alternative approach to writing prompts
- [Streams](/docs/learn/streams) to see how to stream call responses for a more real-time interaction
- [Context](/docs/learn/context) for automatically maintaining context (e.g. conversation history) that is shared across calls
- [Structured Outputs](/docs/learn/structured_outputs) for type-safe approaches to get structured data from LLMs
- [Tools](/docs/learn/tools) to give LLMs access to custom tools that extend their capabilities
- [Agents](/docs/learn/agents) for more powerful LLM-empowered workflows that iteratively accomplish more complex tasks

Pick whichever path aligns best with what you're hoping to get from Mirascope.

