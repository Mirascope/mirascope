---
title: Streams
description: Learn how to stream LLM call responses for responsive systems with immediate feedback
---

# Streams

<Note>
  If you haven't already, we recommend first reading about [Calls](/docs/learn/calls).
</Note>

Streaming allows you to receive and process LLM responses as they're generated, rather than waiting for the complete response. This allows you to create more interactive experiences, with immediate feedback from the LLM. For example, if asking for a book recommendation, instead of getting a three second wait followed by a complete multiparagraph response, users would see the text "streaming" into the interface as it is generated.

## Basic Usage and Syntax

To use streaming, first create a prompt and transform it into a [call](/docs/learn/calls), just as you would with a non-streaming response. Then, instead of directly invoking the call, you call `.stream()` on it to get an [`llm.Stream`](python/mirascope/llm/responses/stream.py) instead.

`llm.Stream` is an iterable. When you iterate on it, you get content chunks as they are generated. In the example below, we generate text chunks, and print each one with `end=""` and `flush=True` to ensure the output is displayed immediately and without line breaks.


<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/intro.py" />
</Tab>
</TabbedSection>

<Info title="On printing raw chunks" collapsible={true} defaultOpen={false}>
In the example above, we directly print every chunk from the stream. These chunks are dataclasses with multiple properties that we'll dive into throughout the course of the guide. However, we've taken care to choose string representations so that simply printing each chunk gives an intuitive result.
</Info>


### Multi-modal stream output

Streams can output every modality of content, via distinct chunk types, like `llm.TextChunk`, `llm.ImageChunk`, and so forth. Each chunk has a string `delta`, which is the incremental content in that chunk. The semantics of the delta vary based on chunk type. For text, the delta is the incremental text of that particular chunk. For images, each delta is a base64-encoded string with a progressive render of the image. For audio, the delta is a base64-encoded string of audio data, but there is also a `transcript_delta` with incremental transcripts.

Mirascope also provides `llm.ChunkStart` and `llm.ChunkEnd` chunks, which demarcate the start and end of a particular piece of content. This is helpful to distinguish between e.g. multiple images that are being progressively rendered and streamed in sequence. When iterating over the chunks in a stream, you can use the `type` discriminator field to process each chunk type distinctly. 

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/content_type.py" />
</Tab>
</TabbedSection>

Note that for multimodal streams, the `ChunkStart` and `ChunkEnd` will print as helpful markers showing where content begins and ends, as in the following example:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/multimodal_print.py" />
</Tab>
</TabbedSection>


### Grouped Streaming API

For processing multimodal streams, Mirascope offers a more advanced `groups()` API that organizes related chunks into distinct content pieces. Each group is itself an iterator, and iterates over a sequence of chunks that will combine into one piece of content. 

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/groups.py" />
</Tab>
</TabbedSection>

Each group provides:
- `type`: The content type for type discrimination (e.g., "text", "image", "audio")
- `partial`: Live-updating accumulated content as chunks are consumed
- `final`: The complete content object once the group is finished (None until then)

Groups enforce sequential consumption - you must fully exhaust one group before moving to the next. This ensures proper ordering of multimodal content generation.

### Converting Streams to Responses

You can convert a stream to a standard response by calling [`Stream.to_response`](/docs/api/streams#to-response). Note that `to_response` can only be successfully called after you have iterated through all the stream's content and exhausted it. Calling it on an un-exhausted stream will raise an exception.

<Warning title="Response Reconstruction Limitations">
Mirascope tries to match the reconstructed `llm.Response` with the corresponding non-streaming response. However, they might not be exactly the same.
</Warning>

<CodeExample file="examples/streams/to_response.py" />

### Monitoring Stream Usage & Cost

Once a stream is complete, you can check its total usage and cost, as in the example below. These properties will not be available until the stream is exhausted.

<CodeExample file="examples/streams/usage_and_cost.py" />

## Error Handling

Error handling in streams is similar to [standard calls](/docs/learn/calls#error-handling). However, it's important to note that errors may occur during iteration rather than at the initial function call:

<CodeExample file="examples/streams/error_handling.py" />

Note how we wrap the iteration loop in a try/except block to catch any errors that might occur during streaming. The initial response when calling `.stream()` will return an iterable, but any errors that occur during streaming will not happen until you actually iterate through it.

<Info title="Error types reference" collapsible={true} defaultOpen={false}>

Here's a quick refresher on Mirascope exception types:

- [`llm.MirascopeError`](/docs/api/exceptions#mirascopeerror): Base exception for all Mirascope errors
- [`llm.APIError`](/docs/api/exceptions#apierror): Base class for API-related errors
- [`llm.ConnectionError`](/docs/api/exceptions#connectionerror): Network connectivity issues or timeouts
- [`llm.AuthenticationError`](/docs/api/exceptions#authenticationerror): Authentication failures (401, invalid API keys)
- [`llm.PermissionError`](/docs/api/exceptions#permissionerror): Permission/authorization failures (403)
- [`llm.BadRequestError`](/docs/api/exceptions#badrequesterror): Malformed requests (400, 422)
- [`llm.NotFoundError`](/docs/api/exceptions#notfounderror): Requested resource not found (404)
- [`llm.RateLimitError`](/docs/api/exceptions#ratelimiterror): Rate limits exceeded (429)
- [`llm.ServerError`](/docs/api/exceptions#servererror): Server-side errors (500+)
- [`llm.TimeoutError`](/docs/api/exceptions#timeouterror): Request timeouts or deadline exceeded

</Info>

## Stream Properties and Methods

After creating a stream, you can access properties and methods on the `llm.Stream` object itself:

### Stream Properties

| Property | Type | Description |
|----------|------|-------------|
| `finish_reason` | [FinishReason](/docs/api/responses#finishreason) \| None | The reason why the LLM finished generating a response, available after the stream completes. |
| `usage` | [Usage](/docs/api/responses#usage) \| None | The token usage statistics for the entire stream. This will not be available until the stream is exhausted. |
| `cost` | [Decimal](https://docs.python.org/3/library/decimal.html) \| None | The cost reflecting all chunks processed so far. This will only be available after the stream is exhausted. |

### Stream Methods

| Method | Type | Description |
|--------|------|-------------|
| `to_response()` | `llm.Response` | Convert the stream into a reconstructed `llm.Response`. Can only be called after the stream has been exhausted (i.e. all content has been iterated). The reconstructed response may not exactly match the response that would have been generated by using a non-streaming call. |

### ContentChunk Types

`llm.ContentChunk` is an alias, which corresponds to five distinct chunk types. Each chunk type has a `type` field for type descrimination. Here are the types and their properties:

| Class | Type | Properties | Notes | 
|-------|------|------------| ----- | 
| [TextChunk](/docs/api/content#textchunk) | "text_chunk" | `id`, `delta` | The standard content for streams. `delta` is a regular string. |
| [ImageChunk](/docs/api/content#imagechunk) | "image_chunk" | `mime_type`, `id`, `delta` | `delta` is a base64-encoded string with progressive image data. Use `id` to distinguish between multiple images. |
| [AudioChunk](/docs/api/content#audiochunk) | "audio_chunk" | `mime_type`, `id`, `delta`, `delta_transcript` | `delta` is base64-encoded audio data, `delta_transcript` is incremental transcript text. |
| [ToolCallChunk](/docs/api/content#toolcallchunk) | "tool_call_chunk" | `id`, `name`, `delta` | `delta` contains incremental JSON arguments. |
| [ThinkingChunk](/docs/api/content#thinkingchunk) | "thinking_chunk" | `id`, `delta` | `delta` is a regular string with incremental thinking content. |

The `mime_type` property specifies supported formats: audio types include `audio/wav`, `audio/mp3`, etc., while image types include `image/png`, `image/jpeg`, etc.


## Async Streams

Async streams operate similarly to [async calls](/docs/learn/calls#async-calls). The simplest approach is to decorate an async function with `call` and then call the `stream()` method. This returns an `llm.AsyncStream` that you can iterate over with `async for`:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/async_basic.py" />
</Tab>
</TabbedSection>

If you have a synchronous call definition, you can use the `stream_async()` method to get an async stream:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/async_stream_method.py" />
</Tab>
</TabbedSection>


### Async Properties 

`llm.AsyncStream` has the same properties and methods as `llm.Stream`, including:

- `finish_reason`, `usage`, and `cost` properties that are populated as the stream is consumed
- The same error handling patterns apply to async streams

The main difference is that you use `async for` instead of `for` to iterate through the chunks. Similarly, you will use `async for` to iterate through groups, and through the chunks within each group. Once the stream is exhausted, `to_response` is still synchronous.

### Async Groups

The groups API works seamlessly with async streams. You use `async for` at both the group level and chunk level:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/async_groups.py" />
</Tab>
</TabbedSection>

The same type-safe patterns apply - check the group type first, then iterate through chunks within that typed boundary. 

## Next Steps

By mastering streaming in Mirascope, you can create more responsive and efficient LLM-powered applications that provide immediate feedback to users.

Next, we recommend exploring:

- [Audio Streaming](/docs/learn/audio-streaming) - Learn how to handle audio outputs in streaming responses for speech-based applications
- [Context](/docs/learn/context) - Automatically maintain conversation history and dependencies across multiple calls
- [Tools](/docs/learn/tools) - Give LLMs access to custom tools and see how tool calls work in streaming responses
- [Agents](/docs/learn/agents) - Build autonomous agents that can stream their reasoning and actions

Pick whichever path you prefer.
