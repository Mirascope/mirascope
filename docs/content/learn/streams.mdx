---
title: Streams
description: Learn how to stream LLM call responses for responsive systems with immediate feedback
---

# Streams

<Note>
  If you haven't already, we recommend first reading about [Calls](/docs/learn/calls).
</Note>

Streaming is a powerful feature when using LLMs that allows you to process chunks of an LLM response as they are generated. This approach offers several benefits, including:

1. **Immediate feedback**: Users can see responses as they're being generated, creating a more 
interactive experience.
2. **Incremental processing**: Applications can process and act on partial results as they arrive.
3. **Early termination**: If the desired information is found early in the response, processing can be
  stopped without waiting for the full generation.

## Basic Usage and Syntax

To use streaming, first create a prompt and transform it into a [call](/docs/learn/calls), just as you would with a non-streaming response. Then, instead of directly invoking the call, you call `.stream()` on it to get an [`llm.Stream`](python/mirascope/llm/responses/stream.py) instead.

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/intro.py" />
</Tab>
</TabbedSection>

`llm.Stream` is an iterable. When you iterate on it, you get [`llm.StreamedContent`](/docs/api/streams#streamedcontent)s as they are generated. In the example, we print each one with `end=""` and `flush=True` to ensure the output is displayed immediately and without line breaks.


### Converting Streams to Responses

You can convert a stream to a standard response by calling [`Stream.to_response`](/docs/api/streams#to-response). Note that `to_response` can only be successfully called after you have iterated through all the stream's content and exhausted it. Calling it on an un-exhausted stream will raise an exception.

<Warning title="Response Reconstruction Limitations">
Mirascope tries its best to match the reconstructed `llm.Response` with the response that would have been generated if streaming were not involved. However, we can't guarantee that they will be exactly the same.
</Warning>

<CodeExample file="examples/streams/to_response.py" />

### Monitoring Stream Usage & Cost

Each chunk provides `usage` and `cost` properties that give token usage and cost for that chunk (if available). The stream also keeps track of its running total `usage` and `cost`, as shown below:

<CodeExample file="examples/streams/usage_and_cost.py" />

## Error Handling

Error handling in streams is similar to [standard calls](/docs/learn/calls#error-handling). However, it's important to note that errors may occur during iteration rather than at the initial function call:

<CodeExample file="examples/streams/error_handling.py" />

Note how we wrap the iteration loop in a try/except block to catch any errors that might occur during streaming. The initial response when calling `.stream()` will return an iterable, but any errors that occur during streaming will not happen until you actually iterate through it.

## Async Streams

Async streams operate similarly to [async calls](/docs/learn/calls#async-calls). The simplest approach is to decorate an async function with `call` and then call the `stream()` method. This returns an `llm.AsyncStream` that you can iterate over with `async for`:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/async_basic.py" />
</Tab>
</TabbedSection>

If you have a synchronous call definition, you can use the `stream_async()` method to get an async stream:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/async_stream_method.py" />
</Tab>
</TabbedSection>


## Stream Properties and Methods

After creating a stream, you can access properties and methods on the `llm.Stream` object itself:

### Stream Properties

| Property | Type | Description |
|----------|------|-------------|
| `finish_reason` | [FinishReason](/docs/api/responses#finishreason) \| None | The reason why the LLM finished generating a response, available after the stream completes. |
| `usage` | [Usage](/docs/api/responses#usage) \| None | The token usage statistics reflecting all chunks processed so far. Updates as chunks are consumed. |
| `cost` | [Decimal](https://docs.python.org/3/library/decimal.html) \| None | The cost reflecting all chunks processed so far. Updates as chunks are consumed. |

### Stream Methods

| Method | Type | Description |
|--------|------|-------------|
| `to_response()` | `llm.Response` | Convert the stream into a reconstructed `llm.Response`. Can only be called after the stream has been exhausted (i.e. all content has been iterated). |

### Async Properties 

`llm.AsyncStream` has the same properties and methods as `llm.Stream`, including:

- `finish_reason`, `usage`, and `cost` properties that are populated as the stream is consumed
- The same error handling patterns apply to async streams
- All the same chunk properties and accessor methods are available

The main difference is that you use `async for` instead of `for` to iterate through the chunks, and that `to_response` is async.

## Next Steps

By mastering streaming in Mirascope, you can create more responsive and efficient LLM-powered applications that provide immediate feedback to users.

Next, we recommend exploring:

- [Audio Streaming](/docs/learn/audio-streaming) - Learn how to handle audio outputs in streaming responses for speech-based applications
- [Context](/docs/learn/context) - Automatically maintain conversation history and dependencies across multiple calls
- [Tools](/docs/learn/tools) - Give LLMs access to custom tools and see how tool calls work in streaming responses
- [Agents](/docs/learn/agents) - Build autonomous agents that can stream their reasoning and actions

Pick whichever path you prefer.
