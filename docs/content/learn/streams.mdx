---
title: Streams
description: Learn how to stream LLM call responses for responsive systems with immediate feedback
---

# Streams

<Note>
  If you haven't already, we recommend first reading about [Calls](/docs/learn/calls).
</Note>

Streaming allows you to receive and process LLM responses as they're generated, rather than waiting for the complete response. This allows you to create more interactive experiences, where users get immediate feedback from the LLM. For example, if asking for a book recommendation, instead of getting a three second wait followed by a complete multiparagraph response, the users would see the text "streaming" onto the screen as it is generated.

## Basic Usage and Syntax

To use streaming, first create a prompt and transform it into a [call](/docs/learn/calls), just as you would with a non-streaming response. Then, instead of directly invoking the call, you call `.stream()` on it to get an [`llm.Stream`](python/mirascope/llm/responses/stream.py) instead.

`llm.Stream` is an iterable. When you iterate on it, you get [`llm.StreamContent`](/docs/api/content#streamcontent)s as they are generated. In the example below, we iterate over text chunks. For each one, we print its delta with `end=""` and `flush=True` to ensure the output is displayed immediately and without line breaks.


<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/intro.py" />
</Tab>
</TabbedSection>


### Multi-modal stream output

Streams can output every modality of content. However, instead of producing `llm.Content`, streams emit content in smaller "chunks". In some cases (e.g. text), the final content is generated by combining all the chunks. In this case, each chunk has a `delta` property (its own data) and a `partial` (the accumulated content). However, for images, we don't have deltas, only partials; in this case, every chunk is a "progressive render" that is closer to the final content.

Here is an example of the text chunks that may be generated by a basic stream:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/text_chunks.py" />
</Tab>
</TabbedSection>

When dealing with multimodal output from a stream, you should use the `type` property on streamed content to discriminate which kind of content you are processing:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/content_type.py" />
</Tab>
</TabbedSection>


### Converting Streams to Responses

You can convert a stream to a standard response by calling [`Stream.to_response`](/docs/api/streams#to-response). Note that `to_response` can only be successfully called after you have iterated through all the stream's content and exhausted it. Calling it on an un-exhausted stream will raise an exception.

<Warning title="Response Reconstruction Limitations">
Mirascope tries to match the reconstructed `llm.Response` with the corresponding non-streaming response. However, they might not be exactly the same.
</Warning>

<CodeExample file="examples/streams/to_response.py" />

### Monitoring Stream Usage & Cost

Once a stream is complete, you can check its total usage and cost, as in the example below. These properties will not be available until the stream is exhausted.

<CodeExample file="examples/streams/usage_and_cost.py" />

## Error Handling

Error handling in streams is similar to [standard calls](/docs/learn/calls#error-handling). However, it's important to note that errors may occur during iteration rather than at the initial function call:

<CodeExample file="examples/streams/error_handling.py" />

Note how we wrap the iteration loop in a try/except block to catch any errors that might occur during streaming. The initial response when calling `.stream()` will return an iterable, but any errors that occur during streaming will not happen until you actually iterate through it.

<Info title="Error types reference" collapsible={true} defaultOpen={false}>

Here's a quick refresher on Mirascope exception types:

- [`llm.MirascopeError`](/docs/api/exceptions#mirascopeerror): Base exception for all Mirascope errors
- [`llm.APIError`](/docs/api/exceptions#apierror): Base class for API-related errors
- [`llm.ConnectionError`](/docs/api/exceptions#connectionerror): Network connectivity issues or timeouts
- [`llm.AuthenticationError`](/docs/api/exceptions#authenticationerror): Authentication failures (401, invalid API keys)
- [`llm.PermissionError`](/docs/api/exceptions#permissionerror): Permission/authorization failures (403)
- [`llm.BadRequestError`](/docs/api/exceptions#badrequesterror): Malformed requests (400, 422)
- [`llm.NotFoundError`](/docs/api/exceptions#notfounderror): Requested resource not found (404)
- [`llm.RateLimitError`](/docs/api/exceptions#ratelimiterror): Rate limits exceeded (429)
- [`llm.ServerError`](/docs/api/exceptions#servererror): Server-side errors (500+)
- [`llm.TimeoutError`](/docs/api/exceptions#timeouterror): Request timeouts or deadline exceeded

</Info>

## Stream Properties and Methods

After creating a stream, you can access properties and methods on the `llm.Stream` object itself:

### Stream Properties

| Property | Type | Description |
|----------|------|-------------|
| `finish_reason` | [FinishReason](/docs/api/responses#finishreason) \| None | The reason why the LLM finished generating a response, available after the stream completes. |
| `usage` | [Usage](/docs/api/responses#usage) \| None | The token usage statistics for the entire stream. This will not be available until the stream is exhausted. |
| `cost` | [Decimal](https://docs.python.org/3/library/decimal.html) \| None | The cost reflecting all chunks processed so far. This will only be available after the stream is exhausted. |

### Stream Methods

| Method | Type | Description |
|--------|------|-------------|
| `to_response()` | `llm.Response` | Convert the stream into a reconstructed `llm.Response`. Can only be called after the stream has been exhausted (i.e. all content has been iterated). The reconstructed response may not exactly match the response that would have been generated by using a non-streaming call. |

### StreamContent Types

All stream content types share a common `type` field for identification. Here are the available types and their properties:

| Class | Type | Properties | Notes | 
|-------|------|------------| ----- | 
| [TextChunk](/docs/api/content#textchunk) | "text_chunk" | `id`, `delta`, `partial` | The standard content for streams. `delta` and `partial` are both regular strings. |
| [AudioChunk](/docs/api/content#audiochunk) | "audio_chunk" | `mime_type`, `id`, `delta`, `partial` | `delta` and `partial` are both base64 encoded strings. Use the `id` property to distinguish between multiple pieces of audio content. |
| [ImagePartial](/docs/api/content#imagepartial) | "image_partial" | `mime_type`, `id`, `partial` | There is no `delta` property; when generating a single image, you may get a sequence of `ImagePartial`s with progressive rendering. use `id` to distinguish between images. |
| [ToolCallChunk](/docs/api/content#toolcallchunk) | "tool_call_chunk" | `id`, `name`, `args_delta`, `args_partial` | `args_delta` is a string, but `args_partial` is `dict[str, Jsonable]`. |
| [ThinkingChunk](/docs/api/content#thinkingchunk) | "thinking_chunk" | `id`, `delta`, `partial` | The `delta` and `partial` are both strings. |

Note that `ImagePartial` is the only type without a `delta` property, since images render progressively rather than incrementally. The `mime_type` property specifies supported formats: audio types include `audio/wav`, `audio/mp3`, etc., while image types include `image/png`, `image/jpeg`, etc.


## Async Streams

Async streams operate similarly to [async calls](/docs/learn/calls#async-calls). The simplest approach is to decorate an async function with `call` and then call the `stream()` method. This returns an `llm.AsyncStream` that you can iterate over with `async for`:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/async_basic.py" />
</Tab>
</TabbedSection>

If you have a synchronous call definition, you can use the `stream_async()` method to get an async stream:

<TabbedSection>
<Tab value="Python">
<CodeExample file="examples/streams/async_stream_method.py" />
</Tab>
</TabbedSection>


### Async Properties 

`llm.AsyncStream` has the same properties and methods as `llm.Stream`, including:

- `finish_reason`, `usage`, and `cost` properties that are populated as the stream is consumed
- The same error handling patterns apply to async streams
- All the same chunk properties and accessor methods are available

The main difference is that you use `async for` instead of `for` to iterate through the chunks. Once the stream is exhausted, `to_response` is still synchronous. 

## Next Steps

By mastering streaming in Mirascope, you can create more responsive and efficient LLM-powered applications that provide immediate feedback to users.

Next, we recommend exploring:

- [Audio Streaming](/docs/learn/audio-streaming) - Learn how to handle audio outputs in streaming responses for speech-based applications
- [Context](/docs/learn/context) - Automatically maintain conversation history and dependencies across multiple calls
- [Tools](/docs/learn/tools) - Give LLMs access to custom tools and see how tool calls work in streaming responses
- [Agents](/docs/learn/agents) - Build autonomous agents that can stream their reasoning and actions

Pick whichever path you prefer.
