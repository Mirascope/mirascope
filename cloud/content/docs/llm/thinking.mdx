---
title: Thinking
description: Learn how to use extended thinking capabilities with models that support reasoning.
---

# Thinking

Many recent models support "extended thinking", in which the model uses additional tokens to reason about the prompt, before generating its response. This can improve performance, especially for complex tasks like math, coding, or multi-step reasoning.

Mirascope supports enabling and configuring thinking, as well as viewing the model's generated thoughts. 

## Enabling Thinking

Configure thinking via the `thinking` parameter, which accepts a `ThinkingConfig`:

<CodeExample file="python/examples/thinking/basic.py" />


### Thinking Levels

The `level` field controls how much reasoning the model performs. If set to `"default"`, then we'll respect that provider's default behavior. Otherwise, you can configure it by choosing `"none"`, `"minimal"`, `"low"`, `"medium"`, `"high"`, or `"max"`.

The interpretation of the thinking level depends on the model and provider. Some model's don't support thinking at all, while others don't allow you to disable it, or only support two levels, like `"low"` and `"high"`. Mirascope will choose the setting that best matches your chosen thinking level.

## Accessing Thoughts

To include the model's reasoning in the response, set `include_thoughts=True`. The thoughts will then appear in `response.thoughts`.

<CodeExample file="python/examples/thinking/access_thoughts.py" />

Each `llm.Thought` contains a `thought` string with the model's reasoning. Depending on the provider, this may be a summary of the reasoning process (often generated by a different model), or the raw reasoning tokens themselves. Some providers may charge additional fees for thought content.

<Note>
Without `include_thoughts=True`, thoughts will not be included in the response, even if the model is using thinking internally.
</Note>

## Reasoning Token Usage 

While a model's reasoning may not be part of the final output, you will still be billed for the token usage. Access `response.reasoning_tokens` to see how many tokens the reasoning process generated. Note that the `output_tokens` field includes *all* output tokens, including reasoning tokens.  

<Note>
Not every model breaks out reasoning tokens separately from output tokens. (For example: Anthropic doesn't.) In such cases, the reasoning_tokens will always be 0.
</Note>

## Cross-Provider Thought Persistence

By default, when you pass a conversation with thoughts to a different model, those thoughts may be ignored—providers handle prior reasoning differently, and often the actual thoughts are persisted on the server-side.

Set `encode_thoughts_as_text=True` to re-encode thoughts as regular text content, ensuring the new model can read the prior reasoning:

<CodeExample file="python/examples/thinking/encode_as_text.py" />

This is primarily useful when:
- Switching providers mid-conversation
- Working around provider-specific limitations on thought persistence
- Ensuring reasoning is preserved across tool call boundaries

<Note>
When `encode_thoughts_as_text=True`, Mirascope re-encodes messages rather than reusing raw provider responses. This may disable provider-specific optimizations like cached reasoning tokens.
</Note>

## Next Steps

- [Tools](/docs/llm/tools) — Combine thinking with tool calling
- [Streaming](/docs/llm/streaming) — Learn more about streaming responses
- [Agents](/docs/llm/agents) — Build agents that reason through complex tasks
