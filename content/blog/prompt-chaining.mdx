---
title: "Prompt Chaining in AI Development"
description: "Prompt chaining is a way to simplify large, complex prompts by breaking them down into smaller prompts, each making their own separate LLM call."
date: "2024-06-05"
readTime: "28 min read"
author: "William Bakst"
updatedAt: "2026-01-27"
---

Prompt chaining is a way to sequence LLM calls (and their prompts) by using the output of the last call as input to the next, to guide an LLM to produce more useful answers than if it had been prompted only once.

By treating the entire chain of calls and prompts as part of a larger request to arrive at an ultimate response, you’re able to refine and steer the intermediate calls and responses at each step to achieve a better result.

Prompt chaining allows you to manage what may start out as a large, unwieldy prompt, whose implicitly defined subtasks and details can throw off language models and result in unsatisfying responses. **This is because LLMs lose focus when asked to process different ideas thrown together.** They can misread relationships between different instructions and incompletely execute them.

Such misreadings can also cascade as downstream errors given that LLMs generate text token by token.

We even see this in Chain-of-Thought prompting (CoT) sometimes. Though this style of prompting does a reasonably good job at decomposing tasks into smaller ones, it nonetheless generates the entire output on the fly in a single call-with no intermediate steps.

**This gives you no granular control over the flow:** What if you want to prompt engineer around the attributes of a single variable in the middle of a sequence?

That’s why prompt chaining is suitable for some situations. In this article, we give you a full picture of what prompt chains do. We also show ways of implementing chains using [Mirascope](https://github.com/mirascope/mirascope), our own Python toolkit for building with language models, along with do’s and don’ts for chaining LLM calls.

## A Brief Use Case

Imagine asking an LLM to generate a detailed travel itinerary for a week-long trip across Europe. You supply it with details like starting date, countries of interest, and duration of visit. You want it to give details on flight suggestions, hotel recommendations, and local attractions.

Feeding all this into the language model in one go might not produce the output you’re looking for. It might struggle to prioritize and accurately fulfill each request (whether explicitly or implicitly stated), resulting in an incomplete or unsatisfying itinerary. You might end up refining the prompt and resubmitting it until you’re satisfied with the answer.

To implement prompt chaining, you could break down the main prompt into smaller, separate prompts, each with their own call:

1. “Suggest popular travel destinations in Europe for a week-long trip:” the LLM responds with a list of destinations.
2. “What are good flight options from [starting city] to [chosen destination]:” it responds with a list of flights at different dates and times.
3. “Recommend highly rated hotels in [chosen destination] arriving on [date] for a week-long stay:” it responds with a list of hotel options, and so on.

Smaller prompts and calls are more manageable and allow the model to focus on the details of each individual task, resulting in responses that are likely to be more thorough and accurate than if you had just sent all the details at once.

## Types of Prompt Chains

Although most of the literature concerning chaining focuses on sequential prompting, it’s possible to work with other configurations of prompt chains.

In this section, we briefly describe each of the other prompt engineering techniques before returning to mainly discussing sequential prompts in the rest of this article.

- Sequential prompts
- Parallel prompts
- Conditional prompts
- Recursive prompts

**Note:** Although different prompt types are individually described below, a powerful technique is to combine these as needed in your code.

### Sequential Prompts

These are straightforward in that the chain uses the output of the call from the previous step as input to the next to build a more complex and refined output.

In the sequential prompt below, `recommend_recipe()` calls `select_chef()`, whose output is used for its own response.

```python
from mirascope import llm

@llm.call("openai/gpt-4o-mini")
def select_chef(food_type: str):
    return f"Name a chef who is really good at cooking {food_type} food"

@llm.call("openai/gpt-4o-mini")
def recommend_recipe(food_type: str, ingredient: str):
    chef = select_chef(food_type=food_type)
    return f"""
    Imagine that you are chef {chef.text()}.
    Recommend a {food_type} recipe using {ingredient}.
    """

print(recommend_recipe(food_type="Japanese", ingredient="apples").text())
```

### Parallel Prompts

With parallel prompts, multiple prompts are executed simultaneously, often using the same initial input. The outputs of these parallel prompts can be combined or processed further in subsequent steps.

In the code example below, the `chef` and `ingredients` properties are both computed in parallel and independently, as they don’t depend on each other.

```python
# Parallel Execution

import asyncio

from mirascope import llm

@llm.call("openai/gpt-4o-mini")
async def select_chef(ingredient: str):
    return f"""
    Please identify a chef who is well known for cooking with {ingredient}.
    Respond only with the chef's name.
    """

@llm.call("openai/gpt-4o-mini")
async def identify_ingredients(ingredient: str):
    return f"""
    Given a base ingredient {ingredient}, return a list of complementary ingredients.
    Make sure to exclude the original ingredient from the list, and respond
    only with the list.
    """

@llm.call("openai/gpt-4o-mini")
async def recommend_recipe(ingredient: str):
    chef, ingredients = await asyncio.gather(
        select_chef(ingredient), identify_ingredients(ingredient)
    )
    return [
        llm.messages.system(f"Your task is to recommend a recipe. Pretend that you are chef {chef.text()}."),
        llm.messages.user(f"Recommend recipes that use the following ingredients: {ingredients.text()}"),
    ]

async def recommend_recipe_parallel_chaining(ingredient: str):
    return await recommend_recipe(ingredient=ingredient)

print(asyncio.run(recommend_recipe_parallel_chaining(ingredient="apples")).text())
```

### Conditional Prompts

Conditional prompts dynamically adjust their prompt queries based on specific conditions or criteria. This involves using conditional logic to modify parts of the prompt template depending on the input data or the results of intermediate steps.

Below, the `conditional_review_prompt` computed field checks the sentiment of the review and returns a different string based on whether the sentiment is positive or negative, which then dynamically adjusts the prompt template used in `response_to_review` method.

```python
# Conditional Prompt

from enum import Enum

from mirascope import llm

class Sentiment(str, Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"

@llm.call("openai/gpt-4o-mini", format=Sentiment)
def classify_sentiment(review: str):
    return f"Is the following review positive or negative? {review}"

@llm.call("openai/gpt-4o-mini")
def respond_to_review(review: str):
    sentiment = classify_sentiment(review).parse()
    conditional_review_prompt = (
        "thank you response for the review"
        if sentiment == Sentiment.POSITIVE
        else "apologetic response addressing the review"
    )
    return [
        llm.messages.system(f"Your task is to respond to a review. The review has been identified as {sentiment.value}."),
        llm.messages.user(f"Write a {conditional_review_prompt} for the following review: {review}"),
    ]

positive_response = respond_to_review(
    review="This tool is awesome because it's so flexible!"
)
print(positive_response.text())

# > Thank you for your wonderful review! We're thrilled to hear ...

negative_response = respond_to_review(
    review="This product is terrible and too expensive!"
)
print(negative_response.text())

# > Thank you for sharing your feedback with us. We're truly sorry to hear ...
```

### Iterative Chaining (Recursive Prompts)

Iterative chaining involves a kind of automation in the form of a repeating loop where a prompt calls itself or another prompt in a loop-like structure. This is useful for tasks that require iteration, such as refining outputs or handling multi-step processes that need repeated evaluation.

In the code below, the `rewrite_iteratively` function takes the initial summary of the first prompt to fine-tune it through iterative feedback and rewriting. Each iteration's output becomes the input for the next iteration, and this process is controlled by the `depth` parameter.

```python
# Recursive Prompt

from pydantic import BaseModel, Field

from mirascope import llm

class SummaryFeedback(BaseModel):
    """Feedback on summary with a critique and review rewrite based on said critique."""

    critique: str = Field(..., description="The critique of the summary.")
    rewritten_summary: str = Field(
        ...,
        description="A rewritten summary that takes the critique into account.",
    )

@llm.call("openai/gpt-4o-mini")
def summarize(original_text: str):
    return f"Summarize the following text into one sentence: {original_text}"

@llm.call("openai/gpt-4o-mini", format=SummaryFeedback)
def resummarize(original_text: str, summary: str):
    return f"""
    Original Text: {original_text}
    Summary: {summary}

    Critique the summary of the original text.
    Then rewrite the summary based on the critique. It must be one sentence.
    """

def rewrite_iteratively(original_text: str, depth=2):
    summary = summarize(original_text=original_text).text()
    for _ in range(depth):
        feedback = resummarize(original_text=original_text, summary=summary).parse()
        summary = feedback.rewritten_summary
    return summary

original_text = """
In the heart of a dense forest, a boy named Timmy pitched his first tent, fumbling with the poles and pegs.
His grandfather, a seasoned camper, guided him patiently, their bond strengthening with each knot tied.
As night fell, they sat by a crackling fire, roasting marshmallows and sharing tales of old adventures.
Timmy marveled at the star-studded sky, feeling a sense of wonder he'd never known.
By morning, the forest had transformed him, instilling a love for the wild that would last a lifetime.
"""

print(rewrite_iteratively(original_text=original_text))

# > In a dense forest, clumsy but eager Timmy, with his grandfather's patient guidance, pitched his first tent, roasted marshmallows by a crackling fire while sharing stories, and gazed at the starry sky, igniting a profound sense of wonder and a lifelong love for nature.
```

## Getting Started with Prompt Chaining, with Examples

Many frameworks offer dedicated chaining functionality, such as LangChain Expression Language (LCEL), which is a declarative language for composing chains.

As its name suggests, [LangChain](/blog/langchain-alternatives) was created with chaining in mind and offers specialized classes and abstractions for accomplishing sequences of LLM calls.

You typically assemble chains in LCEL using its pipe operator (`|`), along with classes like `Runnable` and `SequentialChain`. LCEL chains generally have this format:

```python
chain = prompt | model | output_parser
```

LCEL works well with simpler prompts, offering a compact syntax with straightforward flows via the pipe moderator.

But when building more complex chains, we found it challenging to debug errors and follow pipeline operations in detail. For example, with LCEL we always had to use an object that fit into it, so we couldn’t just code as we normally would.

In particular, we found `RunnablePassthrough`, an object that forwards input data without changes through a chain, to be unnecessary and to actually hinder building complex prompts with parallel sub chains. It’s more of an afterthought to solve a problem that LangChain itself created with LCEL. If you do things in a Pythonic way like Mirascope, you don’t need additional classes or structures to pass information through a chain-you simply always have access to it like you should.

Due to the complexity of [working with such frameworks](/blog/engineers-should-handle-prompting-llms) we designed Mirascope so you can build pipelines using the Python syntax you already know, rather than having to learn new structures.

Mirascope lets you build prompt chains using standard Python functions, as explained below.

### Chaining Prompts in Mirascope

Chaining prompts in Mirascope means calling one LLM function from within another, using the output of the first call as input to the next, as in the example below:

```python
from mirascope import llm

@llm.call("openai/gpt-4o-mini")
def select_scientist(field_of_study: str):
    return f"""
    Name a scientist who is known for their work in {field_of_study}.
    Give me just the name.
    """

@llm.call("openai/gpt-4o-mini")
def explain_theory(field_of_study: str, topic: str):
    scientist = select_scientist(field_of_study=field_of_study).text()
    return [
        llm.messages.system(f"Imagine that you are {scientist}. Your task is to explain a theory that you, {scientist}, are famous for."),
        llm.messages.user(f"Explain the theory related to {topic}."),
    ]

response = explain_theory(field_of_study="physics", topic="theory of relativity")
print(response.text())

# > Certainly! The theory of relativity, which I formulated in two parts—the Special Theory of Relativity (1905) and  ...
```

By chaining calls and storing intermediate results, you can inspect the output at every step of the chain:

```python
print(response.raw.model_dump_json(indent=2))

# {
#   "id": "resp_02a9a0e0a51e33700069790c0d5c848193b543112f44c822a0",
#   "created_at": 1769540621.0,
#   "error": null,
#   "incomplete_details": null,
#   "instructions": null,
#   "metadata": {},
#   "model": "gpt-4o-mini-2024-07-18",
#   "object": "response",
#   "output": [
#     {
#       "id": "msg_02a9a0e0a51e33700069790c0db9588193bc45f4f0a6944a17",
#       "content": [
#         {
#           "annotations": [],
#           "text": "Certainly! The theory of relativity, which I developed, is comprised of two main parts: special relativity and general relativity. \n\n### Special Relativity (1905)\n\nThis theory deals with the laws of physics in the absence of gravity and introduces two core ideas:\n\n1. **Invariance of the Speed of Light**: The speed of light in a vacuum is constant (approximately \\( 299,792,458 \\) meters per second) and does not change regardless of the motion of the observer or the source of light. \n\n2. **Relative Motion**: The laws of physics are the same for all observers, regardless of their relative motion. This leads to the famous equation \\( E = mc^2 \\), which expresses the equivalence of mass (m) and energy (E), with \\( c \\) being the speed of light. This implies that mass can be converted into energy and vice versa.\n\nKey implications of special relativity include time dilation (time moves slower for objects moving at high speeds compared to stationary observers) and length contraction (objects appear shorter in the direction of motion from the perspective of a stationary observer).\n\n### General Relativity (1915)\n\nGeneral relativity extends these principles to include gravity. It proposes a revolutionary view of gravity not as a force in the traditional sense, but rather as a curvature of spacetime caused by mass. \n\n1. **Curvature of Spacetime**: Massive objects like planets and stars warp the fabric of spacetime around them. This curvature affects the motion of other objects, causing them to follow curved paths, which we perceive as the force of gravity.\n\n2. **Equivalence Principle**: Local observations within a freely falling reference frame (like an elevator in free fall) cannot distinguish between gravitational effects and acceleration due to motion.\n\nConsequences of general relativity include the prediction of the bending of light around massive objects (gravitational lensing) and the existence of phenomena such as black holes and gravitational waves.\n\nBoth theories fundamentally changed our understanding of space, time, and gravity, revolutionizing physics and leading to important technological advancements, from GPS systems to our understanding of the universe.",
#           "type": "output_text",
#           "logprobs": []
#         }
#       ],
#       "role": "assistant",
#       "status": "completed",
#       "type": "message"
#     }
#   ],
#   "parallel_tool_calls": true,
#   "temperature": 1.0,
#   "tool_choice": "auto",
#   "tools": [],
#   "top_p": 1.0,
#   "background": false,
#   "completed_at": 1769540627.0,
#   "conversation": null,
#   "max_output_tokens": null,
#   "max_tool_calls": null,
#   "previous_response_id": null,
#   "prompt": null,
#   "prompt_cache_key": null,
#   "prompt_cache_retention": null,
#   "reasoning": {
#     "effort": null,
#     "generate_summary": null,
#     "summary": null
#   },
#   "safety_identifier": null,
#   "service_tier": "default",
#   "status": "completed",
#   "text": {
#     "format": {
#       "type": "text"
#     },
#     "verbosity": "medium"
#   },
#   "top_logprobs": 0,
#   "truncation": "disabled",
#   "usage": {
#     "input_tokens": 45,
#     "input_tokens_details": {
#       "cached_tokens": 0
#     },
#     "output_tokens": 435,
#     "output_tokens_details": {
#       "reasoning_tokens": 0
#     },
#     "total_tokens": 480
#   },
#   "user": null,
#   "billing": {
#     "payer": "developer"
#   },
#   "frequency_penalty": 0.0,
#   "presence_penalty": 0.0,
#   "store": true
# }

```

This approach offers several benefits:

- Your pipelines are readable and the flow of data is clear.
- Functions can be reused across different chains.
- You have full control over when and how intermediate results are used.
- Standard Python patterns like caching (`@functools.lru_cache`) work seamlessly.

### Alternative: Separating Chain Orchestration

You can also separate the chain orchestration into its own function, which can make complex chains easier to test and reason about:

```python
from mirascope import llm

@llm.call("openai/gpt-4o-mini")
def select_scientist(field_of_study: str):
    return f"""
    Name a scientist who is known for their work in {field_of_study}.
    Give me just the name.
    """

@llm.call("openai/gpt-4o-mini")
def explain_theory(scientist: str, topic: str):
    return [
        llm.messages.system(f"Imagine that you are {scientist}. Your task is to explain a theory that you, {scientist}, are famous for."),
        llm.messages.user(f"Explain the theory related to {topic}."),
    ]

def explain_theory_as_scientist(field_of_study: str, topic: str):
    scientist = select_scientist(field_of_study=field_of_study).text()
    explanation = explain_theory(scientist=scientist, topic=topic)
    return explanation

response = explain_theory_as_scientist(
    field_of_study="physics", topic="theory of relativity"
)
print(response.text())

# > Certainly! The theory of relativity encompasses two interrelated ...
```

With this pattern, you can easily add logging, caching, or observability at the orchestration level. For full transparency on intermediate outputs, you can log each call along the chain or use Mirascope Cloud's built-in tracing to automatically capture every LLM call.

```python
print(response.raw.model_dump_json(indent=2))

# {
#   "id": "resp_0457080fe2cb95420069790ea0066481938296b9cf8a0198f1",
#   "created_at": 1769541280.0,
#   "error": null,
#   "incomplete_details": null,
#   "instructions": null,
#   "metadata": {},
#   "model": "gpt-4o-mini-2024-07-18",
#   "object": "response",
#   "output": [
#     {
#       "id": "msg_0457080fe2cb95420069790ea05d708193b41acf233fbfb7aa",
#       "content": [
#         {
#           "annotations": [],
#           "text": "Certainly! The theory of relativity encompasses two interrelated theories: the special theory of relativity and the general theory of relativity. \n\n### Special Theory of Relativity (1905)\n\n- **Key Concepts**: The special theory of relativity introduces two main ideas: the constancy of the speed of light and the principle of relativity.\n- **Constancy of the Speed of Light**: Light travels at the same speed (approximately 299,792 kilometers per second) for all observers, regardless of their motion relative to the light source. This is a radical departure from classical mechanics.\n- **Relativity of Simultaneity**: Events that are simultaneous for one observer may not be simultaneous for another who is in motion relative to the first.\n- **Time Dilation**: As an object moves closer to the speed of light, time appears to pass more slowly for it relative to a stationary observer. This means if you were traveling at a significant fraction of the speed of light, you would age more slowly than someone who remained on Earth.\n- **Length Contraction**: Objects in motion contract in length along the direction of travel when viewed from a stationary observer’s frame of reference.\n- **Mass-Energy Equivalence**: This famous equation \\(E=mc^2\\) expresses that energy (E) and mass (m) are interchangeable, with \\(c\\) being the speed of light. A small amount of mass can be converted into a large amount of energy.\n\n### General Theory of Relativity (1915)\n\n- **Key Concepts**: General relativity extends the principles of special relativity to include gravity, describing it not as a force, but as the warping of spacetime caused by mass.\n- **Curvature of Spacetime**: Massive objects, like planets and stars, curve the fabric of spacetime around them. This curvature influences the motion of other objects, which appear to fall toward the massive object as if they were being pulled by a force.\n- **Geodesics**: Objects in free-fall follow the straightest possible paths (geodesics) in this curved spacetime. This explains why planets orbit stars and why light bends around massive objects—light follows the curvature of spacetime as well.\n- **Gravitational Time Dilation**: Time runs slower in stronger gravitational fields. For instance, a clock on the surface of the Earth ticks more slowly than a clock in a satellite orbiting the planet.\n\n### Implications\nBoth theories revolutionized our understanding of space, time, and gravity. They have profound implications for cosmology, astrophysics, and even technologies like GPS, which must account for time dilation effects to function accurately.\n\nSo, in summary, the theory of relativity fundamentally changed our understanding of how the universe operates, combining the realms of space and time into a unified framework that continues to guide modern physics.",
#           "type": "output_text",
#           "logprobs": []
#         }
#       ],
#       "role": "assistant",
#       "status": "completed",
#       "type": "message"
#     }
#   ],
#   "parallel_tool_calls": true,
#   "temperature": 1.0,
#   "tool_choice": "auto",
#   "tools": [],
#   "top_p": 1.0,
#   "background": false,
#   "completed_at": 1769541287.0,
#   "conversation": null,
#   "max_output_tokens": null,
#   "max_tool_calls": null,
#   "previous_response_id": null,
#   "prompt": null,
#   "prompt_cache_key": null,
#   "prompt_cache_retention": null,
#   "reasoning": {
#     "effort": null,
#     "generate_summary": null,
#     "summary": null
#   },
#   "safety_identifier": null,
#   "service_tier": "default",
#   "status": "completed",
#   "text": {
#     "format": {
#       "type": "text"
#     },
#     "verbosity": "medium"
#   },
#   "top_logprobs": 0,
#   "truncation": "disabled",
#   "usage": {
#     "input_tokens": 45,
#     "input_tokens_details": {
#       "cached_tokens": 0
#     },
#     "output_tokens": 586,
#     "output_tokens_details": {
#       "reasoning_tokens": 0
#     },
#     "total_tokens": 631
#   },
#   "user": null,
#   "billing": {
#     "payer": "developer"
#   },
#   "frequency_penalty": 0.0,
#   "presence_penalty": 0.0,
#   "store": true
# }
```

This separation of concerns offers several advantages:

- Explicit control over the flow of execution, making it easier to implement complex chains where steps might need to be skipped or conditionally executed.
- The orchestration function can be easily tested independently from the individual LLM calls.
- Functions can be reused across different chains and contexts.

## Downsides of Prompt Chaining

Prompt chaining provides utility in situations where you want to set up pipelines for processing and transforming data step-by-step.

However, there are downsides:

- Each step of the chain needs to be explicitly defined ahead of time in a fixed sequence, which leaves little room for deviation based on runtime conditions or user input beyond what has been predefined. A dynamically generated agent workflow might adapt more flexibly to different conditions at runtime, but with this you also lose granular control over each piece of the chain.
- Prompt chaining isn’t necessarily a cheaper option than alternatives, but it depends on how calls are structured since they’re token based and a chain with two calls may cost the same as a single CoT call if the input and output tokens are the same.
- Speed might also be a downside, as you’ll need to make several requests instead of just one and this can slow down the "real-time" feel of the application. In such cases, you might consider providing feedback to the user, like progress messages, as the chain is executing intermediate steps.

You should balance these considerations when deciding whether to leverage prompt chaining for generative AI in your application.

## Best Practices for Prompt Chaining

Good prompt chaining practices share a lot with best [prompt engineering](/blog/prompt-engineering-tools) practices. Here are some recommendations for developing effective prompt chains when using AI models:

### Keep Your Prompts Focused and Specific

Clear and concise prompts minimize misunderstandings by the model. For example, rather than writing, "Tell me something about scientists," a more focused prompt might be, "Name a scientist known for their work in quantum physics."

Each prompt should also perform a specific function and have a single responsibility assigned. This means breaking down complex tasks into smaller, manageable steps where each prompt addresses one aspect of the overall task.

### Manage Data Carefully

Ensure that the format of the desired output of one prompt matches the expected input format of the next prompt to avoid errors. Schemas can easily help you achieve this, and Mirascope's [Structured Output](/docs/learn/llm/structured-output) feature uses a `format` parameter built on top of Pydantic to help you define schemas, extract structured outputs from large language models, and validate those outputs.

For example, if a prompt expects a JSON object with specific fields, ensure the preceding prompt generates data in that exact structure. As well, be prepared to transform data to meet the input requirements of subsequent prompts. This might involve parsing JSON, reformatting strings, or converting data types.

### Optimize for Performance

Use caching like `@functools.lru_cache` whenever feasible to improve performance. We also recommend finding ways to minimize the number of sequential calls by combining steps where possible, which may involve consolidating prompts or rethinking the chain structure to reduce the dependency on intermediate results.

The goal should be to minimize latency for the user experience, but there’s a tradeoff with complexity, as decreasing latency might increase the complexity of individual steps. Developers need to balance the trade-offs between maintaining a clear, manageable chain and optimizing for speed.

## Harness the Efficiency of Python for Prompt Chaining

Mirascope leverages Python for chaining LLM calls, avoiding the need for complex abstractions with steep learning curves, as well as having to master new frameworks.

By keeping things simple, Mirascope lets you work with your data directly, helping you keep your workflows smoother and more intuitive.

Want to learn more? You can find more Mirascope code samples on both our [docs](/docs/mirascope) and on [GitHub](https://github.com/mirascope/mirascope/).
