---
title: "A Guide to LLM Orchestration"
description: "Explore our detailed guide to LLM orchestration and discover how it improves the performance and efficiency of applications driven by large language models."
date: "2024-08-12"
updatedAt: "2026-01-28"
readTime: "15 min read"
author: "William Bakst"
---

LLM orchestration manages and coordinates interactions and workflows for improving the performance and effectiveness of LLM-driven applications.

Central to this is the large language model and its responses, which are non-deterministic and typically unstructured. This makes it challenging for other components of the application to use those responses.

But these challenges can be overcome by decision-making around careful application design on the one hand, and effective orchestration on the other.

**Among other things, orchestration in particular:**

- Ensures the quality of prompts being sent to the LLM.
  ‍
- Evaluates the model’s outputs for criteria like helpfulness, relevance, bias, etc.
  ‍
- Structures the model’s outputs so they’re reusable by other components.
  ‍
- Helps provide a consistent user experience through reliable error handling and load balancing.

These tasks and flows are sufficiently complex that frameworks-which consist of specialized tools and libraries-have been developed to coordinate and execute them. A framework ensures that the application correctly leverages LLMs to deliver high-quality, reliable, and contextually appropriate responses to end users.

Three of the most popular frameworks today are open source and handle a variety of use cases:

- **LangChain** was among the first frameworks to become publicly available, and offers robust agentic capabilities and a standard interface for chaining LLM calls.
- **LlamaIndex** is known for its data ingestion and information retrieval use cases, and is often used for applications like retrieval augmented generation (RAG).
- **Haystack** focuses on building pipelines for NLP tasks like question answering, document search, and summarization.

When implemented correctly, frameworks have the adaptability to handle the intricacies around operations and workflows to ensure a good user experience. And at best they should be intuitive to set up and allow developers to iterate quickly.

We find, however, that [modern frameworks are quite complex](/blog/llamaindex-vs-langchain) (and are often an ecosystem in themselves), at least partly because:

- Some of their abstractions seem to reinvent the wheel for things you’d normally use vanilla Python for.
- Their inner workings can be opaque and hard to figure out, especially for debugging purposes.
- They have a tendency to produce new modules that are less efficient than existing, purpose-built tools.
- Their dependency on numerous wrappers and abstractions, often leading to an excess of import statements, makes maintenance challenging.

**In response to these challenges, we developed [Mirascope](https://github.com/mirascope/mirascope), a lightweight and modular Python toolkit designed for the rapid development of LLM applications.**

Our modules slot into your existing workflows where you’d prefer the simplicity and flexibility of plain Python over using a framework’s abstractions.

Below, we describe key aspects of orchestration and bring examples of how these are implemented by frameworks.

## 6 Key Aspects of LLM Orchestration (with Examples)

Orchestration typically involves six key aspects in the context of application development:

### 1. Task Management

Tasks are organized and managed as part of workflow execution within LLM applications. This involves automation around creation, scheduling, and completion of complex tasks.

#### Prompt Engineering

An example of such a task is the [development of prompts](/blog/llamaindex-vs-langchain), which serve as the primary interface for interacting with the model.

To get the best responses, prompts should [contain clear instructions](/blog/prompt-engineering-best-practices) as shown in the example code below, which generates a prompt for recommending movies based on recently watched titles:

```python
from mirascope import llm

@llm.call("openai/gpt-4o-mini")
def recommend_movie_prompt(movie_titles: list[str]) -> str:
    titles_in_quotes = ", ".join([f'"{title}"' for title in movie_titles])
    return f"""
    I've recently watched the following movies: {titles_in_quotes}.
    What should I watch next?
    """

response = recommend_movie_prompt(["The Dark Knight", "Forrest Gump"])
```

This uses Mirascope's [`@llm.call`](/docs/learn/llm/calls) decorator, which centralizes internal prompt logic and turns the prompt into an actual LLM API call.

The above code also illustrates our principle of colocation, where everything impacting the quality of a call is colocated and versioned together. This can be seen in the computed field `titles_in_quotes` computed within the prompt function.

This approach to colocation emerged from our frustrations around working with both [LangChain](/blog/langchain-runnables) and the OpenAI SDK, where LLM calls and their parameters are separately managed. Making changes to one part of the code, like updating the model specification, often required efforts to track those changes in other parts.

#### Tools (Function Calling)

Frameworks also support [tools (or function calling)](/blog/openai-function-calling) to autonomously execute tasks like checking stock prices and classifying spam. This makes them agents working on your behalf, with the ability to choose which functions or external services to use.

For example, prompting an LLM with flight information will return a call to a function that checks the status of flights.

You enable this capability by passing it a function call signature, which will usually be a raw JSON file in a format like this:

```json
"tools": [
  {
    "type": "function",
    "function": {
      "name": "get_flight_information",
      "description": "Get the current flight status",
      "parameters": {
        "type": "object",
        "properties": {
          "flight_number": {
            "type": "string",
            "description": "The flight number, e.g., AA123"
          },
          "date": {
            "type": "string",
            "description": "The date of the flight in YYYY-MM-DD format"
          }
        },
        "required": ["flight_number", "date"]
      }
    }
  }
]
```

Writing your own JSON schema code can get messy quickly.

With Mirascope, we've added a convenient feature that allows you to use a function's docstring to automatically generate the function signature for you:

```python
from mirascope import llm

@llm.tool
def get_flight_information(flight_number: str, date: str) -> str:
    """Get the current flight status for `flight_number` on `date` and print it.

    Args:
        flight_number: The flight number, e.g., AA123.
        date: The date of the flight in YYYY-MM-DD format.
    """
    if flight_number == "AA123" and date == "2024-08-05":
        return f"The flight {flight_number} on {date} is on time."
    elif flight_number == "DL456" and date == "2024-08-05":
        return f"The flight {flight_number} on {date} is delayed."
    else:
        return f"I'm sorry, I don't have the information for flight {flight_number} on {date}."

@llm.call("openai/gpt-4o", tools=[get_flight_information])
def flight_info(flight_number: str, date: str) -> str:
    return f"What's the flight status of {flight_number} on {date}?"

response = flight_info("AA123", "2024-08-05")
while response.tool_calls:
    tool_results = response.execute_tools()
    response = response.resume(tool_results)
print(response.text())

# > The flight AA123 on 2024-08-05 is on time.
```

In the above code, we set up the function `get_flight_information` with the `@llm.tool` decorator and docstring to generate a function call schema, and then we register this function as a tool (Mirascope also provides ways of turning functions into tools [without needing a docstring](/docs/learn/llm/tools)).

### 2. Data Preparation

Data preparation involves cleaning, structuring, and formatting data from various data sources so it can be used by either the model or by other components (like vector databases) in the application.

An example of this is data extraction, where LLM outputs (usually raw text) are parsed to extract key information and save it in a format like CSV or JSON. This prepares the data for usage by other components and external applications.

The Mirascope code below provides an example of data extraction where key film details are extracted from the model's responses:

```python
from pydantic import BaseModel

from mirascope import llm

class MovieDetails(BaseModel):
    title: str
    director: str

@llm.call("openai/gpt-4o", format=MovieDetails)
def extract_movie_details(movie: str) -> str:
    return f"Extract details from this movie: {movie}"

movie = "Inception directed by Christopher Nolan."
response = extract_movie_details(movie)
movie_details = response.parse()
assert isinstance(movie_details, MovieDetails)
print(movie_details)

# > title='Inception' director='Christopher Nolan'
```

The function `extract_movie_details` takes a string input (`movie`), which is passed to the LLM. The model then generates an output that's parsed and validated by Pydantic against the `MovieDetails` schema using `.parse()`.

Extracting details in this way allows you to parse large documents, articles, or research papers, and to store the details in databases or in so-called extract, transform, load (ETL) pipelines for ingestion into data warehouses.

### 3. Workflow Integration

Workflow integration embeds tools, technologies, or processes into existing operational systems and workflows to improve efficiency, consistency, and productivity.

This includes maintaining [version control of prompts](/blog/prompt-versioning) and AI models and ensuring smooth transitions between different model providers.

#### Changing Model Providers

Mirascope allows you to [change model providers](/docs/learn/llm/calls) by simply changing the model string. The provider is specified in the model identifier:

```python
from mirascope import llm

@llm.call("anthropic/claude-sonnet-4-20250514")
def recommend_recipe(ingredient: str) -> str:
    return f"Recommend a recipe that includes {ingredient}"

response = recommend_recipe(ingredient="chicken")
print(response.text())

# > Chicken Alfredo with Creamy Garlic Sauce
```

Mirascope also makes it convenient to switch between providers at runtime using the `llm.model()` context manager:

```python
from mirascope import llm

@llm.call("openai/gpt-4o-mini")
def recommend_book(genre: str) -> str:
    return f"Recommend a {genre} book"

# OpenAI (default)
openai_response = recommend_book("fantasy")
print(openai_response.text())

# Anthropic (runtime override)
with llm.model("anthropic/claude-sonnet-4-20250514"):
    anthropic_response = recommend_book("fantasy")
    print(anthropic_response.text())
```

This unified API means you can easily experiment with different providers without changing your core logic.

#### Prompt Management

A prompt management system maintains [consistency in prompting](/blog/langchain-prompt-template) while helping you iterate and experiment more productively. It also integrates with CI/CD pipelines while promoting collaboration and automates change tracking.

An example of prompt management is [Mirascope Cloud](/blog/prompt-versioning), which versions and traces all prompts automatically so you don't have to worry about it and can instead focus on iterating and improving your prompts.

The magic is that Mirascope Cloud automatically versions the entire lexical closure. This means that changes to helper methods and utility functions will be considered version changes, enabling you to more easily catch changes that impact your prompt quality even when unexpected.

### 4. Scalability

To support scalability, frameworks leverage technologies that enable high throughput and minimal latency, such as streaming and asynchronous function calls. Additionally, being able to switch between different model providers (as we’ve covered in the last section) lets you choose providers offering better performance and handling of large-scale data.

#### Streaming Model Responses

A common strategy is to stream large responses as chunks, rather than waiting for the entire response to be generated. Mirascope [streams](/docs/learn/llm/streams), which you can access using the `.stream()` method on the [`call`](/docs/learn/llm/calls) function, provide convenience wrappers around the original response chunks:

```python
from mirascope import llm

@llm.call("openai/gpt-4o")
def recommend_book(genre: str) -> str:
    return f"Recommend some {genre} books."

stream = recommend_book.stream(genre="science fiction")
for chunk in stream.text_stream():
    print(chunk, end="", flush=True)
```

The response is processed chunk by chunk in a `for` loop, printing each chunk's text as it's received. The response can be displayed incrementally as it's generated, rather than waiting for the entire response to be completed.

Streaming reduces the wait time associated with generating and delivering large responses and is ideal for real-time applications or when multiple users are interacting with the system simultaneously.

#### Async Streaming

[Asynchronous streaming](/docs/learn/llm/async/#async-streaming) supports concurrency, as offered by our `stream_book_recommendation` function that sets up an asynchronous stream to get responses from the natural language processing model:

```python
import asyncio

from mirascope import llm

@llm.call("openai/gpt-4o")
async def recommend_book(genre: str) -> str:
    return f"Recommend a {genre} book"

async def stream_book_recommendation(genre: str):
    stream = await recommend_book.stream(genre=genre)
    async for chunk in stream.text_stream():
        print(chunk, end="", flush=True)

asyncio.run(stream_book_recommendation(genre="science fiction"))
```

This lets the application handle other tasks while waiting for data chunks to arrive, improving its overall responsiveness.

Async streaming is particularly useful for applications like real-time chat, or in situations where efficient resource usage and handling of multiple concurrent operations are necessary, such as for web servers, data processing pipelines, and interactive user interfaces.

### 5. Error Handling

Since LLMs have an element of unpredictability and sometimes produce undesired results, it's necessary to detect and manage errors effectively.

This is done using validation methods and custom error-handling rules, which can detect potential issues before they become problems and ensure applications run better.

As we've already covered above in [Data Preparation](#2-data-preparation), Pydantic is a popular library for catching errors. We can handle these errors gracefully using both Python and Pydantic as shown below:

```python
from mirascope import llm
from pydantic import BaseModel, ValidationError

class Movie(BaseModel):
    title: str
    rating: float

@llm.call("openai/gpt-4o-mini", format=Movie)
def recommend_movie() -> str:
    return "Recommend a movie"

try:
    response = recommend_movie()
    movie = response.parse()
    assert isinstance(movie, Movie)
    print(movie)
    # > title='Inception' rating=8.8

except ValidationError as e:
    print(e)
    # > 1 validation error for Movie
    #  rating
    #    Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='excellent', input_type=str]
    #      For further information visit https://errors.pydantic.dev/2.6/v/float_parsing
```

The `Movie` class above defines a schema with `title` as a string and `rating` as a float, while `recommend_movie()` returns a response that can be parsed according to the `Movie` schema.

The `try-except` block with `ValidationError` provides error handling.

On the input side of the LLM, however, a data framework will usually expect you to write your own custom validation logic. This is where Mirascope is different, as Mirascope calls will check for type safety for your prompts.

We also provide documentation and linting for your IDE, such as flagging missing arguments:

![IDE Documentation and Linting Example: Argument missing for parameter](/assets/blog/llm-orchestration/mirascope-type-hints.webp)

And providing auto suggestions:

![IDE Auto Suggestions: recommend_book_prompt](/assets/blog/llm-orchestration/mirascope-autocomplete.webp)

Another way to handle errors is by retrying tasks like calling an LLM provider when this fails due to issues with the network, rate limits of external APIs, or service outages.

For instance, Mirascope offers automatic retry functionality via integrating with [Tenacity](https://tenacity.readthedocs.io/en/latest/#):

```python
from mirascope import llm
from tenacity import retry, retry_if_result, stop_after_attempt

@retry(
    stop=stop_after_attempt(3),
    retry=retry_if_result(lambda result: "math" in result.text().lower()),
)
@llm.call("openai/gpt-4o-mini")
def recommend_book(genre: str) -> str:
    return f"Recommend a {genre} book"

response = recommend_book(genre="science fiction")
print(response.text())  # Content should not contain "math"
```

Since Tenacity's `retry` is a decorator, you can use it with any Mirascope `@llm.call` decorated function as well.

### 6. Performance Optimization

Performance optimization is about making sure applications run efficiently, with a focus on reducing latency, managing computational resources, and ensuring quick and accurate LLM responses.

[Frameworks like LangChain](/blog/langchain-alternatives) generally offer strategies for this, such as parallel processing, caching frequently accessed data, efficient memory management, and asynchronous operations to improve responsiveness and throughput.

Async streaming is an example of performance optimization that we’ve previously discussed above, and other examples include LLM call chaining and high-level wrappers for model APIs, described below.

[Chaining LLM calls](/blog/prompt-chaining) can be an optimization strategy as this enables using the `@functools.lru_cache` decorator to cache the output of each call in the chain so it only has to be made once:

```python
from functools import lru_cache

from mirascope import llm

@lru_cache
@llm.call("openai/gpt-4o-mini")
def select_painter(art_style: str) -> str:
    return f"Name a painter known for {art_style}. Return only their name."

@lru_cache
@llm.call("openai/gpt-4o-mini")
def select_artwork(painter: str) -> str:
    return f"Name an art piece created by {painter}. Return only the name."

@lru_cache
@llm.call("openai/gpt-4o-mini")
def describe_artwork(art_style: str) -> str:
    painter = select_painter(art_style).text()
    artwork = select_artwork(painter).text()
    return f"""
    SYSTEM:
    Imagine that you are {painter}.
    Your task is to describe a famous painting that you, {painter}, created.

    USER:
    Describe {artwork}.
    """

print(describe_artwork("contemporary").text())

# > As Yayoi Kusama, I am thrilled to share my work "Infinity Mirror Room," a...

print(describe_artwork("contemporary").text())  # uses cached values

# > As Yayoi Kusama, I am thrilled to share my work "Infinity Mirror Room," a...

print(describe_artwork("modern").text())  # generates new output

# > Guernica, my monumental work created in 1937, is a passionate response...
```

Above, the `@functools.lru_cache` decorator caches the result of the call for each function by its arguments. This means that any calls made to the chain with the same inputs will reuse the previously generated values without re-executing the call.

Another way to optimize performance is by using high-level wrappers around APIs of both model providers and external tools and libraries. These abstract away the low-level details involved in making API calls.

Besides offering a range of utilities for external providers, Mirascope also provides a simple interface for LLM calls with the `@llm.call` decorator using a unified `"provider/model"` format. You can make calls to various providers (e.g., OpenAI, Anthropic, Google) using this same decorator syntax, allowing for switching without needing to change the core logic.

## Achieve Orchestration with Mirascope

Designed according to [software development best practices](/blog/engineers-should-handle-prompting-llms), Mirascope offers tools that handle the complexity of LLM orchestration for building production-grade LLM apps. Our library supports data integration and lets you code in the Python you normally code in, ensuring a smooth integration with your existing projects.

Want to learn more? You can find more Mirascope code samples both on our [docs](/docs/learn/llm) and on our [GitHub page](https://github.com/mirascope/mirascope).
