{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a116fc1117d5fd",
   "metadata": {},
   "source": [
    "# Transcribing Speech\n",
    "\n",
    "In this recipe, we go over how to transcribe the speech from an audio file using Gemini 1.5 Flash’s audio capabilities.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "<p class=\"admonition-title\">Mirascope Concepts Used</p>\n",
    "<ul>\n",
    "<li><a href=\"../../../learn/prompts/\">Prompts</a></li>\n",
    "<li><a href=\"../../../learn/calls/\">Calls</a></li>\n",
    "<li><a href=\"../../../learn/response_models/\">Response Model</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "<p class=\"admonition-title\">Background</p>\n",
    "<p>\n",
    "LLMs have significantly advanced speech transcription beyond traditional machine learning techniques, by improved handling of diverse accents and languages, and the ability to incorporate context for more precise transcriptions. Additionally, LLMs can leverage feedback loops to continuously improve their performance and correct errors through simple prompting.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83819da0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing Mirascope and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mirascope[gemini]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73893d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "# Set the appropriate API key for the provider you're using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf06ec",
   "metadata": {},
   "source": [
    "## Transcribing Speech using Gemini\n",
    "\n",
    "With Gemini’s multimodal capabilities, audio input is treated just like text input, which means we can use it as context to ask questions. We will use an audio clip provided by Google of [a countdown of the Apollo Launch](https://storage.googleapis.com/generativeai-downloads/data/Apollo-11_Day-01-Highlights-10s.mp3). Note that if you use your own URL, Gemini currently has a byte limit of `20971520` when not using their file system.\n",
    "\n",
    "Since we can treat the audio like any other text context, we can create a transcription simply by inserting the audio into the prompt and asking for a transcription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c153509906832b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:25:50.083834Z",
     "start_time": "2024-09-30T07:25:48.132213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 9 8 We have a goal for main engine start. We have a main engine start. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from google.generativeai import configure\n",
    "from mirascope.core import gemini, prompt_template\n",
    "\n",
    "configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "apollo_url = \"https://storage.googleapis.com/generativeai-downloads/data/Apollo-11_Day-01-Highlights-10s.mp3\"\n",
    "\n",
    "\n",
    "@gemini.call(model=\"gemini-1.5-flash\")\n",
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    Transcribe the content of this speech:\n",
    "    {url:audio}\n",
    "    \"\"\"\n",
    ")\n",
    "def transcribe_speech_from_url(url: str): ...\n",
    "\n",
    "\n",
    "response = transcribe_speech_from_url(apollo_url)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380937ffe3a34c5",
   "metadata": {},
   "source": [
    "\n",
    "## Tagging audio \n",
    "\n",
    "We can start by creating a Pydantic Model with the content we want to analyze:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad46415c8c2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class AudioTag(BaseModel):\n",
    "    audio_quality: Literal[\"Low\", \"Medium\", \"High\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"The quality of the audio file.\n",
    "        Low - unlistenable due to severe static, distortion, or other imperfections\n",
    "        Medium - Audible but noticeable imperfections\n",
    "        High - crystal clear sound\"\"\",\n",
    "    )\n",
    "    imperfections: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"A list of the imperfections affecting audio quality, if any.\n",
    "        Common imperfections are static, distortion, background noise, echo, but include\n",
    "        all that apply, even if not listed here\"\"\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ..., description=\"A one sentence description of the audio content\"\n",
    "    )\n",
    "    primary_sound: str = Field(\n",
    "        ...,\n",
    "        description=\"\"\"A quick description of the main sound in the audio,\n",
    "        e.g. `Male Voice`, `Cymbals`, `Rainfall`\"\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383c7a7649fa2eb",
   "metadata": {},
   "source": [
    "Now we make our call passing in our `AudioTag` into the `response_model` field:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889c5303d80e15ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T07:25:55.467244Z",
     "start_time": "2024-09-30T07:25:54.020986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_quality='Medium' imperfections=['Background noise'] description='A countdown from ten with a male voice announcing \"We have a go for main engine start\"' primary_sound='Male Voice'\n"
     ]
    }
   ],
   "source": [
    "@gemini.call(model=\"gemini-1.5-flash\", response_model=AudioTag, json_mode=True)\n",
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    Analyze this audio file\n",
    "    {url:audio}\n",
    "\n",
    "    Give me its audio quality (low, medium, high), a list of its audio flaws (if any),\n",
    "    a quick description of the content of the audio, and the primary sound in the audio.\n",
    "    Use the tool call passed into the API call to fill it out.\n",
    "    \"\"\"\n",
    ")\n",
    "def analyze_audio(url: str): ...\n",
    "\n",
    "\n",
    "response = analyze_audio(apollo_url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d048921fe31fc",
   "metadata": {},
   "source": [
    "\n",
    "## Speaker Diarization\n",
    "\n",
    "Now let's look at an audio file with multiple people talking. For the purposes of this recipe, I grabbed a snippet from Creative Commons[https://www.youtube.com/watch?v=v0l-u0ZUOSI], around 1:15 in the video and giving Gemini the audio file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7763f40edac8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"YOUR_MP3_HERE\", \"rb\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "    @gemini.call(model=\"gemini-1.5-flash\")\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        Transcribe the content of this speech adding speaker tags \n",
    "        for example: \n",
    "            Person 1: hello \n",
    "            Person 2: good morning\n",
    "        \n",
    "        \n",
    "        {data:audio}\n",
    "        \"\"\"\n",
    "    )\n",
    "    def transcribe_speech_from_file(data: bytes): ...\n",
    "\n",
    "    response = transcribe_speech_from_file(data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b2143efcd8a72",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"admonition tip\">\n",
    "<p class=\"admonition-title\">Additional Real-World Examples</p>\n",
    "<ul>\n",
    "<li><b>Subtitles and Closed Captions</b>: Automatically generate subtitles for same and different languages for accessibility.</li>\n",
    "<li><b>Meetings</b>: Transcribe meetings for future reference or summarization.</li>\n",
    "<li><b>Voice Assistant</b>: Transcription is the first step to answering voice requests.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "When adapting this recipe to your specific use-case, consider the following:\n",
    "\n",
    "- Split your audio file into multiple chunks and run the transcription in parallel.\n",
    "- Compare results with traditional machine learning techniques.\n",
    "- Experiment with the prompt by giving it some context before asking to transcribe the audio.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
