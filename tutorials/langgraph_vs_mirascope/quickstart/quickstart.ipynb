{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a63e84444265d",
   "metadata": {},
   "source": [
    "# LangGraph Quickstart using Mirascope\n",
    "\n",
    "We'll implement the [LangGraph Quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/) using Mirascope. We'll build a chatbot with a web search tool, conversation history, and human-in-the-loop functionality. Throughout the process, we'll apply Mirascope best practices, which align with general Python best practices. This approach will demonstrate how straightforward it is to create a sophisticated conversational AI system using Mirascope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce96c5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing Mirascope and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mirascope[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "# Set the appropriate API key for the provider you're using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9580e",
   "metadata": {},
   "source": [
    "## Part 1: Building a Basic Chatbot\n",
    "\n",
    "A chatbot must possess at least two key capabilities to be considered as such:\n",
    "\n",
    "* The ability to engage in conversation with a user\n",
    "* The capacity to retain and reference information from the ongoing dialogue\n",
    "\n",
    "Let's take a look at how that looks using Mirascope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5848dfa49bcc5d69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T13:15:12.562911Z",
     "start_time": "2024-09-30T13:14:35.892754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(User): Hi there! My name is Will\n",
      "(Assistant): Hi Will! How can I assist you today?\n",
      "(User): Remember my name?\n",
      "(Assistant): Yes, your name is Will! What would you like to talk about today?\n",
      "(Assistant): Have a great day!\n"
     ]
    }
   ],
   "source": [
    "from mirascope.core import BaseMessageParam, openai, prompt_template\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Chatbot(BaseModel):\n",
    "    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "\n",
    "    @openai.call(model=\"gpt-4o-mini\", stream=True)\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM: You are a helpful assistant.\n",
    "        MESSAGES: {self.history}\n",
    "        USER: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "    def _call(self, question: str): ...\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            question = input(\"(User): \")\n",
    "            if question in [\"quit\", \"exit\"]:\n",
    "                print(\"(Assistant): Have a great day!\")\n",
    "                break\n",
    "            stream = self._call(question)\n",
    "            print(f\"(User): {question}\", flush=True)\n",
    "            print(\"(Assistant): \", end=\"\", flush=True)\n",
    "            for chunk, _ in stream:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "            print(\"\")\n",
    "            if stream.user_message_param:\n",
    "                self.history.append(stream.user_message_param)\n",
    "            self.history.append(stream.message_param)\n",
    "\n",
    "\n",
    "Chatbot().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415e467166daac",
   "metadata": {},
   "source": [
    "\n",
    "The `run` method serves as the entry point for our Chatbot. It implements a continuous loop that:\n",
    "\n",
    "1. Prompts the user for input\n",
    "2. Processes the user's message\n",
    "3. Generates and streams the assistant's response for a real time feel\n",
    "4. Updates the conversation history\n",
    "\n",
    "This loop persists until the user chooses to exit. After each interaction, both the user's input and the assistant's response are appended to the history list. This accumulation of context allows the language model to maintain continuity and relevance in future conversations.\n",
    "\n",
    "While this basic chatbot demonstrates core functionality, we can enhance its capabilities by incorporating tools.\n",
    "\n",
    "## Part 2: Enhancing the Chatbot with Tools\n",
    "\n",
    "Tools enable language models to extend beyond their training data and access real-time information. Let's implement a `WebSearch` tool that allows the LLM to query the internet for current and relevant data.\n",
    "\n",
    "Here are a few search tools you can use.\n",
    "\n",
    "### DuckDuckGo\n",
    "\n",
    "#### DuckDuckGo Setup\n",
    "\n",
    "Install the [DuckDuckGo](https://duckduckgo.com/) python library:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf946a8c23fe3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckduckgo-search beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad810dda2796129",
   "metadata": {},
   "source": [
    "\n",
    "No API key is required for DuckDuckGo.\n",
    "\n",
    "#### Define the DuckDuckGo tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fde65687bec24b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T13:17:23.345024Z",
     "start_time": "2024-09-30T13:17:23.183213Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class WebSearch(openai.OpenAITool):\n",
    "    \"\"\"Search the web for the given text and parse the paragraphs of the results.\"\"\"\n",
    "\n",
    "    query: str = Field(..., description=\"The text to search for.\")\n",
    "\n",
    "    def call(self) -> str:\n",
    "        \"\"\"Search the web for the given text and parse the paragraphs of the results.\n",
    "\n",
    "        Returns:\n",
    "            Parsed paragraphs of each of the webpages, separated by newlines.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Search the web for the given text\n",
    "            results = DDGS(proxy=None).text(self.query, max_results=2)\n",
    "\n",
    "            # Parse the paragraphs of each resulting webpage\n",
    "            parsed_results = []\n",
    "            for result in results:\n",
    "                link = result[\"href\"]\n",
    "                try:\n",
    "                    response = requests.get(link)\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    parsed_results.append(\n",
    "                        \"\\n\".join([p.text for p in soup.find_all(\"p\")])\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    parsed_results.append(\n",
    "                        f\"{type(e)}: Failed to parse content from URL {link}\"\n",
    "                    )\n",
    "\n",
    "            return \"\\n\\n\".join(parsed_results)\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"{type(e)}: Failed to search the web for text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467fbbeebc82b80d",
   "metadata": {},
   "source": [
    "\n",
    "### Tavily\n",
    "\n",
    "#### Tavily Setup\n",
    "\n",
    "Install the [Tavily](https://tavily.com/) python library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d260fce68ff350",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6a624699aa7f2",
   "metadata": {},
   "source": [
    "\n",
    "Then get a free API key to use for the `WebSearch` tool.\n",
    "\n",
    "#### Define the Tavily tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899a60d40238ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import ClassVar\n",
    "\n",
    "from pydantic import Field\n",
    "from tavily import TavilyClient\n",
    "\n",
    "\n",
    "class WebSearch(openai.OpenAITool):  # noqa: F811\n",
    "    \"\"\"Search the web for the given text using the TavilyClient.\"\"\"\n",
    "\n",
    "    tavily_client: ClassVar[TavilyClient] = TavilyClient(\n",
    "        api_key=os.environ[\"TAVILY_API_KEY\"]\n",
    "    )\n",
    "    query: str = Field(..., description=\"The text to search for.\")\n",
    "\n",
    "    def call(self) -> str:\n",
    "        \"\"\"A web search tool that takes in a query and returns the top 2 search results.\"\"\"\n",
    "        return self.tavily_client.get_search_context(query=self.query, max_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997c4ba24f0fb6f",
   "metadata": {},
   "source": [
    "\n",
    "### Update Mirascope call\n",
    "\n",
    "Now that we have our tool defined, we can easily add the tool to our Mirascope call, like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc49257db205585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot(BaseModel):\n",
    "    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "\n",
    "    @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch])\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM:\n",
    "        You are an expert web searcher. \n",
    "        Your task is to answer the user's question using the provided tools.\n",
    "        You have access to the following tools:\n",
    "            - `WebSearch`: Search the web for information.\n",
    "            - `RequestAssistance`: Request assistance from a human expert if you do not\n",
    "                know how to answer the question.\n",
    "\n",
    "        Once you have gathered all of the information you need, generate a writeup that\n",
    "        strikes the right balance between brevity and completeness. The goal is to\n",
    "        provide as much information to the writer as possible without overwhelming them.\n",
    "\n",
    "        MESSAGES: {self.history}\n",
    "        USER: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "    def _call(self, question: str | None = None): ...\n",
    "\n",
    "    def _step(self, question: str | None = None):\n",
    "        response = self._call(question)\n",
    "        tools_and_outputs = []\n",
    "        for chunk, tool in response:\n",
    "            if tool:\n",
    "                tools_and_outputs.append((tool, tool.call()))\n",
    "            else:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "        if response.user_message_param:\n",
    "            self.history.append(response.user_message_param)\n",
    "        self.history.append(response.message_param)\n",
    "        if tools_and_outputs:\n",
    "            self.history += response.tool_message_params(tools_and_outputs)\n",
    "            return self._step()\n",
    "        return response.content\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            question = input(\"(User): \")\n",
    "            if question in [\"quit\", \"exit\"]:\n",
    "                print(\"(Assistant): Have a great day!\")\n",
    "                break\n",
    "            print(\"(Assistant): \", end=\"\", flush=True)\n",
    "            self._step(question)\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "Chatbot().run()\n",
    "# Prompt:\n",
    "\"\"\"\n",
    "(User): Can you tell me about the Mirascope python library?\n",
    "(Assistant): The **Mirascope** library is a Python toolkit designed for creating applications using language model (LLM) APIs. Developed by William Bakst and released on August 18, 2024, Mirascope emphasizes simplicity, elegance, and developer experience. Here are some key features and details about the library:\n",
    "\n",
    "### Key Features\n",
    "1. **Simplicity and Ease of Use**: Mirascope aims to provide straightforward abstractions that enhance the developer experience without overwhelming complexity. It is designed for ease of onboarding and development.\n",
    "\n",
    "2. **Type Safety**: One of its strengths is the provision of proper type hints throughout the library. It actively manages Python typings, allowing developers to write their code intuitively while still benefiting from type safety.\n",
    "\n",
    "3. **Modular Design**: Mirascope is modular and extensible, enabling developers to tailor the library to their specific needs. Most dependencies are optional and provider-specific, so you can include only the components you require.\n",
    "\n",
    "4. **Core Primitives**: The library offers two main components:\n",
    "   - **Call and BasePrompt**: These primitives facilitate interactions with LLMs. Developers can create functions that integrate seamlessly with multiple LLM providers through decorators.\n",
    "\n",
    "5. **Advanced Functionality**: Mirascope supports features like asynchronous function calls, streaming responses, structured data extraction, custom output parsers, and dynamic variable injection.\n",
    "\n",
    "6. **Integration with FastAPI**: Mirascope includes decorators for wrapping functions into FastAPI routes, making it easier to deploy applications as web services.\n",
    "\n",
    "7. **Documentation and Examples**: The project comes with extensive usage documentation and example code to help users quickly understand how to utilize its features effectively.\n",
    "\n",
    "### Installation\n",
    "To install Mirascope, you can use the following command:\n",
    "```bash\n",
    "pip install mirascope\n",
    "```\n",
    "### Compatibility\n",
    "Mirascope is compatible with Python versions 3.10 to 3.11 (not supporting Python 4.0 and above) and is licensed under the MIT License.\n",
    "\n",
    "### Summary\n",
    "Mirascope positions itself as a simpler, less cumbersome alternative to other LLM frameworks like LangChain. It focuses on providing essential functionalities without unnecessary complexity, making development enjoyable and productive for software engineers looking to integrate LLMs into their applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9792ee4bed5ed",
   "metadata": {},
   "source": [
    "\n",
    "We have enhanced our chatbot's functionality with several key modifications:\n",
    "\n",
    "* Added a `WebSearch` tool to the tools parameter in the `@openai.call` decorator.\n",
    "* Refactored the streaming logic and history management into a new `_step` method.\n",
    "    * This enables calling `_step` iteratively until the agent is done calling tools and is ready to respond.\n",
    "    * For each `_step` we update the history to include tool usage and outputs.\n",
    "* Revised the prompt template to instruct the chatbot on how to utilize the new `WebSearch` tool.\n",
    "\n",
    "## Part 3: Human-in-the-loop\n",
    "\n",
    "Let us take a look at how we can slot in human input or approval using Mirascope.\n",
    "\n",
    "### Permission before using tool\n",
    "\n",
    "Since we are just writing python code, we don't need to setup an `interrupt_before`. We can simply add a function `_interrupt_before` that we call before calling our tool, like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a368f4a8bd2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot(BaseModel):\n",
    "    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "\n",
    "    @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch])\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM:\n",
    "        You are an expert web searcher. \n",
    "        Your task is to answer the user's question using the provided tools.\n",
    "        You have access to the following tools:\n",
    "            - `WebSearch`: Search the web for information.\n",
    "            - `RequestAssistance`: Request assistance from a human expert if you do not\n",
    "                know how to answer the question.\n",
    "\n",
    "        Once you have gathered all of the information you need, generate a writeup that\n",
    "        strikes the right balance between brevity and completeness. The goal is to\n",
    "        provide as much information to the writer as possible without overwhelming them.\n",
    "\n",
    "        MESSAGES: {self.history}\n",
    "        USER: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "    def _call(self, question: str | None = None): ...\n",
    "\n",
    "    def _interrupt_before(self, tool: openai.OpenAITool) -> openai.OpenAITool | None:\n",
    "        \"\"\"Interrupt before the tool is called. Return the modified tool or None.\"\"\"\n",
    "        if not isinstance(tool, WebSearch):\n",
    "            return tool\n",
    "        response = input(f\"Do you want to use the {tool._name()} tool? (y/n): \")\n",
    "        if response.lower() in [\"n\", \"no\"]:\n",
    "            response = input(\n",
    "                f\"Do you want to modify the {tool._name()} tool's query? (y/n): \"\n",
    "            )\n",
    "            if response.lower() in [\"n\", \"no\"]:\n",
    "                return None\n",
    "            else:\n",
    "                tool.query = input(\"(Assistant): Enter a new query: \")\n",
    "                return tool\n",
    "        else:\n",
    "            return tool\n",
    "\n",
    "    def _step(self, question: str | None = None):\n",
    "        response = self._call(question)\n",
    "        tools_and_outputs = []\n",
    "        for chunk, tool in response:\n",
    "            if tool:\n",
    "                new_tool = self._interrupt_before(tool)\n",
    "                if new_tool:\n",
    "                    tools_and_outputs.append((new_tool, new_tool.call()))\n",
    "            else:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "        if response.user_message_param:\n",
    "            self.history.append(response.user_message_param)\n",
    "        self.history.append(response.message_param)\n",
    "        if tools_and_outputs:\n",
    "            self.history += response.tool_message_params(tools_and_outputs)\n",
    "            return self._step()\n",
    "        return response.content\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            question = input(\"(User): \")\n",
    "            if question in [\"quit\", \"exit\"]:\n",
    "                print(\"(Assistant): Have a great day!\")\n",
    "                break\n",
    "            print(\"(Assistant): \", end=\"\", flush=True)\n",
    "            self._step(question)\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887abe0c958aff9f",
   "metadata": {},
   "source": [
    "Now, before the LLM calls the tool, it will prompt the user requesting permission:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44af01552873a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chatbot().run()\n",
    "# > (User): Can you tell me about the Mirascope python library?\n",
    "# > (Assistant): Do you want to use the WebSearch tool? (y/n): y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445d76f363c84b0",
   "metadata": {},
   "source": [
    "### Human-as-a-tool\n",
    "\n",
    "We can use prompt engineering techniques to help the LLM make a decision on whether it needs human intervention or not. Let's add a `RequestAssistance` tool and update our prompt so the LLM knows how to use our new tool.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428cd9863b838c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestAssistance(openai.OpenAITool):\n",
    "    \"\"\"A tool that requests assistance from a human expert.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"The request for assistance needed to properly respond to the user\",\n",
    "    )\n",
    "\n",
    "    def call(self) -> str:\n",
    "        \"\"\"Prompts a human to enter a response.\"\"\"\n",
    "        print(f\"I am in need of assistance. {self.query}\")\n",
    "        response = input(\"\\t(Human): \")\n",
    "        return f\"Human response: {response}\"\n",
    "\n",
    "\n",
    "class Chatbot(BaseModel):\n",
    "    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "\n",
    "    @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch, RequestAssistance])\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM:\n",
    "        You are an expert web searcher. \n",
    "        Your task is to answer the user's question using the provided tools.\n",
    "        You have access to the following tools:\n",
    "            - `WebSearch`: Search the web for information.\n",
    "            - `RequestAssistance`: Request assistance from a human expert if you do not\n",
    "                know how to answer the question.\n",
    "\n",
    "        Once you have gathered all of the information you need, generate a writeup that\n",
    "        strikes the right balance between brevity and completeness. The goal is to\n",
    "        provide as much information to the writer as possible without overwhelming them.\n",
    "\n",
    "        MESSAGES: {self.history}\n",
    "        USER: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "    def _call(self, question: str | None = None): ...\n",
    "\n",
    "    def _interrupt_before(self, tool: openai.OpenAITool) -> openai.OpenAITool | None:\n",
    "        \"\"\"Interrupt before the tool is called. Return the modified tool or None.\"\"\"\n",
    "        if not isinstance(tool, WebSearch):\n",
    "            return tool\n",
    "        response = input(f\"Do you want to use the {tool._name()} tool? (y/n): \")\n",
    "        if response.lower() in [\"n\", \"no\"]:\n",
    "            response = input(\n",
    "                f\"Do you want to modify the {tool._name()} tool's query? (y/n): \"\n",
    "            )\n",
    "            if response.lower() in [\"n\", \"no\"]:\n",
    "                return None\n",
    "            else:\n",
    "                tool.query = input(\"(Assistant): Enter a new query: \")\n",
    "                return tool\n",
    "        else:\n",
    "            return tool\n",
    "\n",
    "    def _step(self, question: str | None = None):\n",
    "        response = self._call(question)\n",
    "        tools_and_outputs = []\n",
    "        for chunk, tool in response:\n",
    "            if tool:\n",
    "                new_tool = self._interrupt_before(tool)\n",
    "                if new_tool:\n",
    "                    tools_and_outputs.append((new_tool, new_tool.call()))\n",
    "            else:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "        if response.user_message_param:\n",
    "            self.history.append(response.user_message_param)\n",
    "        self.history.append(response.message_param)\n",
    "        if tools_and_outputs:\n",
    "            self.history += response.tool_message_params(tools_and_outputs)\n",
    "            return self._step()\n",
    "        return response.content\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            question = input(\"(User): \")\n",
    "            if question in [\"quit\", \"exit\"]:\n",
    "                print(\"(Assistant): Have a great day!\")\n",
    "                break\n",
    "            print(\"(Assistant): \", end=\"\", flush=True)\n",
    "            self._step(question)\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "Chatbot().run()\n",
    "# > (User): What is my name?\n",
    "# > (Assistant): I am in need of assistance. How to identify a user's name in a conversation without them explicitly stating it?\n",
    "#       (Human): Tell them you don't know their name and would love for them to share it with you\n",
    "#   I’m not sure what your name is. I would love to know, so please feel free to share it!\n",
    "# > (User):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea81d5c5be10386",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"admonition note\">\n",
    "<p class=\"admonition-title\">Real-World Difference</p>\n",
    "<p>\n",
    "In the above example, we are getting the \"Human\" input in the console for demonstration purposes. In a real-world use-case, this would be hidden from the user and simply mention that the AI is requesting assistance while actually requesting assistance in the background (where the user would not see this interaction).\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Part 4: Time Travel also known as list splicing\n",
    "\n",
    "In order to revisit previous states, you can take the history list that stores all the messages and manipulate it however you want by basic list splicing.\n",
    "For example, if you want to \"rewind\" your state, you can simply remove user and assistant messages and then run your chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7417bacffb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = Chatbot()\n",
    "chatbot.run()\n",
    "# (User): Hi there! My name is Will.\n",
    "# (Assistant): It's nice to meet you, Will! I'm an AI assistant created by Anthropic. I'm here to help you with any questions or tasks you may have. Please let me know how I can assist you today.\n",
    "\n",
    "\n",
    "chatbot.run()\n",
    "# (User): Remember my name?\n",
    "# (Assistant): Of course, your name is Will. It's nice to meet you again!\n",
    "\n",
    "chatbot.history = chatbot.history[:-4]\n",
    "# (User): Remember my name?\n",
    "# (Assistant): I'm afraid I don't actually have the capability to remember your name. As an AI assistant, I don't have a persistent memory of our previous conversations or interactions. I respond based on the current context provided to me. Could you please restate your name or provide more information so I can try to assist you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa2754d4c9ee2f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "While in this example, we are calling the run function multiple times, there is nothing stopping you from updating the history inside the Chatbot. Note that in real-world applications, conversation history is typically stored in a cache, database, or other persistent storage systems, so add `computed_fields` as necessary to retrieve from storage.\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "<p class=\"admonition-title\">Dynamic, Relevant History</p>\n",
    "<p>\n",
    "We have seen cases where the history is retrieved dynamically from e.g. a vector store. This enables injecting only relevant history along with each call, which can help reduce token usage and keep the assistant's responses more relevant.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Congratulations! You've now learned how to create a sophisticated chatbot using Mirascope and simple Python code. This approach demonstrates that powerful AI applications can be built with minimal reliance on complex abstractions or pre-built tools, giving you greater flexibility and control over your project's architecture.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
