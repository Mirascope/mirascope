{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dynamic Configuration & Chaining\n",
        "\n",
        "This notebook provides a detailed introduction to using Dynamic Configuration and Chaining in Mirascope. We'll cover various examples ranging from basic usage to more complex chaining techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install Mirascope and set up our environment. We'll use OpenAI for our examples, but you can adapt these to other providers supported by Mirascope. For more information on supported providers, see the [Calls documentation](../../../learn/calls)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"mirascope[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Configuration\n",
        "\n",
        "Dynamic Configuration in Mirascope allows you to modify the behavior of LLM calls at runtime based on input arguments or other conditions.\n",
        "\n",
        "### Basic Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-11T07:36:01.762693Z",
          "start_time": "2024-09-11T07:35:57.078707Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Low creativity: I recommend \"The Guest List\" by Lucy Foley. This gripping mystery unfolds during a glamorous wedding on a remote Irish island, where tensions among the guests rise and secrets are revealed. When a murder occurs, everyone becomes a suspect, and the story alternates between different perspectives, keeping you guessing until the very end. It's a compelling read with well-developed characters and a twisty plot that will keep you on the edge of your seat!\n",
            "High creativity: I recommend \"The Girl with the Dragon Tattoo\" by Stieg Larsson. This gripping mystery combines elements of crime, family saga, and social commentary. It follows journalist Mikael Blomkvist and hacker Lisbeth Salander as they investigate a decades-old disappearance amidst a backdrop of dark family secrets and corporate corruption. The complex characters and intricate plot make it a compelling read for mystery enthusiasts. Enjoy!\n"
          ]
        }
      ],
      "source": [
        "from mirascope.core import BaseDynamicConfig, Messages, openai\n",
        "\n",
        "\n",
        "@openai.call(\"gpt-4o-mini\")\n",
        "def recommend_book(genre: str, creativity: float) -> BaseDynamicConfig:\n",
        "    return {\n",
        "        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n",
        "        \"call_params\": {\"temperature\": creativity},\n",
        "    }\n",
        "\n",
        "\n",
        "# Low creativity recommendation\n",
        "response = recommend_book(\"mystery\", 0.2)\n",
        "print(\"Low creativity:\", response.content)\n",
        "\n",
        "# High creativity recommendation\n",
        "response = recommend_book(\"mystery\", 0.8)\n",
        "print(\"High creativity:\", response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Computed Fields\n",
        "\n",
        "When using the `Messages.Type` return for writing prompt templates, you can inject computed fields directly into the formatted strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I recommend **\"An Ember in the Ashes\"** by Sabaa Tahir. This young adult fantasy novel is set in a brutal, ancient Rome-inspired world where the oppressed must fight against a tyrannical regime. The story follows Laia, a young woman who becomes a spy for the resistance to save her brother, and Elias, a soldier who wants to escape the oppressive society. The book is rich in world-building, features complex characters, and explores themes of freedom, loyalty, and sacrifice. It's an engaging read for young adults looking for an exciting fantasy adventure!\n"
          ]
        }
      ],
      "source": [
        "@openai.call(\"gpt-4o-mini\")\n",
        "def recommend_book_by_age(genre: str, age: int) -> Messages.Type:\n",
        "    reading_level = \"adult\"\n",
        "    if age < 12:\n",
        "        reading_level = \"elementary\"\n",
        "    elif age < 18:\n",
        "        reading_level = \"young adult\"\n",
        "    return f\"Recommend a {genre} book with a reading level of {reading_level}\"\n",
        "\n",
        "\n",
        "response = recommend_book_by_age(\"fantasy\", 15)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When using string templates, you can use computed fields to dynamically generate or modify template variables used in your prompt. For more information on prompt templates, see the [Prompts documentation](../../../learn/prompts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-11T07:36:06.480670Z",
          "start_time": "2024-09-11T07:36:05.149070Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I recommend \"An Ember in the Ashes\" by Sabaa Tahir. This gripping fantasy novel is set in a world inspired by ancient Rome, where a fierce soldier and a rebellious scholar find their destinies intertwined. With themes of bravery, sacrifice, and love, it's an engaging read suitable for young adult audiences. The rich world-building and complex characters make it a standout in the young adult fantasy genre. Enjoy your reading!\n"
          ]
        }
      ],
      "source": [
        "from mirascope.core import prompt_template\n",
        "\n",
        "\n",
        "@openai.call(\"gpt-4o-mini\")\n",
        "@prompt_template(\"Recommend a {genre} book with a reading level of {reading_level}\")\n",
        "def recommend_book_by_age(genre: str, age: int) -> openai.OpenAIDynamicConfig:\n",
        "    reading_level = \"adult\"\n",
        "    if age < 12:\n",
        "        reading_level = \"elementary\"\n",
        "    elif age < 18:\n",
        "        reading_level = \"young adult\"\n",
        "    return {\"computed_fields\": {\"reading_level\": reading_level}}\n",
        "\n",
        "\n",
        "response = recommend_book_by_age(\"fantasy\", 15)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dynamic Tools\n",
        "\n",
        "You can dynamically configure which tools are available to the LLM based on runtime conditions. Here's a simple example using a basic tool function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-11T07:36:09.151739Z",
          "start_time": "2024-09-11T07:36:08.392083Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Girl with the Dragon Tattoo by Stieg Larsson (Mystery)\n"
          ]
        }
      ],
      "source": [
        "def format_book(title: str, author: str, genre: str) -> str:\n",
        "    \"\"\"Format a book recommendation.\"\"\"\n",
        "    return f\"{title} by {author} ({genre})\"\n",
        "\n",
        "\n",
        "@openai.call(\"gpt-4o-mini\")\n",
        "def recommend_book_with_tool(genre: str) -> openai.OpenAIDynamicConfig:\n",
        "    return {\n",
        "        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n",
        "        \"tools\": [format_book],\n",
        "    }\n",
        "\n",
        "\n",
        "response = recommend_book_with_tool(\"mystery\")\n",
        "if response.tool:\n",
        "    print(response.tool.call())\n",
        "else:\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more advanced usage of tools, including the `BaseToolKit` class, please refer to the [Tools documentation](../../../learn/tools) and the [Tools and Agents Tutorial](../tools_and_agents)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chaining\n",
        "\n",
        "Chaining in Mirascope allows you to combine multiple LLM calls or operations in a sequence to solve complex tasks. Let's explore two main approaches to chaining: function-based chaining and chaining with computed fields.\n",
        "\n",
        "### Function-based Chaining\n",
        "\n",
        "In function-based chaining, you call multiple functions in sequence, passing the output of one function as input to the next. This approach requires you to manage the sequence of calls manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-11T07:36:16.594586Z",
          "start_time": "2024-09-11T07:36:15.294731Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bien sûr ! Veuillez fournir le long texte en anglais que vous souhaitez que je résume.\n"
          ]
        }
      ],
      "source": [
        "@openai.call(\"gpt-4o-mini\")\n",
        "def summarize(text: str) -> str:\n",
        "    return f\"Summarize this text: {text}\"\n",
        "\n",
        "\n",
        "@openai.call(\"gpt-4o-mini\")\n",
        "def translate(text: str, language: str) -> str:\n",
        "    return f\"Translate this text to {language}: {text}\"\n",
        "\n",
        "\n",
        "original_text = \"Long English text here...\"\n",
        "summary = summarize(original_text)\n",
        "translation = translate(summary.content, \"french\")\n",
        "print(translation.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nested Chaining\n",
        "\n",
        "You can easily create a single function that calls the entire chain simply by calling each part of the chain in the function body of the corresponding parent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bien sûr ! Veuillez fournir le texte que vous souhaitez que je résume, et je ferai de mon mieux pour vous aider.\n"
          ]
        }
      ],
      "source": [
        "@openai.call(\"gpt-4o-mini\")\n",
        "def summarize(text: str) -> str:\n",
        "    return f\"Summarize this text: {text}\"\n",
        "\n",
        "\n",
        "@openai.call(\"gpt-4o-mini\")\n",
        "def summarize_and_translate(text: str, language: str) -> str:\n",
        "    summary = summarize(original_text)\n",
        "    return f\"Translate this text to {language}: {summary.content}\"\n",
        "\n",
        "\n",
        "original_text = \"Long English text here...\"\n",
        "translation = summarize_and_translate(original_text, \"french\")\n",
        "print(translation.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chaining with Computed Fields\n",
        "\n",
        "Chaining with computed fields allows you to better trace your nested chains since the full chain of operations will exist in the response of the single function (rather than having to call and track each part of the chain separately)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-11T07:36:20.903239Z",
          "start_time": "2024-09-11T07:36:19.838736Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation: Bien sûr ! Veuillez fournir le texte que vous aimeriez que je résume, et je serai heureux de vous aider.\n",
            "\n",
            "Computed fields (including summary): {'summary': OpenAICallResponse(metadata={}, response=ChatCompletion(id='chatcmpl-ABSRkdlz6phtmOBWqwVigidqET2tr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Of course! Please provide the text you'd like me to summarize, and I'll be happy to help.\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727294320, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_1bb46167f9', usage=CompletionUsage(completion_tokens=20, prompt_tokens=18, total_tokens=38, completion_tokens_details={'reasoning_tokens': 0})), tool_types=None, prompt_template=None, fn_args={'text': 'Long English text here...'}, dynamic_config={'messages': [BaseMessageParam(role='user', content='Summarize this text: Long English text here...')]}, messages=[{'role': 'user', 'content': 'Summarize this text: Long English text here...'}], call_params={}, call_kwargs={'model': 'gpt-4o-mini', 'messages': [{'role': 'user', 'content': 'Summarize this text: Long English text here...'}]}, user_message_param={'content': 'Summarize this text: Long English text here...', 'role': 'user'}, start_time=1727294320238.506, end_time=1727294320929.094, message_param={'content': \"Of course! Please provide the text you'd like me to summarize, and I'll be happy to help.\", 'refusal': None, 'role': 'assistant', 'tool_calls': None}, tools=None, tool=None)}\n"
          ]
        }
      ],
      "source": [
        "@openai.call(\"gpt-4o-mini\")\n",
        "def summarize(text: str) -> str:\n",
        "    return f\"Summarize this text: {text}\"\n",
        "\n",
        "\n",
        "@openai.call(\"gpt-4o-mini\")\n",
        "@prompt_template(\"Translate this text to {language}: {summary}\")\n",
        "def summarize_and_translate(text: str, language: str) -> BaseDynamicConfig:\n",
        "    return {\"computed_fields\": {\"summary\": summarize(text)}}\n",
        "\n",
        "\n",
        "response = summarize_and_translate(\"Long English text here...\", \"french\")\n",
        "print(\"Translation:\", response.content)\n",
        "print(\n",
        "    \"\\nComputed fields (including summary):\", response.dynamic_config[\"computed_fields\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, with computed fields, you get access to both the final translation and the intermediate summary in a single response. This approach provides better traceability and can be particularly useful for debugging and understanding the entire chain of operations without the need to manage multiple separate function calls.\n",
        "\n",
        "Of course, you can always put the computed fields in the dynamic configuration without using string templates for the same effect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation: Bien sûr ! Veuillez fournir le texte que vous aimeriez que je résume.\n",
            "\n",
            "Computed fields (including summary): {'summary': OpenAICallResponse(metadata={}, response=ChatCompletion(id='chatcmpl-ABSRz6D82ecWKlAU7VUmWU4wbqiaY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Please provide the text you'd like me to summarize.\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727294335, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_1bb46167f9', usage=CompletionUsage(completion_tokens=12, prompt_tokens=18, total_tokens=30, completion_tokens_details={'reasoning_tokens': 0})), tool_types=None, prompt_template=None, fn_args={'text': 'Long English text here...'}, dynamic_config={'messages': [BaseMessageParam(role='user', content='Summarize this text: Long English text here...')]}, messages=[{'role': 'user', 'content': 'Summarize this text: Long English text here...'}], call_params={}, call_kwargs={'model': 'gpt-4o-mini', 'messages': [{'role': 'user', 'content': 'Summarize this text: Long English text here...'}]}, user_message_param={'content': 'Summarize this text: Long English text here...', 'role': 'user'}, start_time=1727294335206.361, end_time=1727294335879.617, message_param={'content': \"Sure! Please provide the text you'd like me to summarize.\", 'refusal': None, 'role': 'assistant', 'tool_calls': None}, tools=None, tool=None)}\n"
          ]
        }
      ],
      "source": [
        "@openai.call(\"gpt-4o-mini\")\n",
        "def summarize(text: str) -> str:\n",
        "    return f\"Summarize this text: {text}\"\n",
        "\n",
        "\n",
        "@openai.call(\"gpt-4o-mini\")\n",
        "def summarize_and_translate(text: str, language: str) -> BaseDynamicConfig:\n",
        "    summary = summarize(text)\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            Messages.User(f\"Translate this text to {language}: {summary.content}\"),\n",
        "        ],\n",
        "        \"computed_fields\": {\"summary\": summary},\n",
        "    }\n",
        "\n",
        "\n",
        "response = summarize_and_translate(\"Long English text here...\", \"french\")\n",
        "print(\"Translation:\", response.content)\n",
        "print(\n",
        "    \"\\nComputed fields (including summary):\", response.dynamic_config[\"computed_fields\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling in Chains\n",
        "\n",
        "Implementing robust error handling is crucial in complex chains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-12T01:08:04.218737Z",
          "start_time": "2024-09-12T01:08:01.204558Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed Result: Bien sûr ! Veuillez fournir le texte que vous souhaiteriez que je résume.\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAIError\n",
        "\n",
        "\n",
        "@openai.call(\"gpt-4o-mini\")\n",
        "def summarize(text: str) -> str:\n",
        "    return f\"Summarize this text: {text}\"\n",
        "\n",
        "\n",
        "@openai.call(\"gpt-4o-mini\")\n",
        "def translate(text: str, language: str) -> str:\n",
        "    return f\"Translate this text to {language}: {text}\"\n",
        "\n",
        "\n",
        "def process_text_with_error_handling(text: str, target_language: str):\n",
        "    try:\n",
        "        summary = summarize(text).content\n",
        "    except OpenAIError as e:\n",
        "        print(f\"Error during summarization: {e}\")\n",
        "        summary = text  # Fallback to original text if summarization fails\n",
        "\n",
        "    try:\n",
        "        translation = translate(summary, target_language).content\n",
        "        return translation\n",
        "    except OpenAIError as e:\n",
        "        print(f\"Error during translation: {e}\")\n",
        "        return summary  # Fallback to summary if translation fails\n",
        "\n",
        "\n",
        "result = process_text_with_error_handling(\"Long text here...\", \"French\")\n",
        "print(\"Processed Result:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook has demonstrated various techniques for using Dynamic Configuration and Chaining in Mirascope. These powerful features allow you to create flexible, efficient, and complex LLM-powered applications. By combining these techniques, you can build sophisticated AI systems that can adapt to different inputs and requirements while maintaining robustness and traceability.\n",
        "\n",
        "Remember to always consider error handling, especially in complex chains, to ensure your applications are resilient to potential issues that may arise during LLM calls or processing steps.\n",
        "\n",
        "If you like what you've seen so far, [give us a star](https://github.com/Mirascope/mirascope) and [join our community](https://join.slack.com/t/mirascope-community/shared_invite/zt-2ilqhvmki-FB6LWluInUCkkjYD3oSjNA)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
